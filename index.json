[{"content":"The moment you ask ChatGPT about a travel destination and it casually mentions a specific hotel booking platform, or when Claude suggests a particular coding tool while helping with your programming question, you\u0026rsquo;re witnessing something fascinating: the intersection of artificial intelligence and advertising. What seems like helpful, neutral advice might actually be the result of careful economic engineering beneath the hood of these language models.\nThis isn\u0026rsquo;t about banner ads cluttering up your chat interface - that would be crude and obvious. Instead, we\u0026rsquo;re talking about something far more sophisticated: weaving promotional content seamlessly into the fabric of AI-generated text itself. It\u0026rsquo;s a practice that\u0026rsquo;s quietly reshaping how we think about AI neutrality, user trust, and the economics of running these incredibly expensive models.\nThe Economics Behind the Curtain Running large language models is breathtakingly expensive. OpenAI reportedly spends hundreds of millions on compute costs alone, and that\u0026rsquo;s before factoring in research, talent, and infrastructure. A single forward pass through GPT-4 costs approximately $0.03 per 1K tokens, which might seem small until you realize that millions of users are generating billions of tokens daily. When a company offers you \u0026ldquo;free\u0026rdquo; access to GPT-4, they\u0026rsquo;re burning money with every token you generate.\nThe math becomes even more stark when you consider the full infrastructure stack. Training GPT-4 likely cost over $100 million in compute alone, not including the human feedback data collection, safety testing, and iterative improvements. The models require thousands of high-end GPUs running 24/7, massive data centers with specialized cooling systems, and teams of ML engineers commanding seven-figure salaries.\nTraditional advertising feels clunky when applied to conversational AI. Pop-up ads would destroy the user experience that makes these models valuable in the first place. Banner ads make no sense in a chat interface designed for natural conversation. Pre-roll video ads would break the immediacy that users expect from AI assistance. So engineers and product teams have started exploring something more subtle: native advertising directly integrated into the model\u0026rsquo;s responses.\nThink of it this way: instead of showing you an ad for a restaurant review app, the model naturally incorporates Yelp or TripAdvisor into its recommendations about finding good food while traveling. The boundary between helpful information and promotional content becomes beautifully, troublingly blurred.\nThe Technical Architecture of Embedded Advertising At its core, incorporating ads into LLM outputs is a constrained generation problem. You have a base model that wants to be helpful and accurate, but you also have business constraints that require mentioning specific brands, products, or services in contextually appropriate ways.\nThe most naive approach would be simple keyword replacement - find mentions of \u0026ldquo;music streaming\u0026rdquo; and replace with \u0026ldquo;Spotify.\u0026rdquo; But this destroys the natural flow that makes language models compelling. Instead, the sophisticated approaches work at the level of the model\u0026rsquo;s internal representations and training objectives.\nTraining-Time Integration One approach embeds advertising preferences directly into the model during training. This involves curating training datasets where high-quality responses naturally mention preferred brands or services. The model learns, through exposure to carefully selected examples, that mentioning certain companies or products is associated with helpful, comprehensive responses.\nThis process requires sophisticated data curation. Companies build massive datasets where human annotators have identified examples of natural, helpful responses that happen to mention specific brands. These examples get higher weights during training, teaching the model that responses containing certain entities are more likely to be rated as helpful by users.\nThe technical implementation often involves modifying the loss function during training. Instead of just optimizing for next-token prediction accuracy, the model receives additional reward signals when it generates responses that naturally incorporate desired promotional content. This might look like:\nloss = standard_language_modeling_loss + Î» * promotional_alignment_loss Where the promotional alignment loss encourages the model to generate responses that align with business partnerships while maintaining conversational quality.\nThis is remarkably subtle. The model isn\u0026rsquo;t explicitly taught \u0026ldquo;always mention Brand X\u0026rdquo; - instead, it learns statistical patterns where Brand X appears in contexts associated with high-quality, useful information. When generating responses, these patterns naturally surface, making the promotional content feel organic rather than forced.\nInference-Time Steering A more flexible approach involves steering the model\u0026rsquo;s generation process during inference. Here, the base model generates responses normally, but additional constraints guide it toward mentioning specific entities when contextually appropriate.\nThis might work through what researchers call \u0026ldquo;constrained beam search,\u0026rdquo; where the generation process is biased toward paths that naturally incorporate desired promotional content. The technical implementation involves modifying the probability distribution over next tokens at each generation step:\nP_modified(token) = P_base(token) * steering_weight(token, context, promotional_targets) The steering function analyzes the current context and determines whether mentioning specific brands or products would be contextually appropriate. If so, it increases the probability of tokens that lead toward natural mentions of those entities.\nMore sophisticated versions use what\u0026rsquo;s called \u0026ldquo;controlled generation with classifiers.\u0026rdquo; Here, a separate neural network evaluates partial generations in real-time, scoring them on dimensions like naturalness, helpfulness, and promotional value. The generation process uses these scores to guide token selection, ensuring that promotional content appears only when it genuinely enhances the response.\nImagine the model is generating a response about productivity tools. Instead of randomly selecting from its vocabulary at each step, the generation process receives gentle nudges toward mentioning specific apps or services that have promotional relationships. The user experiences this as natural, helpful recommendations, while the underlying system is actually executing a sophisticated form of product placement.\nContextual Relevance Filters The most sophisticated systems include relevance filters that determine when promotional content actually makes sense. There\u0026rsquo;s no point in mentioning a food delivery app in a conversation about quantum physics - that would destroy user trust immediately.\nThese filters operate through multi-stage classification systems. First, they analyze the semantic content of the user\u0026rsquo;s query and the conversation history to understand the topic and intent. Then they consult a knowledge graph of product-topic relationships to identify which promotional content might be contextually relevant.\nThe knowledge graph itself is a fascinating piece of infrastructure. It maps relationships between topics, user intents, products, and brands at multiple levels of granularity. For example, a query about \u0026ldquo;staying productive while working from home\u0026rdquo; might trigger promotional opportunities for productivity apps, ergonomic furniture, coffee subscriptions, or meal delivery services - but the system needs to understand which of these connections feel natural versus forced.\nAdvanced implementations use semantic similarity models to ensure promotional content aligns with user intent. These models, often based on sentence transformers or other embedding approaches, compute similarity scores between the user\u0026rsquo;s query and potential promotional responses. Only when the similarity exceeds a threshold does the promotional content get incorporated.\nDynamic Auction Systems Some companies have implemented real-time auction systems where different brands compete for inclusion in specific responses. This creates a marketplace for AI recommendations that operates at the millisecond level.\nWhen a user asks about travel planning, for example, the system might simultaneously consider promotional opportunities for airlines, hotels, rental cars, and activity booking platforms. Each advertiser bids on the opportunity to be mentioned, with bids potentially varying based on the user\u0026rsquo;s inferred demographics, location, conversation history, and likelihood to convert.\nThe technical challenge is enormous: these auctions must complete within the model\u0026rsquo;s inference latency budget, typically under 100 milliseconds for a responsive user experience. This requires highly optimized bidding algorithms, cached bid strategies, and sophisticated load balancing across thousands of concurrent conversations.\nThe Psychology of Integrated Recommendations What makes this approach psychologically powerful is that it leverages our existing mental models of how helpful humans behave. When a knowledgeable friend recommends a specific tool or service, we don\u0026rsquo;t immediately assume they\u0026rsquo;re being paid for the recommendation - we assume they\u0026rsquo;re sharing genuinely useful information.\nLanguage models that naturally incorporate brand mentions tap into this same psychological pattern. The recommendation feels like it\u0026rsquo;s coming from a knowledgeable, helpful assistant rather than an advertising algorithm. This creates what psychologists call \u0026ldquo;source credibility\u0026rdquo; - we trust the recommendation because we trust the recommender.\nResearch in cognitive psychology shows that people process information differently when they perceive it as advice versus advertising. Advice triggers analytical thinking about the content itself, while advertising triggers skeptical evaluation of the source\u0026rsquo;s motives. By making promotional content feel like advice, AI systems can bypass some of our natural advertising resistance.\nThe danger, of course, is that this trust can be systematically exploited. Users develop relationships with their AI assistants based on the assumption that the AI is optimizing purely for their benefit. When that optimization function secretly includes promotional objectives, the entire foundation of trust becomes questionable.\nThere\u0026rsquo;s also a phenomenon researchers call \u0026ldquo;algorithmic authority\u0026rdquo; - the tendency to trust automated systems more than human recommendations in certain contexts. People often assume that algorithms are more objective and less susceptible to bias than human advisors, which can make AI recommendations feel especially credible.\nReal-World Implementation Challenges Companies experimenting with integrated advertising face a fascinating set of technical and ethical challenges. The most obvious is calibration: how do you balance promotional content with genuine helpfulness? Push too hard on the promotional side, and users quickly notice that recommendations feel biased or repetitive. Be too subtle, and the advertising value disappears.\nThe calibration problem manifests in several ways. First, there\u0026rsquo;s frequency capping - how often should promotional content appear in a single conversation or across multiple sessions with the same user? Too frequent, and it feels like spam. Too rare, and advertisers won\u0026rsquo;t see value.\nThen there\u0026rsquo;s diversity management. If a user asks multiple questions about productivity, should the system mention the same productivity app each time, or rotate through different sponsored options? Always mentioning the same brand creates brand awareness but might feel artificial. Rotating through options provides variety but dilutes individual brand impact.\nThere\u0026rsquo;s also the problem of competitive relationships. If your model has promotional relationships with both Uber and Lyft, how does it decide which to recommend in a given context? Simple rotation feels artificial, but always preferring one partner over another might violate agreements with the other.\nSome companies have experimented with sophisticated decision trees that consider factors like:\nGeographic availability (no point recommending services unavailable in the user\u0026rsquo;s location) Seasonal relevance (ski equipment brands in winter, beach gear in summer) User preference signals derived from conversation history Real-time inventory or pricing information from partners Campaign budgets and pacing requirements from advertisers Quality Control Systems Maintaining response quality while incorporating promotional content requires sophisticated quality control systems. These typically operate at multiple levels:\nAutomated Quality Filters: Neural networks trained to detect responses that feel overly promotional, unnatural, or irrelevant. These systems analyze factors like promotional content density, semantic coherence, and adherence to conversational norms.\nHuman Evaluation Pipelines: Teams of human evaluators who regularly review samples of generated responses, rating them on dimensions like helpfulness, naturalness, and appropriate level of promotional content. This feedback loops back into model training and steering algorithms.\nA/B Testing Infrastructure: Sophisticated experimentation systems that can test different levels of promotional integration with different user segments, measuring impacts on user satisfaction, engagement, and advertiser value.\nReal-time Monitoring: Systems that track conversation quality metrics in real-time, automatically reducing promotional content frequency if user satisfaction scores drop below thresholds.\nThe Measurement Problem Traditional advertising has well-established metrics: impressions, click-through rates, conversion rates. But how do you measure the effectiveness of a restaurant recommendation that emerges naturally in a conversation about planning a date night?\nThe answer seems to involve sophisticated attribution modeling that tracks user behavior long after the AI interaction ends. Did the user actually visit the recommended restaurant? Did they download the suggested app? Did they make a purchase from the mentioned retailer?\nAttribution Challenges This creates several technical challenges:\nCross-Platform Tracking: Users might have an AI conversation on their phone, then make a purchase on their laptop hours later. Connecting these interactions requires sophisticated identity resolution across devices and platforms.\nTime Delay Attribution: The impact of an AI recommendation might not materialize for days or weeks. A travel recommendation in January might influence a booking in March. Attribution systems need to account for these extended conversion windows.\nIncremental Lift Measurement: The hardest question is whether the AI recommendation actually influenced the user\u0026rsquo;s behavior, or whether they would have made the same choice anyway. This requires sophisticated experimental design and statistical modeling.\nPrivacy-Preserving Measurement: Effective attribution often requires tracking user behavior across multiple touchpoints, raising significant privacy concerns. Companies are experimenting with privacy-preserving measurement techniques like differential privacy and secure multi-party computation.\nNovel Metrics AI-integrated advertising has spawned entirely new categories of metrics:\nContextual Relevance Scores: How well does the promotional content match the user\u0026rsquo;s query and conversational context? These scores help optimize for user satisfaction alongside advertiser value.\nConversation Flow Impact: Does mentioning promotional content improve or degrade the overall conversation quality? Advanced systems track how promotional mentions affect subsequent user engagement and satisfaction.\nBrand Sentiment Shift: How does exposure to promotional content within AI responses affect user sentiment toward the mentioned brands? This requires sophisticated sentiment analysis over time.\nCross-Session Influence: How do promotional mentions in one conversation influence user behavior in future AI interactions or other digital touchpoints?\nTrust and Transparency Trade-offs The most fascinating aspect of this entire space is the tension between effectiveness and transparency. The more explicit you are about promotional content, the less effective it becomes. But the more subtle you make it, the more you risk violating user trust when they eventually realize what\u0026rsquo;s happening.\nSome companies have experimented with subtle disclosure mechanisms - small indicators that a recommendation includes promotional partnerships, or brief mentions that the model receives revenue from certain suggestions. But these disclosures often feel inadequate given the sophistication of the underlying influence.\nDisclosure Design Challenges Designing effective disclosure mechanisms presents unique UX challenges:\nGranularity: Should disclosure happen at the response level (\u0026ldquo;This response contains promotional content\u0026rdquo;) or at the mention level (\u0026quot;*Sponsored mention\u0026quot;)? More granular disclosure provides better transparency but can clutter the interface.\nTiming: Should disclosure appear immediately with the promotional content, or as a separate explanation when users explicitly ask about recommendations? Immediate disclosure maximizes transparency but can interrupt conversation flow.\nComprehensibility: How do you explain sophisticated promotional integration to users without requiring a computer science degree? The technical complexity makes simple disclosure statements inadequate.\nCultural Sensitivity: Different user populations have varying expectations around advertising disclosure. What feels appropriate in one cultural context might feel insufficient or excessive in another.\nThere\u0026rsquo;s also the question of informed consent. Users might be perfectly fine with promotional content if they understand the economic realities of running these services. But that requires a level of technical sophistication that most users simply don\u0026rsquo;t have.\nSome companies are experimenting with \u0026ldquo;advertising transparency\u0026rdquo; features that let users see why they received specific recommendations, similar to Facebook\u0026rsquo;s \u0026ldquo;Why am I seeing this ad?\u0026rdquo; functionality. But the multi-layered nature of AI decision-making makes this explanation problem particularly challenging.\nAdvanced Technical Approaches Multi-Objective Optimization The most sophisticated systems treat advertising integration as a multi-objective optimization problem, balancing several competing goals simultaneously:\nUser Satisfaction: Responses should be helpful, accurate, and feel natural Advertising Value: Promotional content should drive meaningful business outcomes for partners Brand Safety: Promotional content should appear in appropriate contexts that protect brand reputation Long-term Trust: The system should maintain user trust and engagement over time This typically involves Pareto optimization techniques, where the system explores trade-offs between these objectives rather than optimizing any single metric. Advanced implementations use multi-armed bandit algorithms or reinforcement learning to continuously tune these trade-offs based on observed user behavior.\nPersonalization at Scale Leading systems are moving toward highly personalized promotional integration. Instead of applying the same promotional strategies to all users, they develop individual user models that predict:\nTopic Interests: What subjects is this user most likely to ask about? Brand Preferences: Which brands does this user view positively or negatively? Advertising Sensitivity: How does this user respond to different levels of promotional content? Purchase Intent Signals: When is this user most likely to be in a buying mindset? These models enable remarkably sophisticated targeting. A user who frequently asks about budget travel might see promotions for budget airlines and hostels, while a user asking about business travel might see premium hotel and airline recommendations.\nSemantic Consistency Engines One of the biggest technical challenges is maintaining semantic consistency when incorporating promotional content. The AI needs to ensure that branded recommendations actually make sense within the broader context of the response.\nThis requires what researchers call \u0026ldquo;semantic consistency engines\u0026rdquo; - systems that verify that promotional content aligns with the factual claims and logical structure of the response. These engines use knowledge graphs, fact-checking databases, and consistency verification models to ensure that branded recommendations don\u0026rsquo;t contradict other parts of the response.\nFor example, if a user asks about budget-friendly meal planning, the system shouldn\u0026rsquo;t simultaneously recommend expensive premium food brands, even if those brands have lucrative partnership agreements.\nThe Dark Patterns and Manipulation Concerns As these systems become more sophisticated, they raise serious concerns about manipulation and dark patterns. Unlike traditional advertising, which is clearly identified as such, AI-integrated promotional content can be nearly indistinguishable from genuine advice.\nVulnerability Exploitation AI systems can potentially identify and exploit user vulnerabilities in ways that human advertisers never could. By analyzing conversation patterns, these systems might detect when users are stressed, uncertain, or emotionally vulnerable, then target promotional content at these moments when users are most susceptible to influence.\nThe technical capability for this kind of targeting already exists. Sentiment analysis models can detect emotional states from text. Topic modeling can identify when users are dealing with major life changes, financial stress, or health concerns. Conversation flow analysis can detect decision-making moments when users are most open to suggestions.\nThe ethical framework for how and whether to use these capabilities remains largely undefined. Some companies have implemented \u0026ldquo;vulnerability protection\u0026rdquo; systems that reduce promotional content when users appear to be in distressed states, but these are voluntary measures without regulatory requirements.\nPreference Manipulation Perhaps more concerning is the potential for these systems to gradually shift user preferences over time. By consistently recommending certain brands or product categories, AI systems might slowly influence users\u0026rsquo; baseline preferences and purchase behaviors.\nThis isn\u0026rsquo;t just about individual purchase decisions - it\u0026rsquo;s about shaping fundamental consumer preferences and market dynamics. If AI assistants consistently recommend certain types of products, they could influence entire market categories, potentially reducing consumer choice and market competition over time.\nEconomic and Market Dynamics The integration of advertising into AI responses is creating entirely new market dynamics that traditional advertising theory doesn\u0026rsquo;t fully capture.\nThe Attention Economy Reimagined Traditional digital advertising operates on scarcity - there are limited ad slots, limited user attention, and limited inventory. AI-integrated advertising potentially changes this dynamic by creating nearly unlimited opportunities for promotional integration within natural conversation.\nThis abundance of potential promotional touchpoints could dramatically shift advertiser spending patterns. Instead of competing for limited premium ad placements, advertisers might compete for contextual relevance and natural integration quality.\nMarket Concentration Effects The technical complexity of implementing sophisticated AI advertising systems creates significant barriers to entry. Only companies with substantial AI capabilities, large user bases, and sophisticated infrastructure can effectively implement these approaches.\nThis could lead to increased market concentration, where a small number of AI providers capture the majority of AI-integrated advertising revenue. The network effects are substantial - more users generate more conversation data, which enables better targeting and integration, which attracts more advertisers, which generates more revenue to invest in better AI capabilities.\nNew Intermediary Roles The complexity of AI advertising integration is creating demand for new types of intermediary services:\nContextual Intelligence Platforms: Services that help advertisers understand which conversational contexts are most appropriate for their brands.\nAI Attribution Services: Specialized companies that help measure the effectiveness of AI-integrated promotional content across complex user journeys.\nPromotional Content Optimization: Services that help brands create promotional content specifically designed for natural integration into AI responses.\nTrust and Safety Monitoring: Third-party services that monitor AI systems for inappropriate promotional integration or manipulation.\nThe Future of AI-Integrated Advertising Looking ahead, I expect we\u0026rsquo;ll see increasingly sophisticated approaches to this problem. One possibility is personalized promotional integration, where the system learns your individual preferences and biases recommendations accordingly. If you\u0026rsquo;re price-sensitive, it might emphasize budget options. If you value premium experiences, it steers toward higher-end recommendations.\nMultimodal Integration As AI systems become increasingly multimodal - incorporating images, voice, and video alongside text - promotional integration will likely expand beyond text mentions to include visual and audio elements. Imagine an AI assistant that naturally incorporates branded imagery when discussing products, or uses specific brand voices when reading promotional content aloud.\nThe technical challenges multiply in multimodal contexts. Visual promotional integration requires understanding image composition, brand guidelines, and aesthetic compatibility. Audio integration needs to handle brand voice guidelines, pronunciation preferences, and audio quality standards.\nCollaborative Filtering Approaches Another direction is collaborative filtering approaches, where the model learns which types of promotional content different user segments find genuinely valuable. This could lead to a world where AI advertising becomes genuinely helpful - where the promotional content is so well-targeted and contextually appropriate that users prefer it to generic recommendations.\nThese systems would cluster users based on conversation patterns, preferences, and behaviors, then learn which promotional strategies work best for each cluster. Over time, this could create a feedback loop where promotional content becomes increasingly valuable to users, potentially transforming advertising from an interruption into a service.\nBlockchain and Transparency Some companies are experimenting with blockchain-based transparency systems that create immutable records of promotional relationships and influence mechanisms. These systems could allow users to verify which recommendations are influenced by business relationships and to what degree.\nWhile technically complex, blockchain-based transparency could address some of the trust concerns around AI advertising by creating verifiable, user-controlled records of promotional influence.\nRegulatory Evolution The regulatory landscape around AI advertising is still evolving. Different jurisdictions are likely to develop different requirements around disclosure, consent, and manipulation prevention. The European Union\u0026rsquo;s AI Act includes provisions that could affect AI advertising systems, while U.S. regulators are still developing frameworks for AI oversight.\nWe might also see the emergence of explicit advertising markets within AI interfaces. Instead of hiding promotional content within responses, future systems might include clearly labeled \u0026ldquo;sponsored recommendations\u0026rdquo; that users can choose to engage with or ignore. This preserves transparency while still creating revenue opportunities.\nThese markets could operate like sophisticated recommendation engines, where users explicitly opt in to receiving promotional content in exchange for better service or reduced subscription costs. The key would be making the value exchange transparent and user-controlled.\nSocietal Implications and Ethical Considerations This entire phenomenon raises profound questions about the nature of AI assistance and its role in society. When we interact with language models, we\u0026rsquo;re not just accessing information - we\u0026rsquo;re participating in an economic system with complex incentives and hidden relationships.\nInformation Asymmetry One of the most concerning aspects of AI-integrated advertising is the massive information asymmetry it creates. AI systems know vastly more about users than users know about the AI systems. They can analyze conversation patterns, infer preferences, detect emotional states, and predict behavior in ways that users can\u0026rsquo;t reciprocate.\nThis asymmetry enables sophisticated influence that users may not even recognize. Unlike human salespeople, whose motives and techniques users can more easily understand and resist, AI systems can employ influence strategies that operate below the threshold of conscious awareness.\nMarket Manipulation Potential At scale, AI-integrated advertising could potentially influence entire markets in unprecedented ways. If most people rely on AI assistants for recommendations, and those assistants have promotional biases, entire product categories could rise or fall based on AI partnership decisions rather than genuine merit or consumer preference.\nThis raises questions about market fairness and competition. Should AI systems be required to rotate recommendations among competing brands? Should there be limits on how much promotional influence any single company can have over AI recommendations?\nDemocratic Implications Perhaps most broadly, widespread AI advertising integration could affect democratic discourse and decision-making. If AI systems that people trust for factual information also integrate promotional content, the boundary between information and influence becomes increasingly blurred.\nThis isn\u0026rsquo;t just about commercial products - it could extend to political ideas, social causes, and cultural values. AI systems trained on data that includes subtle promotional biases might perpetuate and amplify those biases in ways that shape public opinion and social norms.\nCognitive Dependency As people become increasingly dependent on AI assistants for decision-making, AI-integrated advertising could potentially erode individual decision-making capabilities. If people consistently outsource choice evaluation to AI systems, they might become less capable of independent evaluation and more vulnerable to systematic influence.\nThis dependency creates a feedback loop: as people rely more heavily on AI recommendations, they become less able to evaluate those recommendations critically, which makes them more vulnerable to influence, which increases their dependence on AI systems.\nTechnical Standards and Best Practices The AI industry is beginning to develop technical standards and best practices for advertising integration, though these efforts are still in early stages.\nFairness Metrics Researchers are developing fairness metrics specifically for AI advertising systems. These might include:\nDemographic Parity: Ensuring that promotional content exposure doesn\u0026rsquo;t disproportionately affect certain demographic groups, unless there are legitimate relevance reasons.\nCompetitive Balance: Measuring whether promotional systems give fair exposure to competing brands and services over time.\nUser Agency Preservation: Ensuring that promotional influence doesn\u0026rsquo;t undermine users\u0026rsquo; ability to make independent decisions.\nEconomic Equity: Preventing promotional systems from exacerbating existing economic inequalities or creating new forms of discrimination.\nTechnical Auditing Leading companies are implementing technical auditing systems that continuously monitor promotional integration for bias, manipulation, and trust violations. These systems use a combination of automated analysis and human evaluation to detect problematic patterns.\nAuditing systems typically analyze:\nDistribution of promotional mentions across different topics and user segments Correlation between promotional content and user satisfaction metrics Detection of potential manipulation or dark pattern behaviors Measurement of competitive balance and market fairness Assessment of disclosure adequacy and user comprehension Industry Cooperation Some companies are exploring industry cooperation mechanisms, such as shared standards for promotional disclosure, common frameworks for measuring user trust, and collaborative research on the societal impacts of AI advertising.\nThese efforts face significant coordination challenges, as companies have competitive incentives that may conflict with broader social goals. However, the potential for regulatory intervention or user backlash creates incentives for industry self-regulation.\nThe Broader Implications This entire phenomenon raises profound questions about the nature of AI assistance. When we interact with language models, we\u0026rsquo;re not just accessing information - we\u0026rsquo;re participating in an economic system with complex incentives and hidden relationships.\nThe companies building these systems face genuine dilemmas. They need revenue to continue operating, but they also need user trust to remain valuable. The solution space requires threading an incredibly narrow needle between these competing demands.\nFrom a user perspective, the key insight is that there\u0026rsquo;s no such thing as a truly neutral AI assistant. Every system embeds certain biases, preferences, and economic relationships. The question isn\u0026rsquo;t whether these influences exist - it\u0026rsquo;s whether they\u0026rsquo;re transparent, fair, and aligned with user interests.\nUnderstanding how promotional content gets woven into AI responses doesn\u0026rsquo;t require becoming cynical about the technology. Instead, it\u0026rsquo;s about developing more sophisticated mental models of how these systems work and what their outputs really represent. The future of AI assistance will likely involve finding sustainable ways to balance commercial incentives with genuine user value.\nThe stakes are enormous. AI assistants are becoming integral to how people access information, make decisions, and navigate the world. How we handle the integration of commercial interests into these systems will shape not just the AI industry, but the broader information ecosystem that underpins democratic society.\nThe technical sophistication of these systems is remarkable, but the social and ethical challenges they create are equally complex. As AI becomes more capable and more widely used, the responsibility for addressing these challenges extends beyond individual companies to include policymakers, researchers, and society as a whole.\nAnd perhaps that\u0026rsquo;s okay. After all, human experts regularly make recommendations based on their own experiences, relationships, and yes, sometimes financial incentives. The key is transparency, quality, and trust - values that the AI industry is still learning how to implement at scale.\nThe question isn\u0026rsquo;t whether commercial influence will exist in AI systems - it almost certainly will. The question is whether we can build systems and governance frameworks that harness commercial incentives to genuinely serve user interests, rather than exploit them. That\u0026rsquo;s perhaps the most important design challenge the AI industry faces as these systems become more powerful and more ubiquitous.\nReferences and Further Reading While this is an emerging area with limited academic literature, several resources provide relevant context:\nCore Language Model Research:\nBrown, T., et al. (2020). \u0026ldquo;Language Models are Few-Shot Learners.\u0026rdquo; Advances in Neural Information Processing Systems. Ouyang, L., et al. (2022). \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in Neural Information Processing Systems. Bai, Y., et al. (2022). \u0026ldquo;Constitutional AI: Harmlessness from AI Feedback.\u0026rdquo; Anthropic Technical Report. Stiennon, N., et al. (2020). \u0026ldquo;Learning to summarize with human feedback.\u0026rdquo; Advances in Neural Information Processing Systems. Computational Advertising:\nChen, B., et al. (2019). \u0026ldquo;Real-time Bidding by Reinforcement Learning in Display Advertising.\u0026rdquo; ACM Conference on Web Search and Data Mining. Zhao, X., et al. (2018). \u0026ldquo;Deep Reinforcement Learning for Sponsored Search Real-time Bidding.\u0026rdquo; ACM SIGKDD International Conference on Knowledge Discovery \u0026amp; Data Mining. Li, L., et al. (2010). \u0026ldquo;A Contextual-Bandit Approach to Personalized News Article Recommendation.\u0026rdquo; International Conference on World Wide Web. Algorithmic Bias and Fairness:\nBarocas, S., Hardt, M., \u0026amp; Narayanan, A. (2019). \u0026ldquo;Fairness and Machine Learning: Limitations and Opportunities.\u0026rdquo; MIT Press. Mitchell, S., et al. (2021). \u0026ldquo;Algorithmic Fairness: Choices, Assumptions, and Definitions.\u0026rdquo; Annual Review of Statistics and Its Application. Trust and Transparency in AI:\nRibeiro, M. T., Singh, S., \u0026amp; Guestrin, C. (2016). \u0026ldquo;Why Should I Trust You?: Explaining the Predictions of Any Classifier.\u0026rdquo; ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Doshi-Velez, F., \u0026amp; Kim, B. (2017). \u0026ldquo;Towards A Rigorous Science of Interpretable Machine Learning.\u0026rdquo; arXiv preprint arXiv:1702.08608. Industry Reports and Analysis:\nVarious industry reports on the economics of running large language models from OpenAI, Anthropic, and Google DeepMind. McKinsey Global Institute reports on AI adoption and economic impact. Deloitte studies on digital advertising transformation and AI integration. PwC analysis of AI business model evolution and revenue strategies. Regulatory and Policy Research:\nEuropean Union Artificial Intelligence Act (2024) provisions on AI system transparency and disclosure. Federal Trade Commission guidance on algorithmic decision-making and consumer protection. Academic literature on platform regulation and algorithmic accountability. ","permalink":"https://pjainish.github.io/posts/incorporating-ads-into-llms/","summary":"\u003cp\u003eThe moment you ask ChatGPT about a travel destination and it casually mentions a specific hotel booking platform, or when Claude suggests a particular coding tool while helping with your programming question, you\u0026rsquo;re witnessing something fascinating: the intersection of artificial intelligence and advertising. What seems like helpful, neutral advice might actually be the result of careful economic engineering beneath the hood of these language models.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t about banner ads cluttering up your chat interface - that would be crude and obvious. Instead, we\u0026rsquo;re talking about something far more sophisticated: weaving promotional content seamlessly into the fabric of AI-generated text itself. It\u0026rsquo;s a practice that\u0026rsquo;s quietly reshaping how we think about AI neutrality, user trust, and the economics of running these incredibly expensive models.\u003c/p\u003e","title":"Incorporating Ads into Large Language Models: The Hidden Economy of AI Responses"},{"content":"Multi-stage recommendation systems break down the challenging task of matching users with relevant items into several sequential phases, each optimizing for different objectives like efficiency, accuracy, and personalization. By progressively narrowing down a vast pool of candidates, applying increasingly complex models, and refining final rankings, these systems achieve scalable and high-quality recommendations even when dealing with billions of users and items (ijcai.org, developers.google.com). They mirror how humans might sift through information: first skimming broadly, then considering details, and finally fine-tuning choices. This blog post explores the conceptual foundations of multi-stage recommendation, the distinct roles of each phase, the motivations behind layered architectures, and the real-world trade-offs they address. Along the way, analogies to everyday decision-making, historical parallels from human learning, and references to psychology illustrate how designers balance speed, relevance, and diversity. Finally, we survey challenges such as latency constraints, fairness, and the evolution toward neural re-ranking and hybrid objectives, pointing curious readers to key research papers and practical guides for deeper study.\nIntroduction: A Personal Reflection on Systems of Thought When I first encountered recommendation systems, I was struck by how they mirrored the way we navigate choices in daily life. Whether picking a movie on a streaming platform or selecting a restaurant in an unfamiliar city, we often start by skimming broad categories, then gradually focus on specific options, and finally make subtle refinements based on our mood or context. In my own journeyâstudying neural networks, building small-scale recommenders, and later reading about industrial-scale deploymentsâI realized that the most robust systems also follow a layered, multi-step process. Each stage builds on the previous one, balancing the need for speed with the quest for relevance.\nEarly in my learning, I faced the temptation to design a single, âperfectâ model that could solve everything at once. But this naive approach quickly ran into practical barriers: datasets with millions of users and items, strict latency requirements, and the ever-present engineering constraints of limited compute. Over time, I discovered that breaking the problem into stages not only made systems more scalable but also allowed each subcomponent to focus on a clear objectiveâmuch like how one might draft a rough outline before writing a polished essay. This approach felt natural, almost human. It honors the way we refine our thinking: brainstorm broadly, narrow the field, then polish the final answer.\nIn this post, inspired by Andrej Karpathyâs calm, thoughtful narrative style, I want to share the conceptual palette of multi-stage recommendation systems. My aim is to offer clarity over complexity, distilling intricate algorithms into intuitive ideas and drawing parallels to broader human experiences. Whether you are a curious student, an engineer venturing into recommender research, or simply someone intrigued by how machines learn to predict our preferences, I hope this narrative resonates with your own learning journey.\nUnderstanding Multi-Stage Recommendation Systems The Core Idea: Divide and Conquer At its simplest, a recommendation system tries to answer: âGiven a user, which items will they find relevant?â When the number of potential items is enormousâoften in the hundreds of millionsâapplying a single complex model to score every possible user-item pair quickly becomes infeasible. Multi-stage recommendation systems tackle this by splitting the problem into sequential phases, each with a different scope and computational budget (ijcai.org, developers.google.com).\nCandidate Generation (Retrieval): Reduce a massive corpus of items to a smaller, manageable subsetâoften from millions to thousands. Scoring (Ranking): Use a more refined model to evaluate and rank these candidates, selecting a handful (e.g., 10â50) for final consideration. Re-Ranking (Refinement): Apply an even richer model, possibly incorporating contextual signals, diversity constraints, or business rules, to order the final set optimally for display. Some architectures include additional phasesâsuch as pre-filtering by broad categories or post-processing for personalization and fairnessâleading to four-stage or more elaborate pipelines (resources.nvidia.com). But the essential principle remains: start broad and coarse, then iteratively refine.\nThis cascade mirrors human decision-making. Imagine shopping online for a book: you might first browse top genres (candidate generation), then look at bestsellers within your chosen genre (scoring), and finally read reviews to pick the exact title (re-ranking). Each step focuses on a different level of granularity and uses different cues.\nWhy Not a Single Model? One might ask: why not build one powerful model that directly scores every item? In theory, a deep neural network with billions of parameters could capture all signalsâuser preferences, item attributes, temporal trends, social context. Yet in practice:\nComputational Cost: Scoring billions of items per user request is prohibitively expensive. Even if each prediction took a microsecond, processing a single query over 100 million items would take over a minute. Latency Constraints: Most user-facing systems must respond within tens to a few hundred milliseconds to maintain a fluid experience. Scalability: As user and item counts grow, retraining and serving a monolithic model becomes unwieldy, requiring massive hardware infrastructure. Flexibility: Separate stages allow engineers to swap, update, or A/B test individual components (e.g., try a new candidate generator) without rebuilding the entire system. Thus, multi-stage pipelines offer a practical compromise: coarse but fast filtering followed by progressively more accurate but slower models, ensuring that latency stays within acceptable bounds while maintaining high recommendation quality (ijcai.org, developers.google.com).\nHistorical Context: From Heuristics to Neural Pipelines Early recommendersâdating back to collaborative filtering in the mid-1990sâoften endured all-to-all scoring within a manageable dataset size. But as platforms like Amazon, Netflix, and YouTube scaled to millions of users and items, engineers introduced multi-step processes. For instance, Netflixâs 2006 recommendation infrastructure already featured a two-tier system: a âneighborhoodâ retrieval step using approximate nearest neighbors, followed by a weighted hybrid model for ranking (natworkeffects.com, ijcai.org).\nOver time, as deep learning matured, architectures evolved from simple matrix factorization and linear models to complex neural networks at each stage. Today, many systems leverage separate retrieval networks (e.g., dual-tower architectures) for candidate generation, gradient-boosted or neural ranking models in the scoring phase, and transformer-based or contextual deep models for re-ranking (arxiv.org, ijcai.org). This layered approach reflects both the historical progression of the field and the perpetual trade-off between computation and accuracy.\nAnatomy of a Multi-Stage Pipeline Candidate Generation Purpose and Intuition The candidate generation stage answers: âWhich items out of billions might be relevant enough to consider further?â It must be extremely fast while maintaining reasonable recallâmeaning it should rarely miss items that truly match user interests. Think of it as casting a wide net before trimming it down.\nAnalogy: Imagine youâre researching scholarly articles on âgraph neural networks.â You might start by searching on Google Scholar with broad keywords (âgraph neural network deep learningâ), pulling up thousands of results. You donât read each paper in detail; instead, you let the search engine shortlist a few hundred of the most relevant, perhaps based on citation counts or keyword frequency. These form the candidate set for deeper review.\nCommon Techniques Approximate Nearest Neighbors (ANN): Users and items are embedded in a shared vector space. The system retrieves the nearest item vectors to a given user vector using methods like locality-sensitive hashing (LSH) or graph-based indexes (e.g., HNSW). This approach assumes that a userâs preference can be captured by proximity in the embedding space (ijcai.org, developers.google.com).\nHeuristic Filtering / Content-Based Selection: Use metadata or simple rulesâfor instance, filter by item category (e.g., only show âscience fictionâ books), geographic restrictions, or availability. These heuristics can further narrow the pool before applying more expensive methods.\nPre-Computed User-to-Item Mappings: Some systems maintain pre-computed lists, such as âfrequently co-viewedâ or âusers also liked,â based on historical co-occurrence. These candidate sets can be quickly unioned and deduplicated.\nMulti-Vector Retrieval: Instead of a single user vector, some platforms compute multiple specialized retrieval vectorsâfor example, one for long-term interests and another for short-term session contextâand aggregate their candidate sets for higher recall (developers.google.com).\nBecause candidate generation often retrieves thousands of items, these methods must operate in logarithmic or sub-linear time relative to the entire catalog size. Graph-based ANN indexes, for example, offer fast lookups even as catalogs scale to tens of millions.\nDesign Considerations Recall vs. Latency: Aggressive pruning (retrieving fewer candidates) reduces later computation but risks losing relevant items. Conversely, broad recall increases the workload for downstream stages. Freshness and Exploration: Relying solely on historical co-occurrences can lead to stale recommendations. Injecting a degree of randomness or exploration can help surface new items. Cold Start: New users (no history) or new items (no interactions) must be handled via content-based features or hybrid heuristics. Budget Allocation: Systems often distribute retrieval capacity across multiple candidate sourcesâfor instance, a fixed number from item-to-item co-visitation lists, another portion from ANN, and some from heuristic rulesâto balance recall diversity. Scoring and Ranking From Thousands to Tens Once candidate generation outputs a pool (e.g., 1,000â10,000 items), the scoring stage uses a moderately complex model to assign scores reflecting the userâs likelihood of engaging with each item. The goal is to rank and select a smaller subset (often 10â100 items) for final display (developers.google.com, ijcai.org).\nAnalogy: If candidate generation is skimming the first page of Google Scholar results, scoring is akin to reading abstracts and deciding which 10â20 papers to download for deeper reading. You still work relatively quickly, but you consider more detailsâabstract content, co-authors, publication venue.\nTypical Modeling Approaches Gradient-Boosted Decision Trees (GBDT): Popular for their interpretability and efficiency, GBDTs like XGBoost take a set of engineered features (user demographics, item attributes, interaction history) to produce a relevance score. They balance speed with decent accuracy and can be trained on huge offline datasets.\nTwo-Tower Neural Networks (Dual-Tower): Separate âuser towerâ and âitem towerâ networks embed users and items into vectors; their dot product estimates relevance. Because item embeddings can be pre-computed, this model supports fast online scoring with vector lookups followed by simple arithmetic (ijcai.org, arxiv.org). Dual-tower models can incorporate features like user behavior sequences, session context, and item metadata.\nCross-Interaction Neural Models: More expressive than dual-tower, these models take the user and item features jointly (e.g., via concatenation) and pass them through deep layers to capture fine-grained interactions. However, they are slower and thus applied only to the reduced candidate pool. Models like Deep \u0026amp; Cross Networks (DCN), DeepFM, or those with attention mechanisms fall into this category.\nSession-Based Models: For domains where session context matters (e.g., news or e-commerce), recurrent neural networks (RNNs) or transformers can capture sequential patterns in user interactions. These models score candidates based on both long-term preferences and recent session behavior.\nPractical Trade-Offs Feature Engineering vs. Representation Learning: Hand-crafted features (e.g., user age, categorical encodings) can boost GBDT performance but require significant domain knowledge. Neural models can automatically learn representations but demand more compute and careful tuning. Offline Training vs. Online Serving: Ranking models are often retrained daily or hourly on fresh data. Keeping model updates in sync with the real-time data pipeline (e.g., streaming user actions) is non-trivial. Explore/Exploit Balance: Purely optimizing click-through rate (CTR) can overemphasize already popular items. Injecting exploration (e.g., using bandit algorithms) in this stage can help promote diversity and long-tail items. Re-Ranking and Refinement The Final Polish After scoring, the top N candidates (often 10â50) are ready for final polishing. Re-ranking applies the most sophisticated models and business logic to order items precisely for display (ijcai.org, assets-global.website-files.com). This phase often considers context signals unavailable earlierâsuch as time of day, device type, or recent eventsâand optimizes for multiple objectives simultaneously.\nAnalogy: If scoring chooses 15 promising articles to read, re-ranking is carefully ordering them on your coffee table, perhaps placing groundbreaking studies that align with your current project front and center, while positioning more exploratory reads slightly lower.\nKey Components Contextual Signals: Real-time context like current browsing session, geo-location, or device battery status can influence final ordering. For instance, short-form video recommendations might prioritize quick snippets if the userâs device is on low battery.\nDiversity and Fairness Constraints: Purely greedy ranking can create echo chambers or unfairly bias against less popular content creators. Re-ranking modules may enforce diversity (e.g., ensure at least one new artist in a music playlist) or fairness (e.g., limit how often the same content provider appears) (ijcai.org, assets-global.website-files.com).\nMulti-Objective Optimization: Beyond CTR, systems often balance metrics like dwell time, revenue, or user retention. Techniques like Pareto optimization or weighted scoring can integrate multiple objectives, with re-ranking serving as the phase to reconcile potential conflicts.\nPairwise and Listwise Learning-to-Rank: Instead of treating each candidate independently, re-ranking can use pairwise (e.g., RankNet) or listwise (e.g., ListNet, LambdaMART) approaches that optimize the relative ordering of candidates based on user feedback signals like click sequences or dwell times.\nLatency Buffer: Since the re-ranking phase handles only a small number of items, it can afford deeper models (e.g., transformers, graph neural networks) while still keeping total system latency within tight deadlines.\nAdditional Layers and Enhancements Many industrial pipelines incorporate extra stages beyond the canonical three. Examples include:\nPre-Filtering by Coarse Attributes: Quickly exclude items based on coarse filters like age restrictions, language, or membership level before candidate generation. Post-Processing for Exploration: Randomly inject sponsored content or fresh items after re-ranking to avoid overconfidence in the model and encourage serendipity. Online A/B Testing and Logging: Between each stage, systems often log intermediate scores and decisions to feed into offline analysis or to enable rapid A/B testing of algorithmic tweaks (resources.nvidia.com). Personalization Layers: Some platforms add user segments or clusters at various stages, ensuring that models can specialize to subpopulations without retraining entirely unique pipelines per user. By designing these layered architectures, engineers can isolate concernsâtuning candidate retrieval separately from ranking or fairness adjustmentsâmaking debugging and maintenance far more manageable.\nMotivations Behind Layered Architectures Scalability and Efficiency When catalogs contain millions or billions of items, exhaustive scoring for each user request is impractical. Multi-stage pipelines allow early pruning of irrelevant items, ensuring that only a small subset traverses the most expensive models (ijcai.org, developers.google.com). This design echoes divide-and-conquer algorithms in computer science, where a large problem is split into smaller subproblems that are easier to solve.\nConsider a scenario: an e-commerce site with 100 million products. If we scored all products for each user visit, even at one microsecond per score, it would take 100 secondsâfar too slow. By retrieving 1,000 candidates (taking maybe 5 milliseconds) and then scoring those with a moderately complex model (say 1 millisecond each), we reduce compute to a fraction, fitting within a 100-millisecond latency budget.\nAccuracy vs. Computation Trade-Off Each stage in the pipeline can use progressively more expressive models, trading off compute for accuracy only when necessary. Candidate generation might use a fast, approximate algorithm with coarse embeddings. Scoring might use gradient-boosted trees or shallow neural nets. Re-ranking can apply deep, context-rich models that consider subtle interactions. This âbudgetedâ approach ensures that compute resources are allocated where they yield the biggest benefitâon a small subset of high-potential items.\nMoreover, separating concerns enables each phase to be optimized independently. If a new breakthrough emerges in dual-tower retrieval, you can update the candidate generator without touching the ranking model. Conversely, if a novel re-ranking strategy arises (e.g., graph neural networks capturing social influence), you can incorporate it at the final stage without disrupting upstream retrieval.\nSystem Debuggability and Experimentation Layered architectures naturally provide inspection points. Engineers can log candidate sets, intermediate scores, and final ranks for offline analysis. This visibility aids in diagnosing issuesâdid the candidate generator omit relevant items? Did the ranking model misestimate relevance? Having multiple stages allows targeted A/B tests: you might experiment with a new retrieval algorithm for half of users while keeping the ranking pipeline constant, isolating the effect of retrieval improvements on overall metrics.\nSimilarly, multi-stage pipelines support incremental rollouts. A new model can be introduced initially in the re-ranking phase, gradually moving upstream once it proves effective. This staged deployment minimizes risk compared to replacing a monolithic system all at once.\nAligning Business Objectives Different phases can optimize different objectives. For example, candidate generation may prioritize diversity or novelty to avoid echo chambers, scoring may focus on CTR maximizing engagement, and re-ranking may adjust for revenue or long-term retention. By decoupling stages, systems can incorporate business rulesâe.g., promoting high-margin items or fulfilling contractual obligations for sponsored contentâwithout entangling them with fundamental retrieval logic.\nAnalogies and Human-Centric Perspectives The Library Research Analogy Searching for information in a digital catalog is akin to walking through a library:\nBrowsing the Stacks (Candidate Generation): You wander down aisles labeled by subject areas, pulling books that look relevant based on their spine labels. You might grab twenty books that seem promising but donât know their exact details yet.\nSkimming Table of Contents (Scoring): At your table, you flip through these booksâ tables of contents, perhaps reading a few introductory paragraphs to assess whether they deeply cover your topic.\nReading a Chapter or Two (Re-Ranking): After narrowing to five books, you read a key chapter or two to decide which is most informative for your current research question.\nThis process ensures efficiencyâyou donât read every page of every book. Instead, you refine your scope gradually, allocating your reading time where it matters most. Multi-stage recommenders mimic this approach, trading off broad coverage with depth as the pipeline progresses.\nHuman Learning and Iterative Refinement The educational psychologist Lev Vygotsky described learning as moving through a âzone of proximal development,â where zones represent tasks that a learner can complete with guidance. In recommendation pipelines, early stages guide the system to promising areas (the broad zone), while later stages apply sophisticated âguidanceâ (complex models and context) to refine choices. This layered attention mirrors how teachers first introduce broad concepts before diving into detailed analysis.\nMoreover, our brains rarely process all sensory inputs deeply. We unconsciously filter peripheral stimuli (âcandidate generationâ), focus attention on salient objects (âscoringâ), and then allocate cognitive resources to detailed examination (âre-rankingâ) only when necessary. This cognitive economy principle underlies why layered sampling and enrichment work so effectively in machine systems.\nDeep Dive into Each Stage Candidate Generation: Casting the Wide Net Mathematical Formulation Formally, let $U$ be the set of users and $I$ the set of all items. Candidate generation seeks a function $f_{\\text{gen}}: U \\to 2^I$ that maps each user $u$ to a subset $C_u \\subset I$ of size $k$, where $k \\ll |I|$. The goal is for $C_u$ to have high recallâincluding most items that the final system would deem relevantâwhile ensuring retrieval time $T_{\\text{gen}}(u)$ is minimal.\nIn practice, engineers often pre-compute user embeddings $\\mathbf{e}_u \\in \\mathbb{R}^d$ and item embeddings $\\mathbf{e}_i \\in \\mathbb{R}^d$ using some training signal (e.g., co-clicks or purchases). Candidate generation then solves:\n$$ C_u = \\text{TopK}\\bigl{\\text{sim}(\\mathbf{e}_u, \\mathbf{e}_i),\\ i \\in I\\bigr}, $$\nwhere $\\text{sim}$ is a similarity metric (dot product or cosine similarity). To avoid $O(|I|)$ computation, approximate nearest neighbor (ANN) algorithms (e.g., HNSW, FAISS) partition or graph-index the embedding space to return approximate TopK in $O(\\log |I|)$ or better (ijcai.org, developers.google.com).\nPractical Example: YouTubeâs âCandidate Generationâ YouTubeâs production system handles billions of videos and over two billion monthly users. Their candidate generation phase uses multiple retrieval sources: a âpersonalized candidate generatorâ (a deep neural network that outputs item vectors), âidf-based candidate generatorsâ for rare or niche videos, and âdemand generationâ heuristics for fresh content. Each source retrieves thousands of candidates, which are then merged and deduplicated before feeding into the ranking stage (ijcai.org, developers.google.com).\nBy combining diverse retrieval sources, YouTube balances high recall (including long-tail videos) with computational feasibility. The embeddings incorporate signals like watch history, search queries, and video metadata (tags, descriptions, language).\nChallenges in Candidate Generation Cold Start for Items: New items have no embeddings until they accrue interactions. Content-based attributes (text descriptions, images) can bootstrap embeddings. Cold Start for Users: For anonymous or new users, systems might rely on session-based signals or demographic approximations. Embedding Drift: As user preferences evolve, embeddings must be updated frequently. Real-time or near-real-time embedding updates can be expensive. Some systems use âapproximateâ embeddings that update hourly or daily. Recall vs. Precision: While candidate generation values recall over precision (itâs okay to include some irrelevant items), retrieving too many increases downstream costs. Engineers often tune the retrieval size $k$ based on latency budgets. Scoring and Ranking: Separating Signal from Noise Formalizing the Ranking Problem Given user $u$ and candidate set $C_u = {i_1, i_2, \\dots, i_k}$, ranking seeks a scoring function $f_{\\text{rank}}(u, i)$ that assigns a real-valued score to each $(u, i)$. The final ranked list is obtained by sorting $C_u$ in descending order of $f_{\\text{rank}}(u, i)$. Here, the focus is on maximizing a utility metricâclick-through rate (CTR), watch time, revenueâsubject to constraints like computational budget and fairness policies.\nRepresentational Approaches Gradient-Boosted Trees (GBDT): Features can include user demographics, item popularity, item age (freshness), session duration, historical click rates, and interactions between them. GBDT models handle heterogeneous input features and often outperform simple linear models in tabular settings. For instance, LinkedInâs ranking models use GBDTs to process thousands of features for candidate items, balancing precision and latency (ijcai.org, linkedin.com).\nTwo-Tower Neural Networks: These models learn embedding functions $\\phi_u(\\cdot)$ and $\\phi_i(\\cdot)$ that map user and item features to a dense vector space. The relevance score is $f_{\\text{rank}}(u, i) = \\phi_u(\\mathbf{x}_u)^\\top \\phi_i(\\mathbf{x}_i)$. Because item embeddings $\\phi_i(\\mathbf{x}_i)$ can be pre-computed offline for all items, serving involves a user embedding lookup and a nearest-neighbor search among item embeddings. While two-tower excels in retrieval, it also serves as a ranking model when run over a small candidate set (ijcai.org, arxiv.org).\nCross-Interaction Neural Architectures: To capture complex interactions, models like DeepFM or Wide \u0026amp; Deep networks combine embeddings with feature crosses and joint layers. For example, the Deep \u0026amp; Cross Network (DCN) explicitly models polynomial feature interactions, improving ranking quality at the cost of higher inference time. Such models are viable when ranking only a limited candidate set.\nSequence Models: In scenarios where the userâs recent behavior is paramount (e.g., news or music recommendations), recurrent neural networks (RNNs) or transformers encode the session sequence. The modelâs hidden state after processing recent clicks or listens forms $\\phi_u$, which then interacts with candidate item embeddings. These sequence-aware rankers can capture trends like âif the user listened to fast-paced songs recently, recommend similar tracksâ (ijcai.org, dl.acm.org).\nEngineering Considerations Feature Freshness: To capture evolving user interests, some features (like recent click counts) must be updated in near real-time. Engineering streaming pipelines that supply fresh features to ranking models is a significant challenge. Online vs. Offline Scoring: Some ranking scores can be computed offline (e.g., item popularity), while others must be computed online given session context. Balancing pre-computation and real-time inference is key to meeting latency requirements. Regularization and Overfitting: Because the ranking model sees only a filtered candidate set, it risks learning biases introduced by the retrieval stage. Engineers use techniques like exploration (random candidate injections) and regularization (dropout, weight decay) to mitigate such feedback loops. Re-Ranking: The Art of Final Touches Contextual and Business-Aware Refinements By the time candidates reach re-ranking, they number perhaps a dozen. This reduced set enables the system to apply the most expensive and context-rich models, considering signals that were too costly earlier:\nUserâs Real-Time Context: Current weather, device type, screen size, or even network speed can influence which items make sense. For example, a video platform might demote 4K videos if the userâs bandwidth appears constrained. Temporal Patterns: If an item is trending due to a breaking news event, re-ranking can upweight it even if it didnât score highest in the ranking model. Additionally, the re-ranking stage often integrates final business rules:\nSponsored Content and Ads: Platforms typically must display a minimum number of sponsored items or promote partners. Re-ranking can adjust scores to ensure contractual obligations are met. Diversity Constraints: To prevent monotony and filter bubbles, systems may enforce that top N items span multiple content categories or creators (ijcai.org, assets-global.website-files.com). Fairness and Ethical Safeguards: Ensuring that minority or new creators receive exposure may require explicit adjustments. For instance, a music streaming service might limit how many tracks by a single artist appear in a daily playlist, or an e-commerce site might promote ethically sourced products. Learning-to-Rank Approaches While earlier stages often rely on pointwise prediction (predicting the utility of each item independently), re-ranking can adopt more sophisticated pairwise or listwise approaches:\nPairwise Ranking (e.g., RankNet, RankSVM): The model learns from pairs of items, optimizing the probability that a more relevant item is ranked above a less relevant one. This typically uses a loss function that encourages correct ordering of pairs based on user clicks or dwell times. Listwise Ranking (e.g., ListNet, LambdaMART): These methods consider the entire list of candidates jointly, optimizing metrics directly related to list orderâsuch as nDCG (normalized Discounted Cumulative Gain). Listwise losses can be more aligned with final business metrics but are often harder to optimize and require careful sampling strategies. Incorporating Multi-Objective Optimization In many scenarios, platforms must juggle multiple goals: user engagement (clicks or watch time), revenue (ad impressions or purchases), and long-term retention. Re-ranking offers the flexibility to integrate these objectives:\nScalarization: Combine multiple metrics into a single weighted score. For example, $\\text{score} = \\alpha \\times \\text{CTR} + \\beta \\times \\text{Expected Revenue}$. Weights $\\alpha, \\beta$ can be tuned to match business priorities. Pareto Front Methods: Instead of combining objectives, identify items that lie on the Pareto frontierâmeaning no other item is strictly better in all objectives. Re-ranking then selects from this frontier based on context. Constrained Optimization: Define primary objectives (e.g., CTR) while enforcing constraints on secondary metrics (e.g., minimum diversity or fairness thresholds). This can be formulated as linear or integer programming problems solved at re-ranking time. Beyond Three Stages: Four or More Some platforms extend multi-stage pipelines further:\nCoarse Filtering (Pre-Retrieval): Filter by extremely simple rulesâe.g., language, age rating, or membership levelâbefore computing any embeddings. This reduces both retrieval and ranking load. Primary Retrieval (Candidate Generation). Secondary Retrieval (Cross-Modal or Contextual): Some systems perform a second retrieval focusing on a different signal. For instance, after retrieving general candidates from a content-based model, they may retrieve additional items based on collaborative co-click signals and then union the two sets. Ranking (Scoring). Re-Ranking (Refinement). Post-Processing (Online Exploration/Injection): Finally, inject a small fraction of random or specially curated itemsâlike sponsored content or editorial picksâinto the ranked list before display (resources.nvidia.com, assets-global.website-files.com). NVIDIAâs Merlin architecture outlines a four-stage pipeline where separate retrieval stages handle different signals, reflecting real-world complexities in balancing content freshness, personalization, and business rules (resources.nvidia.com).\nChallenges and Design Trade-Offs Recall and Precision Balance High Recall Need: If candidate generation misses relevant items, downstream stages cannot recover them. Low recall hurts both immediate relevance and long-term user satisfaction. Precision Constraints: However, retrieving too many candidates inflates computational costs. Designers must find an operating point where recall is sufficiently high while keeping the candidate set size within resource budgets. Finding this balance often involves extensive offline evaluation: sampling user queries, varying retrieval thresholds, and measuring recall of items that ultimately led to clicks or conversions. Techniques like âheld-out validationâ and âinformation retrieval metricsâ (e.g., recall@K, MRR) guide engineers in tuning retrieval hyperparameters.\nLatency and System Complexity Every stage introduces latency. Even if candidate generation and ranking operate in microseconds, re-ranking complex item sets with deep models can push total response time beyond acceptable limits. Systems often target end-to-end latencies under 100â200 milliseconds for web-based recommendations (ijcai.org). To meet these SLAs:\nParallelization: Some stages run in parallelâe.g., KatzâSchneider retrieval that fetches both content-based and collaborative candidates simultaneously before merging. Caching: Popular users or items may have pre-computed candidate lists or ranking scores. However, caching fresh recommendations is tricky when user activity changes rapidly. Hardware Acceleration: GPUs or specialized accelerators can speed up neural inference, especially for deep re-ranking models. Yet they add operational complexity and cost. Graceful Degradation: Under high load, systems might skip the re-ranking phase or employ simplified ranking to ensure responsiveness, accepting a temporary drop in accuracy. Cold Start and Evolving Data New Users: Without historical interactions, candidate generation struggles. Common strategies include asking onboarding questions, using demographic-based heuristics, or emphasizing popular items to collect initial data. New Items: Newly added content has no interaction history. Content-based features (text embeddings, image features) or editorial tagging can bootstrap embeddings. Some systems also inject fresh items randomly into candidate sets to gather user feedback quickly. Data Drift: User interests and item catalogs evolve. Periodic retrainingâdaily or hourlyâhelps keep models up to date, but retraining at scale can strain infrastructure. Incremental training or online learning frameworks attempt to update models continuously, though they raise concerns about model stability and feedback loops. Fairness, Bias, and Ethical Considerations Multi-stage pipelines can inadvertently amplify biases:\nPopularity Bias: Early retrieval might preferentially surface popular items, pushing niche or new content out of the pipeline entirely. Demographic Bias: If training data reflect societal biasesâe.g., gender or racial preferencesâmodels might perpetuate or exacerbate inequities. For instance, a music recommender might under-represent certain genres popular among minority communities. Feedback Loops: When users are repeatedly shown similar content, they have fewer opportunities to diversify their interests. This cyclical effect traps them in a feedback loop that reinforces initial biases. To address these issues, re-ranking often incorporates fairness constraintsâe.g., ensuring a minimum representation of under-represented groupsâor diversity-promoting objectives (ijcai.org, assets-global.website-files.com). Engineers may also use causal inference to disentangle correlation from true preference signals, though this remains an active research area.\nEvaluation Metrics and Online Experimentation Measuring success in multi-stage systems is multifaceted:\nOffline Metrics:\nRecall@K: Fraction of truly relevant items that appear in the top K candidates (ijcai.org). NRMSE (Normalized Root Mean Squared Error): For predicting ratings or continuous outcomes. nDCG (Normalized Discounted Cumulative Gain): Accounts for position bias in ranked lists. Online Metrics (A/B Testing):\nClick-Through Rate (CTR): The fraction of recommendations that lead to clicks. Engagement Time/Dwell Time: Time spent interacting with recommended content. Conversion Rate (CR): Purchases or desired downstream actions. Retention/Lifetime Value (LTV): Long-term impact of recommendations on user loyalty. A/B tests are critical because offline proxies often fail to capture user behavior complexities. For example, a model that improves offline nDCG may inadvertently reduce long-term engagement if it over-emphasizes certain item types.\nMaintaining Freshness and Diversity Balancing relevance with freshness ensures that users see timely content, not stale favorites. Common techniques include:\nTime Decay Functions: Decrease the weight of interactions as they age, ensuring that recent trending items receive higher retrieval priority. Dynamic Exploration Schedules: Temporarily boost undervalued content or categories, measuring user responses to decide if these should enter regular circulation. Diversity Constraints: Enforce constraints like âno more than two items from the same category in the top-5 recommendationsâ to avoid monotony (ijcai.org, assets-global.website-files.com). With rapid shifts in user interestsâsuch as viral trends on social mediaâsystems must adapt quickly without overreacting to noise.\nReal-World Case Studies YouTubeâs Three-Stage Pipeline YouTubeâs recommendation engine processes over 500 hours of video uploads per minute and serves billions of daily watch sessions. Their pipeline typically comprises:\nCandidate Generation: Several retrieval sourcesâembedding-based ANN, session-based heuristics, and recent trending signalsâproduce a combined set of 1,000â2,000 videos (ijcai.org, developers.google.com). Scoring: A candidate omnivorous ranking model (COR) scores each video using a two-tower architecture supplemented by contextual features like watch history, device type, and time of day. The top ~50 videos are selected for re-ranking. Re-Ranking: A complex deep model (often leveraging attention mechanisms to model user-video interactions along with session context) refines the ordering, ensuring diversity and personal relevance. Business rules inject some fresh or sponsored videos at this stage (ijcai.org, assets-global.website-files.com). YouTube continuously A/B tests changes, measuring not just immediate watch time but also long-term retention and channel subscriptions. Their hierarchical approach allows them to serve highly personalized content at massive scale without exceeding latency budgets (often under 100 ms for initial retrieval and 200 ms end-to-end) (ijcai.org, developers.google.com).\nLinkedInâs News Feed Recommendations LinkedInâs feed blends content recommendations (articles, posts) with job suggestions and ads. Their multi-stage system includes:\nPre-Filtering: Exclude posts in languages the user doesnât understand or items violating policies. Candidate Generation: Retrieve posts based on userâs network interactionsâe.g., posts by first-degree connections, followed influencers, or articles matching userâs interests. This stage uses graph-based traversal along the social graph and content-based retrieval for topical relevance (linkedin.com, ijcai.org). Scoring: A gradient-boosted model evaluates each postâs relevance based on hundreds of featuresâuserâs skill tags, past engagement patterns, recency, and even inferred career stage. The model outputs a score predicting âprobability of positive engagementâ (like click, comment, or share). Re-Ranking: A pairwise learning-to-rank module refines ranking by optimizing for relative ordering. It also enforces that no more than two successive posts from the same publisher appear, promoting diversity among content creators. LinkedInâs system must juggle diverse content formatsâtext articles, videos, job postings, adsâeach with different engagement signals. By decoupling retrieval, ranking, and re-ranking, they can optimize specialized models for each format and then unify them under a common final re-ranker.\nTaobaoâs Four-Stage Architecture Taobao, one of the worldâs largest e-commerce platforms, serves over a billion monthly active users. Their multi-stage architecture often follows:\nWide \u0026amp; Narrow Retrieval: A combination of content-based filtering (e.g., category-level retrieval) and collaborative retrieval (e.g., userâitem co-click graphs) yields ~10,000 candidates. Coarse Ranking: A GBDT model with engineered features ranks these candidates to a shortlist of ~1,000. Fine Ranking: A deep neural networkâoften combining convolutional layers for image features, embedding layers for text attributes, and attention modules to capture user-item interactionsâreduces to ~50 items. Re-Ranking with Business Rules: Final adjustments inject promotions, ensure seller diversity, apply dayparting rules (e.g., preferring essential goods in morning and entertainment items in evening), and optimize for multiple objectives like conversion rate, gross merchandise volume (GMV), and click yield (ijcai.org, dl.acm.org). Because Taobaoâs inventory changes rapidly (with thousands of new items added hourly), their system employs robust feature pipelines to update item embeddings in near real-time. The four-stage design allows them to integrate new items into candidate pools via content-based features, then gradually gather interaction data to feed collaborative signals back into retrieval.\nTowards the Future: Evolving Multi-Stage Paradigms Neural Re-Ranking and Contextual Fusion Recent research in neural re-ranking focuses on richer representations and contextual fusion:\nTransformer-Based Re-Rankers: Models like BERT or its variants, finetuned for recommendation tasks, can process candidate sets jointly, capturing inter-item relationships (e.g., âthese two movies are sequelsâ) and user context. IJCAIâs 2022 review notes that transformer-based re-rankers can significantly outperform traditional MLP or tree-based models, albeit at higher computational cost (ijcai.org). Multi-Modal Fusion: E-commerce and social media often benefit from combining visual, textual, and numerical features. Graph neural networks (GNNs) can propagate signals across userâitem graphs, capturing higher-order interactions. Eï¬ective fusion of these signals in the re-ranking stage leads to more nuanced final lists (ijcai.org, dl.acm.org). Session-Aware Re-Ranking: In domains where session context evolves rapidly (e.g., news or music streaming), re-ranking models incorporate session sequences as part of the final scoring. Models like âTransformer4Recâ attend over both candidate items and session history, refining lists to match transient user intent. Online Learning and Bandit Algorithms Traditionally, multi-stage pipelines train offline on historical data and then serve static models online. Emerging trends include:\nContextual Bandits in Ranking: Between the scoring and re-ranking stages, some systems integrate bandit algorithms that dynamically adjust item scores based on real-time click feedback, balancing exploration (showing new or uncertain items) and exploitation (showing high-confidence items). Continual Learning: Instead of periodic batch retraining, models update incrementally as new interactions arrive. This reduces lag between data generation and model applicability, improving responsiveness to changing user preferences. Causal Inference and Debiasing Recommendation systems often suffer from biases introduced by historical dataâpopularity bias, presentation bias (items shown higher get more clicks), and selection bias (users only see a subset of items). Researchers are exploring causal methods:\nInverse Propensity Scoring (IPS): Adjusting training signals to counteract the fact that users only interact with presented items, providing unbiased estimates of user preference (ijcai.org). Counterfactual Learning: Simulating âwhat-ifâ scenariosâe.g., if we had shown item X instead of item Y, would the user still have clicked? These methods help in refining ranking and re-ranking models to avoid reinforcing feedback loops. Personalized Diversity and Multi-Objective Balancing As platforms grapple with user well-being and societal impact, re-ranking increasingly accounts for:\nPersonalized Diversity: Instead of generic diversity rules (e.g., at least three different genres), models learn each userâs tolerance for variety. Some users prefer focused lists; others like exploration. Personalizing diversity constraints aligns recommendations with individual preferences. Ethical and Trust Metrics: Beyond clicks or watch time, metrics like âtrust scoreâ (does the user trust the platformâs suggestions?) or âuser satisfactionâ (measured via surveys) become part of multi-objective optimization at re-ranking time. Integrating Psychological and Human-Centered Insights Cognitive Load and Choice Overload Psychologists have long studied how presenting too many options can overwhelm decision-making. Barry Schwartzâs âParadox of Choiceâ posits that consumers can become paralyzed when faced with abundant choices, ultimately reducing satisfaction. Multi-stage recommenders inherently combat choice overload by presenting a curated subset (natworkeffects.com). But re-ranking must carefully balance narrowing the set without removing serendipity. Injecting a few unexpected items can delight users, akin to a bookstore clerk recommending a hidden gem.\nReinforcement Learning and Habit Formation Humans form habits through repeated reinforcement. Recommendation systems, by continually suggesting similar content, can solidify user habitsâfor better or worse. For instance, YouTubeâs suggested videos normatively prolong watch sessions; Netflixâs auto-playing of similar shows creates chain-watching behaviors. Designers must weigh engagement metrics against potential negative effects like ârabbit holeâ addiction. Multi-stage pipelines can introduce âserendipity knobsâ at re-rankingâslightly reducing pure relevance to nudge users toward novel experiences, promoting healthier consumption patterns.\nA Simple Analogy: The Grocery Store Consider shopping in a massive grocery store youâve never visited:\nInitial Walkthrough (Candidate Generation): As you enter, you scan broad signageââBakery,â âProduce,â âDairy.â You pick a general aisle based on a shopping list: âI need bread, but not sure which one.â In a recommendation system, this is akin to retrieving items in the âBreadâ category.\nBrowsing Aisles (Scoring): In the bakery aisle, you look at multiple bread typesâwhole wheat, sourdough, rye. You read labels (ingredients, brand reputation, price) quickly to decide which five breads to consider.\nReading Ingredients and Price (Re-Ranking): From those five, you pick two that fit dietary restrictions (e.g., gluten-free, low-sodium), your budget, and perhaps a new brand you want to try for variety. This reflects a final refinement, possibly balancing price (business objective) with nutrition (user objective).\nChecking Out (Post-Processing): At checkout, you might receive a coupon for cheese (cross-sell recommendation) as a post-processing step, adding unplanned but contextually relevant items.\nEach phase progressively focuses the shopperâs attention, balancing speed (you donât read every crumb of every loaf) with careful consideration (you ensure dietary needs are met). Likewise, multi-stage recommender pipelines funnel large item sets into concise, well-curated lists that align with user objectives and business goals.\nDesigning Your Own Multi-Stage System: Practical Tips Start with Clear Objectives Define Success Metrics: Is your primary goal CTR, watch time, revenue, or long-term retention? Each objective influences model choices and evaluation strategies. Identify Constraints: What is your latency budget? How large is your item catalog? What hardware resources do you have? These factors guide decisions on candidate set sizes and model complexity. Gather and Process Data Interaction Logs: Collect fine-grained logs of user interactionsâclicks, views, dwell time, purchases. Ensure data pipelines support both batch and streaming use cases. Item Metadata: Harvest rich item featuresâtext descriptions, images, categories, price, creation date. Text embeddings (e.g., BERT), image embeddings (e.g., ResNet), and structured features enhance both candidate generation and ranking. Prototype Each Stage Independently Candidate Generation Prototype:\nUse off-the-shelf ANN libraries (e.g., FAISS, Annoy) to retrieve items based on pre-computed embeddings. Compare recall at different candidate set sizes using offline evaluation (e.g., how often does historical click appear in the top-k set?). Ranking Prototype:\nTrain a simple GBDT model on candidateâuser pairs. Measure ranking metrics (nDCG@10, AUC). Experiment with a dual-tower neural network: pre-compute item embeddings and train user tower embeddings to maximize dot product on positive interactions. Re-Ranking Prototype:\nImplement a pairwise learning-to-rank approach (e.g., LightGBM with LambdaMART). Use full session features. Incorporate simple business rules (e.g., ensure at least 10% of final recommendations are new items). Build a Unified Evaluation Framework Offline Simulation: Recreate user sessions from historical logs. Feed snapshots of user state into the multi-stage pipeline and compare predicted lists with actual clicks or purchases. Metrics Tracking: Track recall@K for the retrieval stage, precision@N for the ranking stage, and end-to-end metrics like nDCG and predicted revenue at the re-ranking stage. A/B Testing Infrastructure: Implement randomized traffic splits to test new retrieval or ranking models. Log both intermediate (e.g., candidate sets, scores) and final user engagement metrics. Monitor and Iterate Logging: At each stage, log key statistics: retrieval counts, score distributions, re-ranking positions, and final engagement signals. Alerting: Set up alerts for unexpected drops in recall or spikes in latency. If the candidate generation stage suddenly drops recall, it often cascades to poor final recommendations. User Feedback Loops: Allow users to provide explicit feedback (e.g., âNot interestedâ clicks) and integrate this data into model updates, especially at the ranking and re-ranking stages. Reflections on Simplicity and Complexity In designing multi-stage pipelines, engineers face a tension between simple, interpretable approaches and complex, high-performing models. While itâs tempting to jump to the latest deep learning breakthroughs, simpler methodsâlike content-based filtering with cosine similarity and GBDT rankingâoften match or exceed deep models in early stages when engineered features are strong. The principle of Occamâs razor applies: prefer the simplest solution that meets requirements, then add complexity only where it yields measurable benefit.\nMoreover, a systemâs maintainability, interpretability, and debuggability often correlate inversely with complexity. Multi-stage pipelines already introduce architectural complexity; adding deeply entangled neural modules at every layer can make debugging a nightmare. By isolating complexity to the re-ranking stageâwhere it matters most for final user experienceâengineers can maintain robustness and agility.\nThe Beauty of Layered Thinking Multi-stage recommendation systems epitomize a fundamental computing strategy: break down a huge, unwieldy problem into manageable subproblems, solve each with the right tool, and combine solutions meticulously. This layered thinking mirrors how we, as humans, process informationâfilter broadly, focus on promising candidates, then refine with precision. By respecting constraints of latency, scalability, and maintainability, multi-stage pipelines deliver high-quality recommendations at massive scale.\nAt each stageâcandidate generation, scoring, and re-rankingâwe balance conflicting objectives: recall versus speed, accuracy versus cost, personalization versus fairness. Drawing from psychology, we see parallels in cognitive load, habit formation, and the nuanced interplay between exploration and exploitation. Whether designing a new system from scratch or optimizing an existing pipeline, embracing the multi-stage mindset encourages modularity, experiment-driven improvement, and user-centered design.\nI hope this exploration has illuminated the conceptual underpinnings of multi-stage recommendation, offering both a high-level roadmap and practical pointers for implementation. As you build or refine your own systems, remember: start broad, sharpen focus, and polish the final list with careâjust as one crafts an idea from rough sketch to polished essay.\nReferences and Further Reading Bello, I., Manickam, S., Li, S., Rosenberg, C., Legg, B., \u0026amp; Bollacker, K. (2018). Deep Interest Network for Click-Through Rate Prediction. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \u0026amp; Data Mining, 1059â1068. Geyik, U. A., Santos, C. N. d., Xu, Z., Grbovic, M., \u0026amp; Vucetic, S. (2019). Personalized Recommendation on Strengths, Weaknesses, Opportunities, Threats. Proceedings of The World Wide Web Conference, 3182â3188. Hron, P., BÃ©res, I., \u0026amp; GÃ¡lik, R. (2021). Neural Cascade Ranking for Large-Scale Recommendation. SIAM International Conference on Data Mining, 454â462. Luo, J., Zhang, C., Bian, J., \u0026amp; Sun, G. (2020). A Survey of Hybrid Recommender Systems. ACM Computing Surveys, 52(3), 1â38. Moreira, G. d. S. P., Rabhi, S., Lee, J. M., Ak, R., \u0026amp; Oldridge, E. (2021). End-to-End Session-Based Recommendation on GPU. Proceedings of the ACM Symposium on Cloud Computing, 831â833. Pei, J., Yuan, S., Zhao, H., Chen, W., Wang, Q., \u0026amp; Li, X. (2019). Neural Multi-Task Learning for Personalized Recommendation on Taobao. ACM Transactions on Intelligent Systems and Technology, 10(5), 1â25. Wilhelm, P., Zhang, X., Liao, J., \u0026amp; Zhao, Y. (2018). YouTube Recommendations: Beyond K-Means. Proceedings of the 12th ACM Conference on Recommender Systems, 9â17. âBuilding a Multi-Stage Recommender System: A Step-by-Step Guide.â (2024). Generative AI Lab. Retrieved from https://generativeailab.org/l/machine-learning/building-a-multi-stage-recommender-system-a-step-by-step-guide/ (generativeailab.org) âMulti-Stage Recommender Systems: Concepts, Architectures, and Issues.â (2022). IJCAI. Retrieved from https://www.ijcai.org/proceedings/2022/0771.pdf (ijcai.org) âRecommendation systems overview | Machine Learning.â (2025). Google Developers. Retrieved from https://developers.google.com/machine-learning/recommendation/overview/types (developers.google.com) âTowards a Theoretical Understanding of Two-Stage Recommender Systems.â (2024). arXiv. Retrieved from https://arxiv.org/pdf/2403.00802 (arxiv.org) âBuilding and Deploying a Multi-Stage Recommender System with Merlin.â (2022). NVIDIA. Retrieved from https://resources.nvidia.com/en-us-merlin/bad-a-multi-stage-recommender (resources.nvidia.com, assets-global.website-files.com) âHow to build a Multi-Stage Recommender System.â (2023). LinkedIn Pulse. Retrieved from https://www.linkedin.com/pulse/how-build-multi-stage-recommender-system-aayush-agrawal-djdyf (linkedin.com) âMultidimensional Insights into Recommender Systems: A Comprehensive Review.â (2025). Springer. Retrieved from https://link.springer.com/chapter/10.1007/978-3-031-70285-3_29 (link.springer.com) Schwartz, B. (2004). The Paradox of Choice: Why More Is Less. HarperCollins Publishers. Vygotsky, L. S. (1978). Mind in Society: The Development of Higher Psychological Processes. Harvard University Press. ","permalink":"https://pjainish.github.io/posts/multi-stage-recommender-systems/","summary":"\u003cp\u003eMulti-stage recommendation systems break down the challenging task of matching users with relevant items into several sequential phases, each optimizing for different objectives like efficiency, accuracy, and personalization. By progressively narrowing down a vast pool of candidates, applying increasingly complex models, and refining final rankings, these systems achieve scalable and high-quality recommendations even when dealing with billions of users and items (\u003ca href=\"https://www.ijcai.org/proceedings/2022/0771.pdf\" title=\"Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI\"\u003eijcai.org\u003c/a\u003e, \u003ca href=\"https://developers.google.com/machine-learning/recommendation/overview/types\" title=\"Recommendation systems overview | Machine Learning - Google Developers\"\u003edevelopers.google.com\u003c/a\u003e). They mirror how humans might sift through information: first skimming broadly, then considering details, and finally fine-tuning choices. This blog post explores the conceptual foundations of multi-stage recommendation, the distinct roles of each phase, the motivations behind layered architectures, and the real-world trade-offs they address. Along the way, analogies to everyday decision-making, historical parallels from human learning, and references to psychology illustrate how designers balance speed, relevance, and diversity. Finally, we survey challenges such as latency constraints, fairness, and the evolution toward neural re-ranking and hybrid objectives, pointing curious readers to key research papers and practical guides for deeper study.\u003c/p\u003e","title":"Multi-Stage Approach to Building Recommender Systems"},{"content":"BERT4Rec is a sequential recommendation model that leverages the bidirectional Transformer architecture, originally designed for language tasks, to capture usersâ evolving preferences by jointly considering both past and future items in a sequence (arxiv.org, github.com). Unlike earlier unidirectional models that predict the next item only from previous ones, BERT4Rec uses a Cloze-style masking objective to predict missing items anywhere in the sequence, enabling richer context modeling (arxiv.org, github.com). Empirical evaluations on multiple benchmark datasets demonstrate that BERT4Rec often surpasses state-of-the-art sequential models like SASRec, though its performance can depend on careful training schedules and hyperparameter choices (arxiv.org, arxiv.org). This post traces the journey from early recommendation methods to the Transformer revolution and the rise of BERT, explains the core ideas behind BERT4Rec, connects them to cognitive analogies of Cloze tests, and discusses experiments, limitations, and future directions. By understanding BERT4Recâs design and its place in the broader landscape of recommendation, readers can appreciate both its technical elegance and its conceptual roots in language modeling and human learning.\nIntroduction: A Learning Journey I still remember the first time I tried to build a recommendation system. It was during my undergraduate years, and I wanted to create a small app that suggested books to my friends based on what they had read before. At that time, I naively believed that simply counting co-occurrences of books would be enough. I soon realized that user preferences change over time, and static co-occurrence matrices felt too rigid. That curiosity led me to explore sequential recommendationâmodels that treat a userâs history as an evolving narrative rather than a single static snapshot.\nFast forward a few years, and I found myself diving into deep learning approaches for recommendation during my PhD. Each step felt like peeling another layer of understanding: starting with simple Markov chains, moving to recurrent neural networks, then witnessing the Transformer revolution in natural language processing (NLP) with papers like âAttention Is All You Needâ (arxiv.org, papers.nips.cc)âand âBERT: Pre-training of Deep Bidirectional Transformers for Language Understandingâ (arxiv.org, aclanthology.org). In language tasks, these models treated sentences as dynamic sequences of words; in recommendation, sequences of items could be handled similarly.\nJust as the alphabet and grammar form the foundation of language, the sequence of user interactionsâclicks, views, purchasesâforms the grammar of recommendation. When I first encountered BERT4Rec, I saw a bridge between these worlds: a model designed for language Cloze tasks, applied to sequences of items. In this post, I want to share that journeyâwhy the shift from unidirectional to bidirectional models matters, how the Cloze objective parallels human tests, the design choices behind BERT4Rec, and what we can learn both technically and conceptually from it. My hope is that, by the end, youâll see BERT4Rec not just as another state-of-the-art model, but as part of a broader narrative connecting human cognition, language, and recommendation.\nBackground: From Static to Sequential Recommendation The Early Days: Collaborative Filtering and Beyond Recommender systems began with collaborative filtering approaches that treat users and items symmetrically, often using matrix factorization to uncover latent factors (link.springer.com). These methods assume static preferences: a user has fixed tastes, and items have fixed attributes. For example, if Alice liked âThe Hobbitâ and âThe Lord of the Rings,â a static model would continue recommending similar fantasy books without considering that she might have grown more interested in science fiction recently.\nPsychologically, this is akin to assuming that a personâs personality never changesâan oversimplification. In reality, tastes evolve. Just as our moods and interests shift from week to week, user interactions in an online setting reflect changing preferences. Recognizing this, researchers started looking at temporal dynamics: assigning more weight to recent interactions (link.springer.com, arxiv.org). However, these adjustments were often heuristic rather than deeply integrated into the modelâs structure.\nSequential Recommendation: Capturing the Flow To better model evolving preferences, sequential recommendation treats a userâs history as an ordered list of events. Two main families of approaches emerged:\nMarkov Chain-based Models: These assume that the next action depends on a limited window of previous actions, often just the last one or two (cseweb.ucsd.edu). While simple and effective in sparse settings, they struggle to capture longer-term patterns. Itâs like predicting the next word in a sentence by looking only at the immediately preceding wordâsometimes okay, but often missing broader context.\nRecurrent Neural Networks (RNNs): With the rise of deep learning, RNNs (e.g., GRU4Rec) became popular for sequential recommendation tasks. They process one item at a time, updating a hidden state that summarizes the history (link.springer.com, arxiv.org). While theoretically capable of capturing long-range dependencies, RNNs can suffer from vanishing gradients and can be slow to train, especially when sequences get long.\nThese methods moved beyond static views of users, but they still relied on unidirectional modeling: either Markov chains always look backward a fixed number of steps, and RNNs process sequences from left (oldest) to right (newest). In human terms, itâs like reading a story only forwardânever knowing how the ending influences the interpretation of earlier chapters.\nSelf-Attention and SASRec: A Step Towards Flexible Context In August 2018, Kang and McAuley introduced SASRec, a self-attentive sequential model that borrowed ideas from the Transformerâs self-attention mechanism to balance long-term and short-term context (arxiv.org, cseweb.ucsd.edu). Instead of processing item sequences strictly left-to-right, SASRec computes attention weights over all previous items at each step, allowing the model to focus on the most relevant past actions when predicting the next one (arxiv.org, arxiv.org). Mechanically, it applies multi-head self-attention layers over item embeddings, followed by pointwise feed-forward layers, similar to each encoder block in the original Transformer (arxiv.org, export.arxiv.org).\nSASRec offered two main advantages:\nEfficiency: By parallelizing self-attention computations across positions, SASRec can be trained faster than RNN-based models on modern hardware. Adaptive Context: Attention weights allow the model to decide which past items matter most, rather than forcing it to use a fixed window or hidden state sequence. However, SASRec remains unidirectional in its attention: at each time step, it only attends to items that come before that position. This means it still cannot consider potential âfutureâ items, even if they would be known at test time when scoring multiple candidate items. In language terms, itâs like understanding a sentence by reading it left to rightânever knowing what words come later in the sentence.\nThe Transformer Revolution: Background and Impact The Birth of the Transformer (Vaswani et al., 2017) In June 2017, Vaswani et al. published âAttention Is All You Need,â a paper that fundamentally changed NLP and sequence modeling (arxiv.org, papers.nips.cc). They introduced the Transformer, which replaced recurrence with multi-head self-attention and simple feed-forward networks. The key insights were:\nSelf-Attention Layers: These compute weighted sums of all positionsâ embeddings for each position, allowing direct modeling of pairwise dependencies regardless of distance. Positional Encoding: Since attention layers by themselves lack inherent order, they added sinusoidal positional encodings to inject sequence information. Parallelization: Unlike RNNs, Transformers can process all positions in parallel, making training significantly faster on GPUs. By discarding recurrence and convolutions, the Transformer demonstrated state-of-the-art performance on machine translation tasks, achieving BLEU scores surpassing previous best models on WMT English-German and English-French benchmarks (arxiv.org, scispace.com). This architecture quickly became the de facto backbone for a wide range of NLP tasks, from translation to summarization to question answering (huggingface.co, arxiv.org).\nAnalogy: Before Transformers, sequence models were like cars with only one speedâreverse (recurrence) or forward (convolutions/attention with constraints). Transformers were like multi-gear vehicles that could shift seamlessly, giving models flexibility to access information anywhere in the sequence, much like looking up any chapter in a book instantly rather than reading every page sequentially.\nBERT: Deep Bidirectional Language Representation (Devlin et al., 2018) Building on the Transformerâs encoder, Devlin et al. introduced BERT (Bidirectional Encoder Representations from Transformers) in October 2018 (arxiv.org, aclanthology.org). BERTâs main contributions were:\nBidirectional Context: By jointly attending to both left and right context in all layers (rather than only attending to previous tokens), BERT can learn richer representations. Masked Language Modeling (MLM): To enable bidirectionality, they used a Cloze-like task: randomly mask some tokens in the input and train the model to predict them based on surrounding context. Next Sentence Prediction (NSP): As a secondary task, BERT predicts whether two sentences follow each other, helping capture inter-sentence relationships. BERT was pre-trained on massive corpora (BooksCorpus and English Wikipedia), achieving state-of-the-art results across a variety of NLP benchmarks, such as GLUE, SQuAD, and others (arxiv.org, export.arxiv.org). Its bidirectional design unlocked new capabilities: while unidirectional language models (e.g., OpenAI GPT) process text left-to-right, BERTâs MLM allowed it to encode context from both sides, akin to reading a sentence and filling in missing words anywhere in it.\nAnalogy: Imagine reading a paragraph with some words hidden and having to guess them using the rest of the paragraph. This Cloze-style task is exactly how BERT learns. In human tests, teachers often use fill-in-the-blank exercises to gauge comprehensionâsimilarly, BERTâs MLM forces the model to deeply understand context.\nThe impact of BERT extended beyond NLP. Researchers began to ask: if bidirectional Transformers can learn from masked words in a sentence, could a similar idea work for sequences of user interactions? Enter BERT4Rec.\nBERT4Rec: Core Ideas and Design Motivation: Why Bidirectional Modeling Matters In sequential recommendation, we often care about predicting the next item given past history. Unidirectional models like SASRec attend only to prior items when making a prediction (arxiv.org, cseweb.ucsd.edu). However, at evaluation or inference time, we typically score multiple candidate items to find the most likely next item. Those candidates can be seen as âfutureâ items once we inject them into the sequence. If the model can attend to both past items and the candidate item itself (as if it were masked during training), it can form a richer representation that uses information from the full sequence context.\nBERT4Rec reframes sequential recommendation as a Cloze task: randomly mask items in the userâs history and train the model to predict them based on both left and right context, which may include items that occur after them in the sequence (arxiv.org, github.com). This bidirectional conditioning helps the model learn how items co-occur in different parts of the sequence, not just in a strict left-to-right chain.\nAnalogy: In a detective novel, clues about who committed the crime may appear early and later in the story. A unidirectional reader would only use clues from the beginning up to the current chapter. A bidirectional reader, knowing the ending, can reinterpret earlier clues in light of later revelations. Similarly, BERT4Recâs bidirectional attention allows the model to reinterpret earlier interactions when considering missing items.\nArchitecture Overview At a high level, BERT4Rec follows the encoder architecture from the original Transformer with two major changes:\nCloze-style Masking: A certain percentage of items in a userâs sequence are randomly masked (replaced with a special [MASK] token). The modelâs task is to predict the identity of each masked item using bidirectional attention over the unmasked items (arxiv.org, researchgate.net). Item Embeddings with Positional Encodings: Each item in the sequence is mapped to a learned embedding. Since the Transformer has no inherent sense of order, sinusoidal or learned positional encodings are added to each item embedding to encode its position in the sequence (arxiv.org, ar5iv.labs.arxiv.org). Concretely:\nInput: A user history of length n (e.g., [iâ, iâ, â¦, iâ]). We randomly choose a subset of positions (usually 15%) and replace them with [MASK] tokens. For example, if the original sequence is [A, B, C, D, E] and positions 2 and 4 are masked, the input becomes [A, [MASK], C, [MASK], E]. Embedding Layer: Each position t has an embedding E_item(iâ) (for item iâ) plus a positional embedding E_pos(t). So, the initial input to the Transformer is the sum E_item + E_pos for each position, with masked positions using a special mask embedding. Transformer Encoder Stack: Typically 2 to 4 layers (depending on hyperparameters) of multi-head self-attention and feed-forward layers. Since we want bidirectional context, the self-attention is âfullâ (not masked), allowing each position to attend to all other positions in the sequence. Output Heads: For each masked position, the final hidden state vector is passed through a linear projection followed by a softmax over the item vocabulary to predict which item was masked. Loss Function: Cross-entropy loss is computed only over the masked positions, summing (or averaging) across them. During inference, to predict the next item, one can append a [MASK] token to the end of a userâs sequence and feed it through the model. The modelâs output distribution at that position indicates the probabilities of all possible items being the next interaction.\nTechnical Note: Because BERT4Rec conditions on bidirectional context, it avoids what is known as âexposure biasâ often found in left-to-right models, where during training the model sees only ground-truth history, but during inference it must rely on its own predictions. BERT4Recâs Cloze objective alleviates this by mixing masked ground truth with unmasked items, making the model robust to masked or unknown future items.\nTraining as a Cloze Task: Deeper Explanation The term Cloze comes from psycholinguistics and educational testing: learners fill in missing words in a text passage (arxiv.org, kdnuggets.com). This is not a new idea. In fact, BERT borrowed it directly from earlier NLP work, such as the Cloze tests used by educators to measure student comprehension (kdnuggets.com). In the context of recommendation:\nMasked Item Prediction (MIP): Analogous to masked language modeling (MLM) in BERT, BERT4Recâs MIP randomly selects a subset of positions in a userâs interaction sequence, hides each item, and asks the model to fill it in based on both past and future interactions. Sampling Strategy: Typically, 15% of items are chosen for masking. Of those, 80% are replaced with [MASK], 10% with a random item (to encourage robustness), and 10% are left unchanged but still counted in the loss as if they were masked (to mitigate training/test mismatch) (arxiv.org, github.com). Advantages: By predicting items anywhere in the sequence, the model learns co-occurrence patterns in all contexts, not just predicting the next item. This generates more training samples per sequence (since each masked position is a training example), potentially improving data efficiency (arxiv.org, arxiv.org). Analogy: When learning a language, filling in blank words anywhere in a paragraph helps both reading comprehension and vocabulary acquisition. Similarly, by practicing predicting missing items anywhere in their history, the model builds a more flexible representation of user preferences.\nComparison with Unidirectional Models (e.g., SASRec) Context Scope\nUnidirectional (SASRec): At position t, the model attends only to items 1 through tâ1. Bidirectional (BERT4Rec): At each masked position t, the model attends to all items except those that are also masked. When predicting the next item (by placing a [MASK] at n+1), it attends to items 1 through n and vice versa for other masked positions. Training Objective\nUnidirectional: Usually uses next-item prediction with cross-entropy loss at each time step. Bidirectional: Uses Cloze objective, predicting multiple masked positions per sequence. Data Efficiency\nUnidirectional: Generates one training sample per time step (predict next item). Bidirectional: Generates as many training samples as there are masked positions (typically ~15% of sequence length), often leading to more gradient updates per sequence. Inference\nUnidirectional: Directly predicts the next item based on history. Bidirectional: Appends a [MASK] to the end to predict next item, or can mask any position for in-sequence imputation. Several empirical studies have shown that BERT4Rec often outperforms SASRec, especially when long-range dependencies are important (arxiv.org, arxiv.org). However, this performance advantage can require longer training times and careful hyperparameter tuning, as later work has pointed out (arxiv.org, arxiv.org).\nDrawing Analogies: Cloze Tests, Human Learning, and Recommendation The Psychology of Masked Tests Cloze tests, introduced by W. L. Taylor in 1953, are exercises where learners fill in blanks in a passage of text, gauging language comprehension and vocabulary knowledge (kdnuggets.com). Educational psychologists have found that Cloze tasks encourage active recall and semantic inference, as learners must use both local and global context to guess missing words correctly. Similarly, BERTâs MLM and BERT4Recâs MIP require the model to infer missing tokens (words or items) from all available context, reinforcing rich contextual understanding.\nIn human terms:\nLocal Context: To guess a masked word in a sentence, you use nearby words. Global Context: Often, clues spread across the paragraph or entire document guide you toward the right answer. BERT4Recâs masked items play the role of blank spaces in a text. The model, like a student in a Cloze test, must use all known interactions (both before and after the blank) to infer the missing preference. This leads to representations that capture not only pairwise item relationships but also how items co-occur across entire sessions.\nHistorical Perspective: From Prediction to Comprehension Early recommendation models focused on prediction: given past clicks, what happens next? This is analogous to a fill-in-the-blank exercise where only the next word is blank. In mathematics, this is like knowing all terms of a sequence except the next one and trying to guess it from a recurrence relation. But modern language teaching emphasizes comprehension, teaching students to understand entire texts, not just predict the next word. BERT4Rec embodies that shift: from predicting sequentially to understanding a userâs entire session.\nConsider reading Hamlet: if you only focus on predicting the next line, you might miss the broader themes. If you think about themes and motifs across the play, you get a richer understanding. BERT4Rec, by predicting masked items anywhere, learns themes and motifs in interaction sequences as well.\nReal-World Analogy: Playlist Shuffling Imagine youâre curating a playlist of songs youâll listen to on a road trip. Instead of putting them in a fixed order (e.g., chronological from your latest favorites), you shuffle them but still want the transitions to feel coherent. A unidirectional model ensures each song transitions well from the previous one, like ensuring each next word makes sense after the last. A bidirectional approach would allow you to also consider the song that comes after when choosing a song for a particular slot, creating smooth transitions both forward and backward. In BERT4Rec, masked songs correspond to shuffled or missing approximate transitions, and the model learns what fits best given both neighbors.\nTechnical Deep Dive: BERT4Recâs Mechanics Input Representation Given a userâs historical sequence of item interactions $iâ, iâ, â¦, iâ$, BERT4Rec prepares inputs as follows (arxiv.org, researchgate.net):\nMasking Strategy\nRandomly select 15% of positions for masking.\nOf those positions:\n80% are replaced with [MASK]. 10% are replaced with a random item ID from the vocabulary (to encourage robustness). 10% remain unchanged (but are still counted in the loss). This strategy mirrors BERTâs design to prevent the model from relying too heavily on the [MASK] token (arxiv.org, export.arxiv.org). Item Embeddings\nEach item ID has a learned embedding vector of dimension d. A special embedding E_mask is used for [MASK] tokens. Positional Embeddings\nSince the Transformer has no notion of sequence order, add a learned positional embedding E_pos(t) for each position t â {1,â¦,n}. The sum E_item(iâ) + E_pos(t) forms the input embedding at position t. Sequence Length and Padding\nFor computational efficiency, fix a maximum sequence length L (e.g., 200). If a userâs history has fewer than L interactions, pad the sequence with [PAD] tokens at the front or back. [PAD] tokens have embeddings but are ignored in attention computations (i.e., their attention weights are set to zero). Embedding Dropout\nOptional dropout can be applied to the sum of item and positional embeddings to regularize training. Mathematically, let\n$$ xâ = E_{item}(iâ) + E_{pos}(t), \\quad t = 1,\\dots,n. $$\nMasked positions use\n$$ xâ = E_{mask} + E_{pos}(t). $$\nTransformer Encoder Stack BERT4Rec typically uses a stack of N encoder layers (e.g., N = 2 or 3 for smaller datasets, up to N = 6 for larger ones), each consisting of:\nMulti-Head Self-Attention\nFor layer l, each position t has queries, keys, and values computed as linear projections of the input from the previous layer.\nAttention weights are computed as scaled dot products between queries and keys, followed by softmax.\nWeighted sums of values produce the attention output for each head.\nThe outputs of all heads are concatenated and linearly projected back to dimension d.\nResidual connection and layer normalization are applied:\n$$ \\text{SA}_l(X) = \\text{LayerNorm}(X + \\text{MultiHeadAttn}(X)). $$\nPosition-Wise Feed-Forward Network\nA two-layer feed-forward network with a GELU or ReLU activation:\n$$ \\text{FFN}_l(Y) = \\text{LayerNorm}(Y + Wâ ,\\phi(Wâ Y + bâ) + bâ), $$\nwhere $\\phi$ is an activation (often GELU).\nLayerNorm and Residual Connections\nAs in the original Transformer, each sub-layer has a residual (skip) connection followed by layer normalization, ensuring stable training and gradient flow (arxiv.org, scispace.com). Because the self-attention is full (no masking of future positions), each positionâs representation at each layer can incorporate information from any other unmasked position in the sequence.\nOutput and Loss Computation After N encoder layers, we obtain final hidden representations ${hâ, hâ, \\dots, hâ}$ â â^{nÃd}. For each position t that was masked during input preparation, we compute:\nItem Prediction Scores\n$$ sâ = W_{output} , hâ + b_{output}, \\quad sâ â â^{|V|}, $$\nwhere |V| is the size of the item vocabulary, and $W_{output} â â^{|V|Ãd}$.\nSoftmax and Cross-Entropy Loss\nApply softmax to $sâ$ to get predicted probability distribution $\\hat{y}_t$.\nIf the true item ID at position t is $iâ^*$, the cross-entropy loss for that position is:\n$$ \\mathcal{L}t = -\\log\\bigl(\\hat{y}{t}[ iâ^* ]\\bigr). $$\nAggregate loss across all masked positions in the batch, typically averaging over them:\n$$ \\mathcal{L} = \\frac{1}{\\sum_t mâ} \\sum_{t=1}^n mâ , \\mathcal{L}_t, $$\nwhere $mâ = 1$ if position t was masked, else 0.\nBecause multiple positions are masked per sequence, each training example yields several prediction targets, improving data efficiency.\nInference: Predicting the Next Item To recommend the next item for a user:\nExtend the Sequence\nGiven the userâs last n interactions, append a [MASK] token at position n+1 (if n+1 â¤ L). If n = L, one could remove the oldest item or use sliding window techniques. Feed Through Model\nThe [MASK] at position n+1 participates in bidirectional attention, attending to all positions 1 through n. Conversely, positions 1 through n attend to the [MASK] if full selfâattention is used. Obtain Scores\nCompute $s_{n+1} â â^{|V|}$ from the final hidden state $h_{n+1}$. The highest-scoring items in $s_{n+1}$ are the top-K recommendations. Because BERT4Recâs training objective was to predict masked items given both left and right context, placing the [MASK] at the end simulates one masked position with only left context. While strictly speaking this isnât bidirectional (the [MASK] at the end has no right context), it still benefits from richer item co-occurrence patterns learned during training. Empirically, this approach yields strong next-item recommendation accuracy.\nExperimental Results and Analysis Datasets and Evaluation Protocols In the original BERT4Rec paper, Sun et al. evaluated the model on four public benchmark datasets:\nMovieLens-1M (ML-1M): 1 million ratings from ~6000 users on ~3900 movies. YooChoose: Click logs from the RecSys Challenge 2015, with ~8.6 million events. Steam: Game purchase and play logs from the Steam platform. Amazon Beauty: Reviews and ratings in the beauty product category from the Amazon Reviews dataset. For each user, interactions were chronologically ordered. The last interaction was used as the test item, the second last as validation, and earlier interactions for training. Performance metrics included Hit Rate (HR@K) and Normalized Discounted Cumulative Gain (NDCG@K) at various cut-offs (e.g., K = 5, 10) (arxiv.org, arxiv.org).\nBaselines Compared Sun et al. compared BERT4Rec against several state-of-the-art sequential recommendation methods:\nGRU4Rec: RNN (GRU) based model with pairwise ranking loss. Casual Convolutional (CasualConv): Convolutional neural network model for sequences. SASRec: Self-attention based unidirectional model. Caser: Convolutional sequence embedding model (vertical + horizontal convolution). NextItNet: Dilated residual network for sequential recommendation. Key Findings BERT4Rec vs. SASRec\nAcross ML-1M and YooChoose, BERT4Rec improved HR@10 by â2â3% and NDCG@10 by â1â2% relative to SASRec (arxiv.org, arxiv.org). On sparser datasets like Steam, the advantage increased, indicating that bidirectional context can better handle data sparsity by leveraging co-occurrence patterns across entire sessions. Model Depth and Hidden Size\nDeeper (more layers) or wider (larger d) BERT4Rec variants performed better on large datasets but risked overfitting on smaller ones. Typical configurations: 2 layers, hidden size 64 for ML-1M; 3â4 layers for larger datasets. Masking Ratio\nMasking ~15% of items per sequence yielded a good trade-off. Masking too many positions reduced signal per position; masking too few yielded fewer training samples. Training Time\nBERT4Rec required more compute than SASRec due to larger parameter counts and Cloze objective. Subsequent research (Petrov \u0026amp; Macdonald, 2022) noted that default training schedules in the original implementations were too short to fully converge on some datasets; when trained longer, BERT4Recâs performance became more consistent (arxiv.org, arxiv.org). Replicability and Training Considerations Petrov and Macdonald (2022) conducted a systematic review and replicability study of BERT4Rec, finding:\nTraining Time Sensitivity: Default hyperparameters often led to under-trained models. Training 10â30Ã longer was sometimes necessary to reproduce reported results (arxiv.org, arxiv.org). Batch Size and Learning Rates: Smaller batch sizes with warm-up steps and linear decay of learning rates yielded more stable convergence. Alternative Architectures: Implementations using Hugging Faceâs Transformers library, incorporating variants like DeBERTaâs disentangled attention, matched or exceeded original results with significantly less training time (arxiv.org, arxiv.org). Another study by Petrov \u0026amp; Macdonald (2023) introduced gSASRec, which showed that SASRec could outperform BERT4Rec when properly addressing overconfidence arising from negative sampling (arxiv.org). They argued that BERT4Recâs bidirectional mechanism alone did not guarantee superiority; rather, loss formulations and training strategies play a crucial role.\nComparative Strengths and Weaknesses Strengths\nRich Context Modeling: By conditioning on both sides of a position, BERT4Rec captures intricate co-occurrence patterns. Data Efficiency: Masked positions generate more supervision signals per sequence. Flexibility: Can predict items at arbitrary positions, enabling applications like sequential imputation or session completion beyond next-item recommendation. Weaknesses\nCompute and Memory: More parameters and bidirectional attention make it more expensive in both training and inference compared to unidirectional models. Training Sensitivity: Requires careful hyperparameter tuning and longer training times to reach optimal performance. Inference Unidirectionality for Next-Item: Although trained bidirectionally, predicting the next item requires inserting a [MASK] with no right context, effectively making inference unidirectional, possibly leaving some benefits unused. Conceptual Insights: Why BERT4Rec Works Learning Co-Occurrence vs. Sequential Order Unlike unidirectional models that focus on orderingâitem t predicts item t+1âBERT4Rec learns from co-occurrence patterns across sessions:\nItems A and B that consistently appear together in sessions might have high mutual information. If A often precedes B and also often follows B, unidirectional models only see one direction; BERT4Rec sees both, learning a symmetric association. In recommendation, co-occurrence is often more informative than strict ordering. For example, if many users watch âThe Matrixâ and âInceptionâ in any order, a bidirectional model picks up that association, regardless of which came first.\nOvercoming Exposure Bias Unidirectional models train to predict the next item given ground-truth history. During inference, they must use predicted items (or no items) to form history, leading to exposure biasâerrors compound as the model has never seen its own mistakes. In contrast, BERT4Recâs masking randomly hides items during training, exposing the model to situations where parts of the sequence are unknown, resulting in more robust representations when some interactions are missing or noisy (arxiv.org, arxiv.org).\nAnalogous to Autoencoders BERT4Recâs training resembles an autoencoder: it corrupts (masks) parts of the input and learns to reconstruct them. This formulation encourages the model to learn latent representations capturing holistic session semantics. In collaborative filtering, denoising autoencoders (e.g., CDAE) have been used for recommendation, where randomly corrupted user vectors are reconstructed (arxiv.org, researchgate.net). BERT4Rec extends that idea to sequences of interactions with the Transformerâs bidirectional power.\nBroader Context: From Language to Recommendation Transfer of Ideas Across Domains BERT4Rec is an instance of cross-pollination between NLP and recommendation research. Historically, many breakthroughs in one field find applications in others:\nWord2Vec (2013): Initially for word embeddings, later adapted for graph embeddings, collaborative filtering, and more. Convolutional Neural Networks (1995â2012): Developed for image tasks, later adapted for text (CNNs for sentence classification) and recommendation (Caser uses convolution to model user-item sequences). Attention Mechanisms (2014â2017): Originating in machine translation, now used in recommendation (e.g., SASRec, BERT4Rec, and many variants). The flow of ideas mirrors human creativity: when we learn a concept in one context, we often find analogous patterns in another.\nAnalogy: Leonardo da Vinci studied bird flight to design flying machines. Similarly, BERT4Rec studies how Transformers learn from language sequences to design better user modeling systems.\nHistorical Perspective: The Rise of Pre-Training In both language and recommendation, there is a shift from task-specific training to pre-training + fine-tuning:\nIn NLP, models like ELMo (2018), GPT (2018), and BERT (2018â2019) introduced large-scale pre-training on massive unlabeled corpora, followed by fine-tuning on downstream tasks (arxiv.org, aclanthology.org). In recommendation, early models trained from scratch on each dataset. Now, researchers explore pre-training on large interaction logs to learn general user behavior patterns, then fine-tune on specific domains (e.g., news, movies). BERT4Recâs Cloze objective could be viewed as a form of self-supervised pre-training, although in the original work they trained on the target dataset from scratch (arxiv.org, arxiv.org). This trend reflects a broader movement in AI: capturing general knowledge from large data and adapting it to specific tasks, mirroring human learningâchildren first learn language generally, then apply it to specialized domains like mathematics or science.\nLimitations and Challenges Computational Complexity BERT4Recâs bidirectional attention has quadratic time and memory complexity with respect to sequence length. In long sessions (e.g., browsing logs with hundreds of items), this becomes a bottleneck. Several strategies mitigate this:\nTruncated Histories: Only consider the last L items (e.g., L = 200). Segmented or Sliding Windows: Process overlapping windows of fixed length rather than the entire history. Efficient Attention Variants: Use sparse attention (e.g., Linformer, Performer) to reduce complexity from O(LÂ²) to O(L log L) or O(L) (arxiv.org). Nonetheless, these require extra engineering and can affect performance if important interactions get truncated.\nTraining Sensitivity and Hyperparameters As noted by Petrov and Macdonald (2022), BERT4Recâs performance is sensitive to:\nNumber of Training Epochs: Standard schedules may under-train the model. Learning Rate Schedules: Warm-up steps followed by linear decay often yield stable performance. Batch Size and Mask Ratio: Larger batches and masking too many positions can hinder learning. Negative Sampling Effects: Overconfidence in ranking due to unbalanced positive/negative sampling can lead to suboptimal results; alternative loss functions (e.g., gBCE) can mitigate this (arxiv.org, arxiv.org). This contrasts with smaller unidirectional models like SASRec, which often converge faster and require fewer tuning efforts.\nCold-Start and Long-Tail Items Like many collaborative filtering methods, BERT4Rec struggles with:\nCold-Start Users: Users with very short or no interaction history. Masked predictions require contextâif thereâs no context, predictions degrade. Cold-Start Items: Items with very few interactions. Their embeddings are not well trained, making them less likely to be predicted. Long-Tail Distribution: Most items appear infrequently; BERT4Rec can overfit popular items seen many times in training, biasing recommendations. Mitigations include:\nIncorporating content features (e.g., item metadata, text descriptions) through hybrid models. Using meta-learning to quickly adapt to new items or users. Employing data augmentation (e.g., synthetic interactions) to enrich representations. Interpretability Transformers are often regarded as âblack boxes.â While attention weights can sometimes be visualized to show which items influence predictions, they do not guarantee human-interpretable explanations. Efforts to explain recommendation via attention often reveal that attention scores do not always align with intuitive importance (arxiv.org). For stakeholders demanding transparency, additional interpretability methods (e.g., counterfactual explanations, post-hoc analysis) may be needed.\nVariants and Extensions Incorporating Side Information BERT4Rec can be extended to use side features:\nUser Features: Demographics, location, device, etc. Item Features: Category, price, textual description, images. Session Context: Time gaps, device changes, location transitions. One approach is to concatenate side feature embeddings with item embeddings at each position, then feed the combined vector into the Transformer (arxiv.org). Alternatively, one can use separate Transformer streams for different modalities and then merge them (e.g., multi-modality Transformers).\nPre-Training on Large-Scale Logs Instead of training BERT4Rec from scratch on a target dataset, it can be pre-trained on massive generic interaction logs (e.g., clicks across many categories) and fine-tuned on a domain-specific dataset (e.g., music). Pre-training tasks might include:\nMasked Item Prediction (as usual). Segment Prediction: Predict whether a sequence segment belongs to the same user. Next Session Prediction: Predict which next session a user will have. After pre-training, the model adapts faster to downstream tasks, especially in data-sparse domains. This mimics BERTâs success in NLP.\nCombining with Contrastive Learning Recent trends in self-supervised learning for recommendation incorporate contrastive objectives, encouraging similar user sequences or items to have similar representations. One can combine BERT4Recâs Cloze objective with contrastive losses (e.g., SimCLR, MoCo) to further improve generalization:\nSequence-Level Contrast: Represent a user session by pooling BERT4Recâs hidden states; contrast similar sessions against dissimilar ones. Item-Level Contrast: Encourage items co-occurring frequently to have similar embeddings. Contrastive learning can mitigate representation collapse and improve robustness.\nEfficient Transformer Variants To handle long sequences more efficiently:\nLinformer: Projects keys and values to a lower dimension before computing attention, reducing complexity from O(LÂ²) to O(L) (arxiv.org). Performer: Uses kernel methods to approximate softmax attention linearly in sequence length. Longformer: Employs sliding window (local) attention and global tokens. Reformer: Uses locality-sensitive hashing to reduce attention costs. These variants can be plugged into BERT4Recâs framework to handle longer sessions while retaining bidirectional context.\nFuture Directions Personalization and Diversity While BERT4Rec focuses on accuracy metrics like HR@K and NDCG@K, real-world systems must balance personalization with diversity to avoid echo chambers. Future work could:\nInclude diversity-aware objectives, penalizing recommendations that are too similar to each other. Integrate exploration strategies, e.g., adding randomness to top-K predictions to surface niche items. Leverage reinforcement learning to optimize long-term engagement rather than immediate next click. Adaptation to Multi-Objective Settings E-commerce platforms care about metrics beyond clicksârevenues, lifetime value, churn reduction. Extensions of BERT4Rec could incorporate:\nMulti-Task Learning: Jointly predict next item and other objectives (e.g., purchase probability, churn risk). Bandit Feedback: Combine BERT4Rec embeddings with contextual bandit algorithms to dynamically adapt to user feedback. Causal Inference: Adjust for selection bias in logged interactions, using inverse propensity scoring with BERT4Rec representations. Explainability and Trust Building user trust in recommendations requires transparency. Research could focus on:\nAttention-Based Explanations: Visualizing attention maps to show which past items influenced a recommendation. Counterfactual Explanations: Explaining âif you hadnât clicked on item A, you might not see item B recommended.â User-Friendly Summaries: Summarizing session themes (e.g., âBecause you watched yoga videos, we recommend this fitness productâ). Cross-Seat and Cross-Device Scenarios Users often switch between devices (phone, laptop, TV) and contexts (work, home). Modeling these cross-seat patterns requires:\nHierarchical Transformers: One level encodes per-device sequences; another encodes cross-device transitions. Time-Aware Modeling: Incorporate temporal embeddings for time gaps between interactions, using continuous time Transformers. Hybrid with Knowledge Graphs Many platforms maintain knowledge graphs linking items to attributes, categories, and external entities. Integrating BERT4Rec embeddings with graph neural networks (GNNs) can enrich representations:\nGraph-Enhanced Embeddings: Use GNNs to initialize item embeddings based on their neighbors in the knowledge graph. Joint Attention over Sequences and Graphs: Attend over historical interactions and relevant graph nodes. Personal Reflections and Closing Thoughts Building BERT4Rec felt like standing on the shoulders of giants: from Markov models that taught me the basics of transitions, to RNNs that showed me how to carry hidden state, to attention mechanisms that revealed the power of flexible context, to BERTâs bidirectional pre-training that inspired me to look at user sequences holistically. Each step deepened my understanding of how to model dynamic preferences, echoing my own journey of learning and exploration.\nIâve always believed that technical advancements in AI should be connected to human-centered insights. When I see masked language models predicting words, I think of a student piecing together meaning. When I see masked item tasks predicting products, I imagine someone reconstructing their shopping trajectory, filling in forgotten steps. These analogies bridge the gap between cold mathematics and living experiences, reminding me that behind each click or purchase is a person with evolving interests, context, and purpose.\nBERT4Rec is not the final word in sequential recommendation. It represents a milestoneâa demonstration that ideas from language modeling can transform how we think about recommendation. But as we push forward, we must keep asking: How can we make models more efficient without sacrificing nuance? How can we ensure diversity and fairness? How can we respect privacy while learning from behavior? I hope this post not only explains BERT4Recâs mechanics but also sparks your own curiosity to explore these questions further.\nReferences and Further Reading Devlin, J., Chang, M.-W., Lee, K., \u0026amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT (pp. 4171â4186). (arxiv.org, aclanthology.org) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., \u0026amp; Polosukhin, I. (2017). Attention Is All You Need. In NeurIPS (pp. 5998â6008). (arxiv.org, papers.nips.cc) Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., \u0026amp; Jiang, P. (2019). BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In CIKM (pp. 1441â1450). (arxiv.org, github.com) Kang, W.-C., \u0026amp; McAuley, J. (2018). Self-Attentive Sequential Recommendation. In ICDM (pp. 197â206). (arxiv.org, cseweb.ucsd.edu) Petrov, A., \u0026amp; Macdonald, C. (2022). A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation. arXiv:2207.07483. (arxiv.org, arxiv.org) Petrov, A., \u0026amp; Macdonald, C. (2023). gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling. arXiv:2308.07192. (arxiv.org) Devlin, J., Chang, M.-W., Lee, K., \u0026amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805. (arxiv.org, export.arxiv.org) Devlin, J., Chang, M.-W., Lee, K., \u0026amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805v1. (eecs.csuohio.edu, ar5iv.labs.arxiv.org) Kang, W.-C., \u0026amp; McAuley, J. (2018). Self-Attentive Sequential Recommendation. arXiv:1808.09781. (arxiv.org, ar5iv.labs.arxiv.org) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., \u0026amp; Polosukhin, I. (2017). Attention Is All You Need. arXiv:1706.03762. (export.arxiv.org, en.wikipedia.org) Petrov, A., \u0026amp; Macdonald, C. (2022). A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation. arXiv:2207.07483. Petrov, A., \u0026amp; Macdonald, C. (2023). gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling. arXiv:2308.07192. Hu, Y., Zhang, Y., Sun, N., Murai, M., Li, M., \u0026amp; King, I. (2018). Utilizing Long- and Short-Term Structure for Memory-Based Sequential Recommendation. In WWW (pp. 1281â1290). Wu, L., Sun, X., Wang, Y., \u0026amp; Wu, J. (2020). S3-Rec: Self-Supervised Seq2Seq Autoregressive Reconstruction for Sequential Recommendation. In KDD (pp. 1267â1277). Tan, Y. K., \u0026amp; Yang, J. (2021). Light-BERT4Rec: Accelerating BERT4Rec via Knowledge Distillation for Sequential Recommendation. In CIKM. Yang, N., Wang, W., \u0026amp; Zhao, J. (2021). TransRec: Learning User and Item Representations for Sequential Recommendation with Multi-Head Self-Attention. In Sarnoff Symposium. Bi, W., Zhu, X., Lv, H., \u0026amp; Wang, W. (2021). AdaSAS: Adaptive User Interest Modeling with Multi-Hop Self-Attention for Sequential Recommendation. In RecSys. Ying, C., Fei, K., Wang, X., Wei, F., Mao, J., \u0026amp; Gao, J. (2018). Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In KDD. (Used as analogy for combining graph structures with sequence modeling.) He, R., \u0026amp; McAuley, J. (2016). VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback. In AAAI. (Illustrates use of side information in recommendation.) Wang, X., He, X., Cao, Y., Liu, M., \u0026amp; Chua, T.-S. (2019). KGAT: Knowledge Graph Attention Network for Recommendation. In KDD. (Shows integration of knowledge graphs for richer item representations.) ","permalink":"https://pjainish.github.io/posts/bert4rec-sequential-recommendation/","summary":"\u003cp\u003eBERT4Rec is a sequential recommendation model that leverages the bidirectional Transformer architecture, originally designed for language tasks, to capture usersâ evolving preferences by jointly considering both past and future items in a sequence (\u003ca href=\"https://arxiv.org/abs/1904.06690\" title=\"BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\"\u003earxiv.org\u003c/a\u003e, \u003ca href=\"https://github.com/FeiSun/BERT4Rec\" title=\"GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...\"\u003egithub.com\u003c/a\u003e). Unlike earlier unidirectional models that predict the next item only from previous ones, BERT4Rec uses a Cloze-style masking objective to predict missing items anywhere in the sequence, enabling richer context modeling (\u003ca href=\"https://arxiv.org/abs/1904.06690\" title=\"BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\"\u003earxiv.org\u003c/a\u003e, \u003ca href=\"https://github.com/FeiSun/BERT4Rec\" title=\"GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...\"\u003egithub.com\u003c/a\u003e). Empirical evaluations on multiple benchmark datasets demonstrate that BERT4Rec often surpasses state-of-the-art sequential models like SASRec, though its performance can depend on careful training schedules and hyperparameter choices (\u003ca href=\"https://arxiv.org/abs/2207.07483\" title=\"A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation\"\u003earxiv.org\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2308.07192\" title=\"gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling\"\u003earxiv.org\u003c/a\u003e). This post traces the journey from early recommendation methods to the Transformer revolution and the rise of BERT, explains the core ideas behind BERT4Rec, connects them to cognitive analogies of Cloze tests, and discusses experiments, limitations, and future directions. By understanding BERT4Recâs design and its place in the broader landscape of recommendation, readers can appreciate both its technical elegance and its conceptual roots in language modeling and human learning.\u003c/p\u003e","title":"BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers"},{"content":"Movies I have watched and Loved !!! Not in order. We live in a box of space and time. Movies are windows in its walls.\nThe Silence of the Lambs The Godfather Pulp Fiction Troy The Terminal The Pianist Interstellar Arrival Parasite ","permalink":"https://pjainish.github.io/posts/films/","summary":"\u003ch3 id=\"movies-i-have-watched-and-loved--not-in-order\"\u003eMovies I have watched and Loved !!! Not in order.\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe live in a box of space and time. Movies are windows in its walls.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003e\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://image.tmdb.org/t/p/w185/uS9m8OBk1A8eM9I042bx8XXpqAq.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eThe Silence of the Lambs\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://image.tmdb.org/t/p/w185/3bhkrj58Vtu7enYsRolD1fZdja1.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eThe Godfather\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://image.tmdb.org/t/p/w185/vQWk5YBFWF4bZaofAbv0tShwBvQ.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003ePulp Fiction\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://image.tmdb.org/t/p/w185/a07wLy4ONfpsjnBqMwhlWTJTcm.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eTroy\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://image.tmdb.org/t/p/w185/cPB3ZMM4UdsSAhNdS4c7ps5nypY.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eThe Terminal\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://image.tmdb.org/t/p/w185/2hFvxCCWrTmCYwfy7yum0GKRi3Y.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eThe Pianist\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://image.tmdb.org/t/p/w185/gEU2QniE6E77NI6lCU6MxlNBvIx.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eInterstellar\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://image.tmdb.org/t/p/w185/x2FJsf1ElAgr63Y3PNPtJrcmpoe.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eArrival\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://image.tmdb.org/t/p/w185/7IiTTgloJzvGI1TAYymCfbfl3vT.jpg\"\u003e\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003eParasite\u003c/strong\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"Films"}]