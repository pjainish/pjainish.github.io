<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>About on Jainish Patel</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in About on Jainish Patel</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Sep 2025 17:02:56 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Foundation Models in Recommendation Systems: How AI Language Models Are Revolutionizing Personalized Recommendations</title>
      <link>http://localhost:1313/posts/foundation-models-in-recsys/</link>
      <pubDate>Thu, 18 Sep 2025 17:02:56 +0530</pubDate>
      <guid>http://localhost:1313/posts/foundation-models-in-recsys/</guid>
      <description>&lt;p&gt;&lt;em&gt;How large language models are transforming recommendation engines from pattern matchers into intelligent reasoning systems that truly understand user preferences&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Keywords: foundation models, recommendation systems, large language models, personalized recommendations, AI recommendation engines, machine learning recommendations, LLM recommendation systems&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;There&amp;rsquo;s something beautifully ironic happening in recommendation systems right now. For decades, we&amp;rsquo;ve been obsessed with learning the perfect embedding - that magical vector representation that captures everything about a user or item in a few hundred dimensions. We&amp;rsquo;ve built elaborate architectures, from collaborative filtering to deep neural networks, all centered around this core idea: learn good embeddings, and recommendations will follow.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Incorporating Ads into Large Language Models: The Hidden Economy of AI Responses</title>
      <link>http://localhost:1313/posts/incorporating-ads-into-llms/</link>
      <pubDate>Mon, 09 Jun 2025 13:00:00 +0530</pubDate>
      <guid>http://localhost:1313/posts/incorporating-ads-into-llms/</guid>
      <description>&lt;p&gt;The moment you ask ChatGPT about a travel destination and it casually mentions a specific hotel booking platform, or when Claude suggests a particular coding tool while helping with your programming question, you&amp;rsquo;re witnessing something fascinating: the intersection of artificial intelligence and advertising. What seems like helpful, neutral advice might actually be the result of careful economic engineering beneath the hood of these language models.&lt;/p&gt;&#xA;&lt;p&gt;This isn&amp;rsquo;t about banner ads cluttering up your chat interface - that would be crude and obvious. Instead, we&amp;rsquo;re talking about something far more sophisticated: weaving promotional content seamlessly into the fabric of AI-generated text itself. It&amp;rsquo;s a practice that&amp;rsquo;s quietly reshaping how we think about AI neutrality, user trust, and the economics of running these incredibly expensive models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multi-Stage Approach to Building Recommender Systems</title>
      <link>http://localhost:1313/posts/multi-stage-recommender-systems/</link>
      <pubDate>Tue, 03 Jun 2025 13:48:45 +0530</pubDate>
      <guid>http://localhost:1313/posts/multi-stage-recommender-systems/</guid>
      <description>&lt;p&gt;Multi-stage recommendation systems break down the challenging task of matching users with relevant items into several sequential phases, each optimizing for different objectives like efficiency, accuracy, and personalization. By progressively narrowing down a vast pool of candidates, applying increasingly complex models, and refining final rankings, these systems achieve scalable and high-quality recommendations even when dealing with billions of users and items (&lt;a href=&#34;https://www.ijcai.org/proceedings/2022/0771.pdf&#34; title=&#34;Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI&#34;&gt;ijcai.org&lt;/a&gt;, &lt;a href=&#34;https://developers.google.com/machine-learning/recommendation/overview/types&#34; title=&#34;Recommendation systems overview | Machine Learning - Google Developers&#34;&gt;developers.google.com&lt;/a&gt;). They mirror how humans might sift through information: first skimming broadly, then considering details, and finally fine-tuning choices. This blog post explores the conceptual foundations of multi-stage recommendation, the distinct roles of each phase, the motivations behind layered architectures, and the real-world trade-offs they address. Along the way, analogies to everyday decision-making, historical parallels from human learning, and references to psychology illustrate how designers balance speed, relevance, and diversity. Finally, we survey challenges such as latency constraints, fairness, and the evolution toward neural re-ranking and hybrid objectives, pointing curious readers to key research papers and practical guides for deeper study.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improving Search Relevance Using Large Language Models</title>
      <link>http://localhost:1313/posts/improving-search-relevance-using-large-language-models/</link>
      <pubDate>Sat, 03 May 2025 13:48:45 +0530</pubDate>
      <guid>http://localhost:1313/posts/improving-search-relevance-using-large-language-models/</guid>
      <description>&lt;p&gt;Search is the invisible backbone of our digital lives. Every time you type a query into Google, search through Netflix&amp;rsquo;s catalog, or hunt for a specific product on Amazon, you&amp;rsquo;re interacting with systems designed to understand what you really want - not just what you literally typed. But here&amp;rsquo;s the thing: traditional search has always been a bit like playing telephone with a robot that only speaks in keywords.&lt;/p&gt;&#xA;&lt;p&gt;Large Language Models are changing this game entirely. They&amp;rsquo;re teaching search systems to understand language the way humans do - with context, nuance, and genuine comprehension. The transformation is so profound that we&amp;rsquo;re witnessing the biggest shift in information retrieval since the invention of the web crawler. Let me show you how this revolution works and why it&amp;rsquo;s reshaping everything from how we shop to how we discover knowledge.&lt;/p&gt;</description>
    </item>
    <item>
      <title>BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers</title>
      <link>http://localhost:1313/posts/bert4rec-sequential-recommendation/</link>
      <pubDate>Fri, 03 Jan 2025 17:23:15 +0530</pubDate>
      <guid>http://localhost:1313/posts/bert4rec-sequential-recommendation/</guid>
      <description>&lt;p&gt;BERT4Rec is a sequential recommendation model that leverages the bidirectional Transformer architecture, originally designed for language tasks, to capture users’ evolving preferences by jointly considering both past and future items in a sequence (&lt;a href=&#34;https://arxiv.org/abs/1904.06690&#34; title=&#34;BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer&#34;&gt;arxiv.org&lt;/a&gt;, &lt;a href=&#34;https://github.com/FeiSun/BERT4Rec&#34; title=&#34;GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...&#34;&gt;github.com&lt;/a&gt;). Unlike earlier unidirectional models that predict the next item only from previous ones, BERT4Rec uses a Cloze-style masking objective to predict missing items anywhere in the sequence, enabling richer context modeling (&lt;a href=&#34;https://arxiv.org/abs/1904.06690&#34; title=&#34;BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer&#34;&gt;arxiv.org&lt;/a&gt;, &lt;a href=&#34;https://github.com/FeiSun/BERT4Rec&#34; title=&#34;GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...&#34;&gt;github.com&lt;/a&gt;). Empirical evaluations on multiple benchmark datasets demonstrate that BERT4Rec often surpasses state-of-the-art sequential models like SASRec, though its performance can depend on careful training schedules and hyperparameter choices (&lt;a href=&#34;https://arxiv.org/abs/2207.07483&#34; title=&#34;A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation&#34;&gt;arxiv.org&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2308.07192&#34; title=&#34;gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling&#34;&gt;arxiv.org&lt;/a&gt;). This post traces the journey from early recommendation methods to the Transformer revolution and the rise of BERT, explains the core ideas behind BERT4Rec, connects them to cognitive analogies of Cloze tests, and discusses experiments, limitations, and future directions. By understanding BERT4Rec’s design and its place in the broader landscape of recommendation, readers can appreciate both its technical elegance and its conceptual roots in language modeling and human learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GitHub Copilot and the Many Dimensions of Developer Productivity: A Deep Dive Into the Real-World Complexity of AI Coding</title>
      <link>http://localhost:1313/posts/arxiv/</link>
      <pubDate>Fri, 03 Jan 2025 17:23:15 +0530</pubDate>
      <guid>http://localhost:1313/posts/arxiv/</guid>
      <description>&lt;h2 class=&#34;heading&#34; id=&#34;github-copilot-and-developer-productivity-what-a-two-year-real-world-study-reveals-about-ais-true-impact&#34;&gt;&#xD;&#xA;  GitHub Copilot and Developer Productivity: What a Two-Year Real-World Study Reveals About AI’s True Impact&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#github-copilot-and-developer-productivity-what-a-two-year-real-world-study-reveals-about-ais-true-impact&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h2&gt;&#xD;&#xA;&lt;h3 class=&#34;heading&#34; id=&#34;introduction-the-ai-productivity-hype&#34;&gt;&#xD;&#xA;  Introduction: The AI Productivity Hype&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction-the-ai-productivity-hype&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h3&gt;&#xD;&#xA;&lt;p&gt;Generative AI, led by tools like GitHub Copilot and ChatGPT, is rapidly changing the software development landscape. Headlines tout &lt;strong&gt;“incredible boosts in developer productivity”&lt;/strong&gt;, while social media buzzes with code snippets generated with simple prompts. But do these promises hold up in real-world, large-team environments? Or is the productivity payoff less clear than the hype suggests? That’s what a groundbreaking two-year mixed-methods study decided to find out: &lt;strong&gt;Is GitHub Copilot living up to the productivity hype, or is the real story much more nuanced?&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Contact</title>
      <link>http://localhost:1313/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/contact/</guid>
      <description>&lt;iframe src=&#34;https://docs.google.com/forms/d/e/1FAIpQLSfYHoQ-YpgCuxPmH1B5LJr0nTjeCAeX8DsxhwXbqAu9Nf6Ejw/viewform?embedded=true&#34; width=&#34;720&#34; height=&#34;1000&#34; frameborder=&#34;0&#34; marginheight=&#34;0&#34; marginwidth=&#34;0&#34;&gt;Loading…&lt;/iframe&gt;</description>
    </item>
    <item>
      <title>Films</title>
      <link>http://localhost:1313/posts/films/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/films/</guid>
      <description>&lt;h3 class=&#34;heading&#34; id=&#34;movies-i-have-watched-and-loved--not-in-order&#34;&gt;&#xD;&#xA;  Movies I have watched and Loved !!! Not in order.&#xD;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#movies-i-have-watched-and-loved--not-in-order&#34;&gt;#&lt;/a&gt;&#xD;&#xA;&lt;/h3&gt;&#xD;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;We live in a box of space and time. Movies are windows in its walls.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&lt;figure class=&#34;&#34;&gt;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&lt;/figure&gt;&#xD;&#xA;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;The Silence of the Lambs&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&lt;figure class=&#34;&#34;&gt;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&lt;/figure&gt;&#xD;&#xA;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;The Godfather&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&lt;figure class=&#34;&#34;&gt;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&lt;/figure&gt;&#xD;&#xA;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pulp Fiction&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&lt;figure class=&#34;&#34;&gt;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&lt;/figure&gt;&#xD;&#xA;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Troy&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&lt;figure class=&#34;&#34;&gt;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&lt;/figure&gt;&#xD;&#xA;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;The Terminal&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&lt;figure class=&#34;&#34;&gt;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&lt;/figure&gt;&#xD;&#xA;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;The Pianist&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&lt;figure class=&#34;&#34;&gt;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&lt;/figure&gt;&#xD;&#xA;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Interstellar&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&lt;figure class=&#34;&#34;&gt;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&lt;/figure&gt;&#xD;&#xA;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Arrival&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&lt;figure class=&#34;&#34;&gt;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&lt;/figure&gt;&#xD;&#xA;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Parasite&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
  </channel>
</rss>
