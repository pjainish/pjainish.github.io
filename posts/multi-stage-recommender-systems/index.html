<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Multi-Stage Approach to Building Recommender Systems | Jainish&#39;s Log</title>
<meta name="keywords" content="">
<meta name="description" content="Multi-stage recommendation systems break down the challenging task of matching users with relevant items into several sequential phases, each optimizing for different objectives like efficiency, accuracy, and personalization. By progressively narrowing down a vast pool of candidates, applying increasingly complex models, and refining final rankings, these systems achieve scalable and high-quality recommendations even when dealing with billions of users and items (ijcai.org, developers.google.com). They mirror how humans might sift through information: first skimming broadly, then considering details, and finally fine-tuning choices. This blog post explores the conceptual foundations of multi-stage recommendation, the distinct roles of each phase, the motivations behind layered architectures, and the real-world trade-offs they address. Along the way, analogies to everyday decision-making, historical parallels from human learning, and references to psychology illustrate how designers balance speed, relevance, and diversity. Finally, we survey challenges such as latency constraints, fairness, and the evolution toward neural re-ranking and hybrid objectives, pointing curious readers to key research papers and practical guides for deeper study.">
<meta name="author" content="">
<link rel="canonical" href="https://pjainish.github.io/posts/multi-stage-recommender-systems/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fd9097ad76bddf704cd630b8ef895f18be00a4239538b567c948b65b650535f.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://pjainish.github.io/assets/images/favicon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://pjainish.github.io/assets/images/favicon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://pjainish.github.io/assets/images/favicon.png">
<link rel="apple-touch-icon" href="https://pjainish.github.io/assets/images/favicon.png">
<link rel="mask-icon" href="https://pjainish.github.io/assets/images/favicon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://pjainish.github.io/posts/multi-stage-recommender-systems/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-79V8YMLKHG"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-79V8YMLKHG');
</script><meta property="og:url" content="https://pjainish.github.io/posts/multi-stage-recommender-systems/">
  <meta property="og:site_name" content="Jainish&#39;s Log">
  <meta property="og:title" content="Multi-Stage Approach to Building Recommender Systems">
  <meta property="og:description" content="Multi-stage recommendation systems break down the challenging task of matching users with relevant items into several sequential phases, each optimizing for different objectives like efficiency, accuracy, and personalization. By progressively narrowing down a vast pool of candidates, applying increasingly complex models, and refining final rankings, these systems achieve scalable and high-quality recommendations even when dealing with billions of users and items (ijcai.org, developers.google.com). They mirror how humans might sift through information: first skimming broadly, then considering details, and finally fine-tuning choices. This blog post explores the conceptual foundations of multi-stage recommendation, the distinct roles of each phase, the motivations behind layered architectures, and the real-world trade-offs they address. Along the way, analogies to everyday decision-making, historical parallels from human learning, and references to psychology illustrate how designers balance speed, relevance, and diversity. Finally, we survey challenges such as latency constraints, fairness, and the evolution toward neural re-ranking and hybrid objectives, pointing curious readers to key research papers and practical guides for deeper study.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-06-03T13:48:45+05:30">
    <meta property="article:modified_time" content="2025-06-03T13:48:45+05:30">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-Stage Approach to Building Recommender Systems">
<meta name="twitter:description" content="Multi-stage recommendation systems break down the challenging task of matching users with relevant items into several sequential phases, each optimizing for different objectives like efficiency, accuracy, and personalization. By progressively narrowing down a vast pool of candidates, applying increasingly complex models, and refining final rankings, these systems achieve scalable and high-quality recommendations even when dealing with billions of users and items (ijcai.org, developers.google.com). They mirror how humans might sift through information: first skimming broadly, then considering details, and finally fine-tuning choices. This blog post explores the conceptual foundations of multi-stage recommendation, the distinct roles of each phase, the motivations behind layered architectures, and the real-world trade-offs they address. Along the way, analogies to everyday decision-making, historical parallels from human learning, and references to psychology illustrate how designers balance speed, relevance, and diversity. Finally, we survey challenges such as latency constraints, fairness, and the evolution toward neural re-ranking and hybrid objectives, pointing curious readers to key research papers and practical guides for deeper study.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://pjainish.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Multi-Stage Approach to Building Recommender Systems",
      "item": "https://pjainish.github.io/posts/multi-stage-recommender-systems/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Multi-Stage Approach to Building Recommender Systems",
  "name": "Multi-Stage Approach to Building Recommender Systems",
  "description": "Multi-stage recommendation systems break down the challenging task of matching users with relevant items into several sequential phases, each optimizing for different objectives like efficiency, accuracy, and personalization. By progressively narrowing down a vast pool of candidates, applying increasingly complex models, and refining final rankings, these systems achieve scalable and high-quality recommendations even when dealing with billions of users and items (ijcai.org, developers.google.com). They mirror how humans might sift through information: first skimming broadly, then considering details, and finally fine-tuning choices. This blog post explores the conceptual foundations of multi-stage recommendation, the distinct roles of each phase, the motivations behind layered architectures, and the real-world trade-offs they address. Along the way, analogies to everyday decision-making, historical parallels from human learning, and references to psychology illustrate how designers balance speed, relevance, and diversity. Finally, we survey challenges such as latency constraints, fairness, and the evolution toward neural re-ranking and hybrid objectives, pointing curious readers to key research papers and practical guides for deeper study.\n",
  "keywords": [
    
  ],
  "articleBody": "Multi-stage recommendation systems break down the challenging task of matching users with relevant items into several sequential phases, each optimizing for different objectives like efficiency, accuracy, and personalization. By progressively narrowing down a vast pool of candidates, applying increasingly complex models, and refining final rankings, these systems achieve scalable and high-quality recommendations even when dealing with billions of users and items (ijcai.org, developers.google.com). They mirror how humans might sift through information: first skimming broadly, then considering details, and finally fine-tuning choices. This blog post explores the conceptual foundations of multi-stage recommendation, the distinct roles of each phase, the motivations behind layered architectures, and the real-world trade-offs they address. Along the way, analogies to everyday decision-making, historical parallels from human learning, and references to psychology illustrate how designers balance speed, relevance, and diversity. Finally, we survey challenges such as latency constraints, fairness, and the evolution toward neural re-ranking and hybrid objectives, pointing curious readers to key research papers and practical guides for deeper study.\nIntroduction: A Personal Reflection on Systems of Thought When I first encountered recommendation systems, I was struck by how they mirrored the way we navigate choices in daily life. Whether picking a movie on a streaming platform or selecting a restaurant in an unfamiliar city, we often start by skimming broad categories, then gradually focus on specific options, and finally make subtle refinements based on our mood or context. In my own journey—studying neural networks, building small-scale recommenders, and later reading about industrial-scale deployments—I realized that the most robust systems also follow a layered, multi-step process. Each stage builds on the previous one, balancing the need for speed with the quest for relevance.\nEarly in my learning, I faced the temptation to design a single, “perfect” model that could solve everything at once. But this naive approach quickly ran into practical barriers: datasets with millions of users and items, strict latency requirements, and the ever-present engineering constraints of limited compute. Over time, I discovered that breaking the problem into stages not only made systems more scalable but also allowed each subcomponent to focus on a clear objective—much like how one might draft a rough outline before writing a polished essay. This approach felt natural, almost human. It honors the way we refine our thinking: brainstorm broadly, narrow the field, then polish the final answer.\nIn this post, inspired by Andrej Karpathy’s calm, thoughtful narrative style, I want to share the conceptual palette of multi-stage recommendation systems. My aim is to offer clarity over complexity, distilling intricate algorithms into intuitive ideas and drawing parallels to broader human experiences. Whether you are a curious student, an engineer venturing into recommender research, or simply someone intrigued by how machines learn to predict our preferences, I hope this narrative resonates with your own learning journey.\nUnderstanding Multi-Stage Recommendation Systems The Core Idea: Divide and Conquer At its simplest, a recommendation system tries to answer: “Given a user, which items will they find relevant?” When the number of potential items is enormous—often in the hundreds of millions—applying a single complex model to score every possible user-item pair quickly becomes infeasible. Multi-stage recommendation systems tackle this by splitting the problem into sequential phases, each with a different scope and computational budget (ijcai.org, developers.google.com).\nCandidate Generation (Retrieval): Reduce a massive corpus of items to a smaller, manageable subset—often from millions to thousands. Scoring (Ranking): Use a more refined model to evaluate and rank these candidates, selecting a handful (e.g., 10–50) for final consideration. Re-Ranking (Refinement): Apply an even richer model, possibly incorporating contextual signals, diversity constraints, or business rules, to order the final set optimally for display. Some architectures include additional phases—such as pre-filtering by broad categories or post-processing for personalization and fairness—leading to four-stage or more elaborate pipelines (resources.nvidia.com). But the essential principle remains: start broad and coarse, then iteratively refine.\nThis cascade mirrors human decision-making. Imagine shopping online for a book: you might first browse top genres (candidate generation), then look at bestsellers within your chosen genre (scoring), and finally read reviews to pick the exact title (re-ranking). Each step focuses on a different level of granularity and uses different cues.\nWhy Not a Single Model? One might ask: why not build one powerful model that directly scores every item? In theory, a deep neural network with billions of parameters could capture all signals—user preferences, item attributes, temporal trends, social context. Yet in practice:\nComputational Cost: Scoring billions of items per user request is prohibitively expensive. Even if each prediction took a microsecond, processing a single query over 100 million items would take over a minute. Latency Constraints: Most user-facing systems must respond within tens to a few hundred milliseconds to maintain a fluid experience. Scalability: As user and item counts grow, retraining and serving a monolithic model becomes unwieldy, requiring massive hardware infrastructure. Flexibility: Separate stages allow engineers to swap, update, or A/B test individual components (e.g., try a new candidate generator) without rebuilding the entire system. Thus, multi-stage pipelines offer a practical compromise: coarse but fast filtering followed by progressively more accurate but slower models, ensuring that latency stays within acceptable bounds while maintaining high recommendation quality (ijcai.org, developers.google.com).\nHistorical Context: From Heuristics to Neural Pipelines Early recommenders—dating back to collaborative filtering in the mid-1990s—often endured all-to-all scoring within a manageable dataset size. But as platforms like Amazon, Netflix, and YouTube scaled to millions of users and items, engineers introduced multi-step processes. For instance, Netflix’s 2006 recommendation infrastructure already featured a two-tier system: a “neighborhood” retrieval step using approximate nearest neighbors, followed by a weighted hybrid model for ranking (natworkeffects.com, ijcai.org).\nOver time, as deep learning matured, architectures evolved from simple matrix factorization and linear models to complex neural networks at each stage. Today, many systems leverage separate retrieval networks (e.g., dual-tower architectures) for candidate generation, gradient-boosted or neural ranking models in the scoring phase, and transformer-based or contextual deep models for re-ranking (arxiv.org, ijcai.org). This layered approach reflects both the historical progression of the field and the perpetual trade-off between computation and accuracy.\nAnatomy of a Multi-Stage Pipeline Candidate Generation Purpose and Intuition The candidate generation stage answers: “Which items out of billions might be relevant enough to consider further?” It must be extremely fast while maintaining reasonable recall—meaning it should rarely miss items that truly match user interests. Think of it as casting a wide net before trimming it down.\nAnalogy: Imagine you’re researching scholarly articles on “graph neural networks.” You might start by searching on Google Scholar with broad keywords (“graph neural network deep learning”), pulling up thousands of results. You don’t read each paper in detail; instead, you let the search engine shortlist a few hundred of the most relevant, perhaps based on citation counts or keyword frequency. These form the candidate set for deeper review.\nCommon Techniques Approximate Nearest Neighbors (ANN): Users and items are embedded in a shared vector space. The system retrieves the nearest item vectors to a given user vector using methods like locality-sensitive hashing (LSH) or graph-based indexes (e.g., HNSW). This approach assumes that a user’s preference can be captured by proximity in the embedding space (ijcai.org, developers.google.com).\nHeuristic Filtering / Content-Based Selection: Use metadata or simple rules—for instance, filter by item category (e.g., only show “science fiction” books), geographic restrictions, or availability. These heuristics can further narrow the pool before applying more expensive methods.\nPre-Computed User-to-Item Mappings: Some systems maintain pre-computed lists, such as “frequently co-viewed” or “users also liked,” based on historical co-occurrence. These candidate sets can be quickly unioned and deduplicated.\nMulti-Vector Retrieval: Instead of a single user vector, some platforms compute multiple specialized retrieval vectors—for example, one for long-term interests and another for short-term session context—and aggregate their candidate sets for higher recall (developers.google.com).\nBecause candidate generation often retrieves thousands of items, these methods must operate in logarithmic or sub-linear time relative to the entire catalog size. Graph-based ANN indexes, for example, offer fast lookups even as catalogs scale to tens of millions.\nDesign Considerations Recall vs. Latency: Aggressive pruning (retrieving fewer candidates) reduces later computation but risks losing relevant items. Conversely, broad recall increases the workload for downstream stages. Freshness and Exploration: Relying solely on historical co-occurrences can lead to stale recommendations. Injecting a degree of randomness or exploration can help surface new items. Cold Start: New users (no history) or new items (no interactions) must be handled via content-based features or hybrid heuristics. Budget Allocation: Systems often distribute retrieval capacity across multiple candidate sources—for instance, a fixed number from item-to-item co-visitation lists, another portion from ANN, and some from heuristic rules—to balance recall diversity. Scoring and Ranking From Thousands to Tens Once candidate generation outputs a pool (e.g., 1,000–10,000 items), the scoring stage uses a moderately complex model to assign scores reflecting the user’s likelihood of engaging with each item. The goal is to rank and select a smaller subset (often 10–100 items) for final display (developers.google.com, ijcai.org).\nAnalogy: If candidate generation is skimming the first page of Google Scholar results, scoring is akin to reading abstracts and deciding which 10–20 papers to download for deeper reading. You still work relatively quickly, but you consider more details—abstract content, co-authors, publication venue.\nTypical Modeling Approaches Gradient-Boosted Decision Trees (GBDT): Popular for their interpretability and efficiency, GBDTs like XGBoost take a set of engineered features (user demographics, item attributes, interaction history) to produce a relevance score. They balance speed with decent accuracy and can be trained on huge offline datasets.\nTwo-Tower Neural Networks (Dual-Tower): Separate “user tower” and “item tower” networks embed users and items into vectors; their dot product estimates relevance. Because item embeddings can be pre-computed, this model supports fast online scoring with vector lookups followed by simple arithmetic (ijcai.org, arxiv.org). Dual-tower models can incorporate features like user behavior sequences, session context, and item metadata.\nCross-Interaction Neural Models: More expressive than dual-tower, these models take the user and item features jointly (e.g., via concatenation) and pass them through deep layers to capture fine-grained interactions. However, they are slower and thus applied only to the reduced candidate pool. Models like Deep \u0026 Cross Networks (DCN), DeepFM, or those with attention mechanisms fall into this category.\nSession-Based Models: For domains where session context matters (e.g., news or e-commerce), recurrent neural networks (RNNs) or transformers can capture sequential patterns in user interactions. These models score candidates based on both long-term preferences and recent session behavior.\nPractical Trade-Offs Feature Engineering vs. Representation Learning: Hand-crafted features (e.g., user age, categorical encodings) can boost GBDT performance but require significant domain knowledge. Neural models can automatically learn representations but demand more compute and careful tuning. Offline Training vs. Online Serving: Ranking models are often retrained daily or hourly on fresh data. Keeping model updates in sync with the real-time data pipeline (e.g., streaming user actions) is non-trivial. Explore/Exploit Balance: Purely optimizing click-through rate (CTR) can overemphasize already popular items. Injecting exploration (e.g., using bandit algorithms) in this stage can help promote diversity and long-tail items. Re-Ranking and Refinement The Final Polish After scoring, the top N candidates (often 10–50) are ready for final polishing. Re-ranking applies the most sophisticated models and business logic to order items precisely for display (ijcai.org, assets-global.website-files.com). This phase often considers context signals unavailable earlier—such as time of day, device type, or recent events—and optimizes for multiple objectives simultaneously.\nAnalogy: If scoring chooses 15 promising articles to read, re-ranking is carefully ordering them on your coffee table, perhaps placing groundbreaking studies that align with your current project front and center, while positioning more exploratory reads slightly lower.\nKey Components Contextual Signals: Real-time context like current browsing session, geo-location, or device battery status can influence final ordering. For instance, short-form video recommendations might prioritize quick snippets if the user’s device is on low battery.\nDiversity and Fairness Constraints: Purely greedy ranking can create echo chambers or unfairly bias against less popular content creators. Re-ranking modules may enforce diversity (e.g., ensure at least one new artist in a music playlist) or fairness (e.g., limit how often the same content provider appears) (ijcai.org, assets-global.website-files.com).\nMulti-Objective Optimization: Beyond CTR, systems often balance metrics like dwell time, revenue, or user retention. Techniques like Pareto optimization or weighted scoring can integrate multiple objectives, with re-ranking serving as the phase to reconcile potential conflicts.\nPairwise and Listwise Learning-to-Rank: Instead of treating each candidate independently, re-ranking can use pairwise (e.g., RankNet) or listwise (e.g., ListNet, LambdaMART) approaches that optimize the relative ordering of candidates based on user feedback signals like click sequences or dwell times.\nLatency Buffer: Since the re-ranking phase handles only a small number of items, it can afford deeper models (e.g., transformers, graph neural networks) while still keeping total system latency within tight deadlines.\nAdditional Layers and Enhancements Many industrial pipelines incorporate extra stages beyond the canonical three. Examples include:\nPre-Filtering by Coarse Attributes: Quickly exclude items based on coarse filters like age restrictions, language, or membership level before candidate generation. Post-Processing for Exploration: Randomly inject sponsored content or fresh items after re-ranking to avoid overconfidence in the model and encourage serendipity. Online A/B Testing and Logging: Between each stage, systems often log intermediate scores and decisions to feed into offline analysis or to enable rapid A/B testing of algorithmic tweaks (resources.nvidia.com). Personalization Layers: Some platforms add user segments or clusters at various stages, ensuring that models can specialize to subpopulations without retraining entirely unique pipelines per user. By designing these layered architectures, engineers can isolate concerns—tuning candidate retrieval separately from ranking or fairness adjustments—making debugging and maintenance far more manageable.\nMotivations Behind Layered Architectures Scalability and Efficiency When catalogs contain millions or billions of items, exhaustive scoring for each user request is impractical. Multi-stage pipelines allow early pruning of irrelevant items, ensuring that only a small subset traverses the most expensive models (ijcai.org, developers.google.com). This design echoes divide-and-conquer algorithms in computer science, where a large problem is split into smaller subproblems that are easier to solve.\nConsider a scenario: an e-commerce site with 100 million products. If we scored all products for each user visit, even at one microsecond per score, it would take 100 seconds—far too slow. By retrieving 1,000 candidates (taking maybe 5 milliseconds) and then scoring those with a moderately complex model (say 1 millisecond each), we reduce compute to a fraction, fitting within a 100-millisecond latency budget.\nAccuracy vs. Computation Trade-Off Each stage in the pipeline can use progressively more expressive models, trading off compute for accuracy only when necessary. Candidate generation might use a fast, approximate algorithm with coarse embeddings. Scoring might use gradient-boosted trees or shallow neural nets. Re-ranking can apply deep, context-rich models that consider subtle interactions. This “budgeted” approach ensures that compute resources are allocated where they yield the biggest benefit—on a small subset of high-potential items.\nMoreover, separating concerns enables each phase to be optimized independently. If a new breakthrough emerges in dual-tower retrieval, you can update the candidate generator without touching the ranking model. Conversely, if a novel re-ranking strategy arises (e.g., graph neural networks capturing social influence), you can incorporate it at the final stage without disrupting upstream retrieval.\nSystem Debuggability and Experimentation Layered architectures naturally provide inspection points. Engineers can log candidate sets, intermediate scores, and final ranks for offline analysis. This visibility aids in diagnosing issues—did the candidate generator omit relevant items? Did the ranking model misestimate relevance? Having multiple stages allows targeted A/B tests: you might experiment with a new retrieval algorithm for half of users while keeping the ranking pipeline constant, isolating the effect of retrieval improvements on overall metrics.\nSimilarly, multi-stage pipelines support incremental rollouts. A new model can be introduced initially in the re-ranking phase, gradually moving upstream once it proves effective. This staged deployment minimizes risk compared to replacing a monolithic system all at once.\nAligning Business Objectives Different phases can optimize different objectives. For example, candidate generation may prioritize diversity or novelty to avoid echo chambers, scoring may focus on CTR maximizing engagement, and re-ranking may adjust for revenue or long-term retention. By decoupling stages, systems can incorporate business rules—e.g., promoting high-margin items or fulfilling contractual obligations for sponsored content—without entangling them with fundamental retrieval logic.\nAnalogies and Human-Centric Perspectives The Library Research Analogy Searching for information in a digital catalog is akin to walking through a library:\nBrowsing the Stacks (Candidate Generation): You wander down aisles labeled by subject areas, pulling books that look relevant based on their spine labels. You might grab twenty books that seem promising but don’t know their exact details yet.\nSkimming Table of Contents (Scoring): At your table, you flip through these books’ tables of contents, perhaps reading a few introductory paragraphs to assess whether they deeply cover your topic.\nReading a Chapter or Two (Re-Ranking): After narrowing to five books, you read a key chapter or two to decide which is most informative for your current research question.\nThis process ensures efficiency—you don’t read every page of every book. Instead, you refine your scope gradually, allocating your reading time where it matters most. Multi-stage recommenders mimic this approach, trading off broad coverage with depth as the pipeline progresses.\nHuman Learning and Iterative Refinement The educational psychologist Lev Vygotsky described learning as moving through a “zone of proximal development,” where zones represent tasks that a learner can complete with guidance. In recommendation pipelines, early stages guide the system to promising areas (the broad zone), while later stages apply sophisticated “guidance” (complex models and context) to refine choices. This layered attention mirrors how teachers first introduce broad concepts before diving into detailed analysis.\nMoreover, our brains rarely process all sensory inputs deeply. We unconsciously filter peripheral stimuli (“candidate generation”), focus attention on salient objects (“scoring”), and then allocate cognitive resources to detailed examination (“re-ranking”) only when necessary. This cognitive economy principle underlies why layered sampling and enrichment work so effectively in machine systems.\nDeep Dive into Each Stage Candidate Generation: Casting the Wide Net Mathematical Formulation Formally, let $U$ be the set of users and $I$ the set of all items. Candidate generation seeks a function $f_{\\text{gen}}: U \\to 2^I$ that maps each user $u$ to a subset $C_u \\subset I$ of size $k$, where $k \\ll |I|$. The goal is for $C_u$ to have high recall—including most items that the final system would deem relevant—while ensuring retrieval time $T_{\\text{gen}}(u)$ is minimal.\nIn practice, engineers often pre-compute user embeddings $\\mathbf{e}_u \\in \\mathbb{R}^d$ and item embeddings $\\mathbf{e}_i \\in \\mathbb{R}^d$ using some training signal (e.g., co-clicks or purchases). Candidate generation then solves:\n$$ C_u = \\text{TopK}\\bigl{\\text{sim}(\\mathbf{e}_u, \\mathbf{e}_i),\\ i \\in I\\bigr}, $$\nwhere $\\text{sim}$ is a similarity metric (dot product or cosine similarity). To avoid $O(|I|)$ computation, approximate nearest neighbor (ANN) algorithms (e.g., HNSW, FAISS) partition or graph-index the embedding space to return approximate TopK in $O(\\log |I|)$ or better (ijcai.org, developers.google.com).\nPractical Example: YouTube’s “Candidate Generation” YouTube’s production system handles billions of videos and over two billion monthly users. Their candidate generation phase uses multiple retrieval sources: a “personalized candidate generator” (a deep neural network that outputs item vectors), “idf-based candidate generators” for rare or niche videos, and “demand generation” heuristics for fresh content. Each source retrieves thousands of candidates, which are then merged and deduplicated before feeding into the ranking stage (ijcai.org, developers.google.com).\nBy combining diverse retrieval sources, YouTube balances high recall (including long-tail videos) with computational feasibility. The embeddings incorporate signals like watch history, search queries, and video metadata (tags, descriptions, language).\nChallenges in Candidate Generation Cold Start for Items: New items have no embeddings until they accrue interactions. Content-based attributes (text descriptions, images) can bootstrap embeddings. Cold Start for Users: For anonymous or new users, systems might rely on session-based signals or demographic approximations. Embedding Drift: As user preferences evolve, embeddings must be updated frequently. Real-time or near-real-time embedding updates can be expensive. Some systems use “approximate” embeddings that update hourly or daily. Recall vs. Precision: While candidate generation values recall over precision (it’s okay to include some irrelevant items), retrieving too many increases downstream costs. Engineers often tune the retrieval size $k$ based on latency budgets. Scoring and Ranking: Separating Signal from Noise Formalizing the Ranking Problem Given user $u$ and candidate set $C_u = {i_1, i_2, \\dots, i_k}$, ranking seeks a scoring function $f_{\\text{rank}}(u, i)$ that assigns a real-valued score to each $(u, i)$. The final ranked list is obtained by sorting $C_u$ in descending order of $f_{\\text{rank}}(u, i)$. Here, the focus is on maximizing a utility metric—click-through rate (CTR), watch time, revenue—subject to constraints like computational budget and fairness policies.\nRepresentational Approaches Gradient-Boosted Trees (GBDT): Features can include user demographics, item popularity, item age (freshness), session duration, historical click rates, and interactions between them. GBDT models handle heterogeneous input features and often outperform simple linear models in tabular settings. For instance, LinkedIn’s ranking models use GBDTs to process thousands of features for candidate items, balancing precision and latency (ijcai.org, linkedin.com).\nTwo-Tower Neural Networks: These models learn embedding functions $\\phi_u(\\cdot)$ and $\\phi_i(\\cdot)$ that map user and item features to a dense vector space. The relevance score is $f_{\\text{rank}}(u, i) = \\phi_u(\\mathbf{x}_u)^\\top \\phi_i(\\mathbf{x}_i)$. Because item embeddings $\\phi_i(\\mathbf{x}_i)$ can be pre-computed offline for all items, serving involves a user embedding lookup and a nearest-neighbor search among item embeddings. While two-tower excels in retrieval, it also serves as a ranking model when run over a small candidate set (ijcai.org, arxiv.org).\nCross-Interaction Neural Architectures: To capture complex interactions, models like DeepFM or Wide \u0026 Deep networks combine embeddings with feature crosses and joint layers. For example, the Deep \u0026 Cross Network (DCN) explicitly models polynomial feature interactions, improving ranking quality at the cost of higher inference time. Such models are viable when ranking only a limited candidate set.\nSequence Models: In scenarios where the user’s recent behavior is paramount (e.g., news or music recommendations), recurrent neural networks (RNNs) or transformers encode the session sequence. The model’s hidden state after processing recent clicks or listens forms $\\phi_u$, which then interacts with candidate item embeddings. These sequence-aware rankers can capture trends like “if the user listened to fast-paced songs recently, recommend similar tracks” (ijcai.org, dl.acm.org).\nEngineering Considerations Feature Freshness: To capture evolving user interests, some features (like recent click counts) must be updated in near real-time. Engineering streaming pipelines that supply fresh features to ranking models is a significant challenge. Online vs. Offline Scoring: Some ranking scores can be computed offline (e.g., item popularity), while others must be computed online given session context. Balancing pre-computation and real-time inference is key to meeting latency requirements. Regularization and Overfitting: Because the ranking model sees only a filtered candidate set, it risks learning biases introduced by the retrieval stage. Engineers use techniques like exploration (random candidate injections) and regularization (dropout, weight decay) to mitigate such feedback loops. Re-Ranking: The Art of Final Touches Contextual and Business-Aware Refinements By the time candidates reach re-ranking, they number perhaps a dozen. This reduced set enables the system to apply the most expensive and context-rich models, considering signals that were too costly earlier:\nUser’s Real-Time Context: Current weather, device type, screen size, or even network speed can influence which items make sense. For example, a video platform might demote 4K videos if the user’s bandwidth appears constrained. Temporal Patterns: If an item is trending due to a breaking news event, re-ranking can upweight it even if it didn’t score highest in the ranking model. Additionally, the re-ranking stage often integrates final business rules:\nSponsored Content and Ads: Platforms typically must display a minimum number of sponsored items or promote partners. Re-ranking can adjust scores to ensure contractual obligations are met. Diversity Constraints: To prevent monotony and filter bubbles, systems may enforce that top N items span multiple content categories or creators (ijcai.org, assets-global.website-files.com). Fairness and Ethical Safeguards: Ensuring that minority or new creators receive exposure may require explicit adjustments. For instance, a music streaming service might limit how many tracks by a single artist appear in a daily playlist, or an e-commerce site might promote ethically sourced products. Learning-to-Rank Approaches While earlier stages often rely on pointwise prediction (predicting the utility of each item independently), re-ranking can adopt more sophisticated pairwise or listwise approaches:\nPairwise Ranking (e.g., RankNet, RankSVM): The model learns from pairs of items, optimizing the probability that a more relevant item is ranked above a less relevant one. This typically uses a loss function that encourages correct ordering of pairs based on user clicks or dwell times. Listwise Ranking (e.g., ListNet, LambdaMART): These methods consider the entire list of candidates jointly, optimizing metrics directly related to list order—such as nDCG (normalized Discounted Cumulative Gain). Listwise losses can be more aligned with final business metrics but are often harder to optimize and require careful sampling strategies. Incorporating Multi-Objective Optimization In many scenarios, platforms must juggle multiple goals: user engagement (clicks or watch time), revenue (ad impressions or purchases), and long-term retention. Re-ranking offers the flexibility to integrate these objectives:\nScalarization: Combine multiple metrics into a single weighted score. For example, $\\text{score} = \\alpha \\times \\text{CTR} + \\beta \\times \\text{Expected Revenue}$. Weights $\\alpha, \\beta$ can be tuned to match business priorities. Pareto Front Methods: Instead of combining objectives, identify items that lie on the Pareto frontier—meaning no other item is strictly better in all objectives. Re-ranking then selects from this frontier based on context. Constrained Optimization: Define primary objectives (e.g., CTR) while enforcing constraints on secondary metrics (e.g., minimum diversity or fairness thresholds). This can be formulated as linear or integer programming problems solved at re-ranking time. Beyond Three Stages: Four or More Some platforms extend multi-stage pipelines further:\nCoarse Filtering (Pre-Retrieval): Filter by extremely simple rules—e.g., language, age rating, or membership level—before computing any embeddings. This reduces both retrieval and ranking load. Primary Retrieval (Candidate Generation). Secondary Retrieval (Cross-Modal or Contextual): Some systems perform a second retrieval focusing on a different signal. For instance, after retrieving general candidates from a content-based model, they may retrieve additional items based on collaborative co-click signals and then union the two sets. Ranking (Scoring). Re-Ranking (Refinement). Post-Processing (Online Exploration/Injection): Finally, inject a small fraction of random or specially curated items—like sponsored content or editorial picks—into the ranked list before display (resources.nvidia.com, assets-global.website-files.com). NVIDIA’s Merlin architecture outlines a four-stage pipeline where separate retrieval stages handle different signals, reflecting real-world complexities in balancing content freshness, personalization, and business rules (resources.nvidia.com).\nChallenges and Design Trade-Offs Recall and Precision Balance High Recall Need: If candidate generation misses relevant items, downstream stages cannot recover them. Low recall hurts both immediate relevance and long-term user satisfaction. Precision Constraints: However, retrieving too many candidates inflates computational costs. Designers must find an operating point where recall is sufficiently high while keeping the candidate set size within resource budgets. Finding this balance often involves extensive offline evaluation: sampling user queries, varying retrieval thresholds, and measuring recall of items that ultimately led to clicks or conversions. Techniques like “held-out validation” and “information retrieval metrics” (e.g., recall@K, MRR) guide engineers in tuning retrieval hyperparameters.\nLatency and System Complexity Every stage introduces latency. Even if candidate generation and ranking operate in microseconds, re-ranking complex item sets with deep models can push total response time beyond acceptable limits. Systems often target end-to-end latencies under 100–200 milliseconds for web-based recommendations (ijcai.org). To meet these SLAs:\nParallelization: Some stages run in parallel—e.g., Katz–Schneider retrieval that fetches both content-based and collaborative candidates simultaneously before merging. Caching: Popular users or items may have pre-computed candidate lists or ranking scores. However, caching fresh recommendations is tricky when user activity changes rapidly. Hardware Acceleration: GPUs or specialized accelerators can speed up neural inference, especially for deep re-ranking models. Yet they add operational complexity and cost. Graceful Degradation: Under high load, systems might skip the re-ranking phase or employ simplified ranking to ensure responsiveness, accepting a temporary drop in accuracy. Cold Start and Evolving Data New Users: Without historical interactions, candidate generation struggles. Common strategies include asking onboarding questions, using demographic-based heuristics, or emphasizing popular items to collect initial data. New Items: Newly added content has no interaction history. Content-based features (text embeddings, image features) or editorial tagging can bootstrap embeddings. Some systems also inject fresh items randomly into candidate sets to gather user feedback quickly. Data Drift: User interests and item catalogs evolve. Periodic retraining—daily or hourly—helps keep models up to date, but retraining at scale can strain infrastructure. Incremental training or online learning frameworks attempt to update models continuously, though they raise concerns about model stability and feedback loops. Fairness, Bias, and Ethical Considerations Multi-stage pipelines can inadvertently amplify biases:\nPopularity Bias: Early retrieval might preferentially surface popular items, pushing niche or new content out of the pipeline entirely. Demographic Bias: If training data reflect societal biases—e.g., gender or racial preferences—models might perpetuate or exacerbate inequities. For instance, a music recommender might under-represent certain genres popular among minority communities. Feedback Loops: When users are repeatedly shown similar content, they have fewer opportunities to diversify their interests. This cyclical effect traps them in a feedback loop that reinforces initial biases. To address these issues, re-ranking often incorporates fairness constraints—e.g., ensuring a minimum representation of under-represented groups—or diversity-promoting objectives (ijcai.org, assets-global.website-files.com). Engineers may also use causal inference to disentangle correlation from true preference signals, though this remains an active research area.\nEvaluation Metrics and Online Experimentation Measuring success in multi-stage systems is multifaceted:\nOffline Metrics:\nRecall@K: Fraction of truly relevant items that appear in the top K candidates (ijcai.org). NRMSE (Normalized Root Mean Squared Error): For predicting ratings or continuous outcomes. nDCG (Normalized Discounted Cumulative Gain): Accounts for position bias in ranked lists. Online Metrics (A/B Testing):\nClick-Through Rate (CTR): The fraction of recommendations that lead to clicks. Engagement Time/Dwell Time: Time spent interacting with recommended content. Conversion Rate (CR): Purchases or desired downstream actions. Retention/Lifetime Value (LTV): Long-term impact of recommendations on user loyalty. A/B tests are critical because offline proxies often fail to capture user behavior complexities. For example, a model that improves offline nDCG may inadvertently reduce long-term engagement if it over-emphasizes certain item types.\nMaintaining Freshness and Diversity Balancing relevance with freshness ensures that users see timely content, not stale favorites. Common techniques include:\nTime Decay Functions: Decrease the weight of interactions as they age, ensuring that recent trending items receive higher retrieval priority. Dynamic Exploration Schedules: Temporarily boost undervalued content or categories, measuring user responses to decide if these should enter regular circulation. Diversity Constraints: Enforce constraints like “no more than two items from the same category in the top-5 recommendations” to avoid monotony (ijcai.org, assets-global.website-files.com). With rapid shifts in user interests—such as viral trends on social media—systems must adapt quickly without overreacting to noise.\nReal-World Case Studies YouTube’s Three-Stage Pipeline YouTube’s recommendation engine processes over 500 hours of video uploads per minute and serves billions of daily watch sessions. Their pipeline typically comprises:\nCandidate Generation: Several retrieval sources—embedding-based ANN, session-based heuristics, and recent trending signals—produce a combined set of 1,000–2,000 videos (ijcai.org, developers.google.com). Scoring: A candidate omnivorous ranking model (COR) scores each video using a two-tower architecture supplemented by contextual features like watch history, device type, and time of day. The top ~50 videos are selected for re-ranking. Re-Ranking: A complex deep model (often leveraging attention mechanisms to model user-video interactions along with session context) refines the ordering, ensuring diversity and personal relevance. Business rules inject some fresh or sponsored videos at this stage (ijcai.org, assets-global.website-files.com). YouTube continuously A/B tests changes, measuring not just immediate watch time but also long-term retention and channel subscriptions. Their hierarchical approach allows them to serve highly personalized content at massive scale without exceeding latency budgets (often under 100 ms for initial retrieval and 200 ms end-to-end) (ijcai.org, developers.google.com).\nLinkedIn’s News Feed Recommendations LinkedIn’s feed blends content recommendations (articles, posts) with job suggestions and ads. Their multi-stage system includes:\nPre-Filtering: Exclude posts in languages the user doesn’t understand or items violating policies. Candidate Generation: Retrieve posts based on user’s network interactions—e.g., posts by first-degree connections, followed influencers, or articles matching user’s interests. This stage uses graph-based traversal along the social graph and content-based retrieval for topical relevance (linkedin.com, ijcai.org). Scoring: A gradient-boosted model evaluates each post’s relevance based on hundreds of features—user’s skill tags, past engagement patterns, recency, and even inferred career stage. The model outputs a score predicting “probability of positive engagement” (like click, comment, or share). Re-Ranking: A pairwise learning-to-rank module refines ranking by optimizing for relative ordering. It also enforces that no more than two successive posts from the same publisher appear, promoting diversity among content creators. LinkedIn’s system must juggle diverse content formats—text articles, videos, job postings, ads—each with different engagement signals. By decoupling retrieval, ranking, and re-ranking, they can optimize specialized models for each format and then unify them under a common final re-ranker.\nTaobao’s Four-Stage Architecture Taobao, one of the world’s largest e-commerce platforms, serves over a billion monthly active users. Their multi-stage architecture often follows:\nWide \u0026 Narrow Retrieval: A combination of content-based filtering (e.g., category-level retrieval) and collaborative retrieval (e.g., user–item co-click graphs) yields ~10,000 candidates. Coarse Ranking: A GBDT model with engineered features ranks these candidates to a shortlist of ~1,000. Fine Ranking: A deep neural network—often combining convolutional layers for image features, embedding layers for text attributes, and attention modules to capture user-item interactions—reduces to ~50 items. Re-Ranking with Business Rules: Final adjustments inject promotions, ensure seller diversity, apply dayparting rules (e.g., preferring essential goods in morning and entertainment items in evening), and optimize for multiple objectives like conversion rate, gross merchandise volume (GMV), and click yield (ijcai.org, dl.acm.org). Because Taobao’s inventory changes rapidly (with thousands of new items added hourly), their system employs robust feature pipelines to update item embeddings in near real-time. The four-stage design allows them to integrate new items into candidate pools via content-based features, then gradually gather interaction data to feed collaborative signals back into retrieval.\nTowards the Future: Evolving Multi-Stage Paradigms Neural Re-Ranking and Contextual Fusion Recent research in neural re-ranking focuses on richer representations and contextual fusion:\nTransformer-Based Re-Rankers: Models like BERT or its variants, finetuned for recommendation tasks, can process candidate sets jointly, capturing inter-item relationships (e.g., “these two movies are sequels”) and user context. IJCAI’s 2022 review notes that transformer-based re-rankers can significantly outperform traditional MLP or tree-based models, albeit at higher computational cost (ijcai.org). Multi-Modal Fusion: E-commerce and social media often benefit from combining visual, textual, and numerical features. Graph neural networks (GNNs) can propagate signals across user–item graphs, capturing higher-order interactions. Eﬀective fusion of these signals in the re-ranking stage leads to more nuanced final lists (ijcai.org, dl.acm.org). Session-Aware Re-Ranking: In domains where session context evolves rapidly (e.g., news or music streaming), re-ranking models incorporate session sequences as part of the final scoring. Models like “Transformer4Rec” attend over both candidate items and session history, refining lists to match transient user intent. Online Learning and Bandit Algorithms Traditionally, multi-stage pipelines train offline on historical data and then serve static models online. Emerging trends include:\nContextual Bandits in Ranking: Between the scoring and re-ranking stages, some systems integrate bandit algorithms that dynamically adjust item scores based on real-time click feedback, balancing exploration (showing new or uncertain items) and exploitation (showing high-confidence items). Continual Learning: Instead of periodic batch retraining, models update incrementally as new interactions arrive. This reduces lag between data generation and model applicability, improving responsiveness to changing user preferences. Causal Inference and Debiasing Recommendation systems often suffer from biases introduced by historical data—popularity bias, presentation bias (items shown higher get more clicks), and selection bias (users only see a subset of items). Researchers are exploring causal methods:\nInverse Propensity Scoring (IPS): Adjusting training signals to counteract the fact that users only interact with presented items, providing unbiased estimates of user preference (ijcai.org). Counterfactual Learning: Simulating “what-if” scenarios—e.g., if we had shown item X instead of item Y, would the user still have clicked? These methods help in refining ranking and re-ranking models to avoid reinforcing feedback loops. Personalized Diversity and Multi-Objective Balancing As platforms grapple with user well-being and societal impact, re-ranking increasingly accounts for:\nPersonalized Diversity: Instead of generic diversity rules (e.g., at least three different genres), models learn each user’s tolerance for variety. Some users prefer focused lists; others like exploration. Personalizing diversity constraints aligns recommendations with individual preferences. Ethical and Trust Metrics: Beyond clicks or watch time, metrics like “trust score” (does the user trust the platform’s suggestions?) or “user satisfaction” (measured via surveys) become part of multi-objective optimization at re-ranking time. Integrating Psychological and Human-Centered Insights Cognitive Load and Choice Overload Psychologists have long studied how presenting too many options can overwhelm decision-making. Barry Schwartz’s “Paradox of Choice” posits that consumers can become paralyzed when faced with abundant choices, ultimately reducing satisfaction. Multi-stage recommenders inherently combat choice overload by presenting a curated subset (natworkeffects.com). But re-ranking must carefully balance narrowing the set without removing serendipity. Injecting a few unexpected items can delight users, akin to a bookstore clerk recommending a hidden gem.\nReinforcement Learning and Habit Formation Humans form habits through repeated reinforcement. Recommendation systems, by continually suggesting similar content, can solidify user habits—for better or worse. For instance, YouTube’s suggested videos normatively prolong watch sessions; Netflix’s auto-playing of similar shows creates chain-watching behaviors. Designers must weigh engagement metrics against potential negative effects like “rabbit hole” addiction. Multi-stage pipelines can introduce “serendipity knobs” at re-ranking—slightly reducing pure relevance to nudge users toward novel experiences, promoting healthier consumption patterns.\nA Simple Analogy: The Grocery Store Consider shopping in a massive grocery store you’ve never visited:\nInitial Walkthrough (Candidate Generation): As you enter, you scan broad signage—“Bakery,” “Produce,” “Dairy.” You pick a general aisle based on a shopping list: “I need bread, but not sure which one.” In a recommendation system, this is akin to retrieving items in the “Bread” category.\nBrowsing Aisles (Scoring): In the bakery aisle, you look at multiple bread types—whole wheat, sourdough, rye. You read labels (ingredients, brand reputation, price) quickly to decide which five breads to consider.\nReading Ingredients and Price (Re-Ranking): From those five, you pick two that fit dietary restrictions (e.g., gluten-free, low-sodium), your budget, and perhaps a new brand you want to try for variety. This reflects a final refinement, possibly balancing price (business objective) with nutrition (user objective).\nChecking Out (Post-Processing): At checkout, you might receive a coupon for cheese (cross-sell recommendation) as a post-processing step, adding unplanned but contextually relevant items.\nEach phase progressively focuses the shopper’s attention, balancing speed (you don’t read every crumb of every loaf) with careful consideration (you ensure dietary needs are met). Likewise, multi-stage recommender pipelines funnel large item sets into concise, well-curated lists that align with user objectives and business goals.\nDesigning Your Own Multi-Stage System: Practical Tips Start with Clear Objectives Define Success Metrics: Is your primary goal CTR, watch time, revenue, or long-term retention? Each objective influences model choices and evaluation strategies. Identify Constraints: What is your latency budget? How large is your item catalog? What hardware resources do you have? These factors guide decisions on candidate set sizes and model complexity. Gather and Process Data Interaction Logs: Collect fine-grained logs of user interactions—clicks, views, dwell time, purchases. Ensure data pipelines support both batch and streaming use cases. Item Metadata: Harvest rich item features—text descriptions, images, categories, price, creation date. Text embeddings (e.g., BERT), image embeddings (e.g., ResNet), and structured features enhance both candidate generation and ranking. Prototype Each Stage Independently Candidate Generation Prototype:\nUse off-the-shelf ANN libraries (e.g., FAISS, Annoy) to retrieve items based on pre-computed embeddings. Compare recall at different candidate set sizes using offline evaluation (e.g., how often does historical click appear in the top-k set?). Ranking Prototype:\nTrain a simple GBDT model on candidate–user pairs. Measure ranking metrics (nDCG@10, AUC). Experiment with a dual-tower neural network: pre-compute item embeddings and train user tower embeddings to maximize dot product on positive interactions. Re-Ranking Prototype:\nImplement a pairwise learning-to-rank approach (e.g., LightGBM with LambdaMART). Use full session features. Incorporate simple business rules (e.g., ensure at least 10% of final recommendations are new items). Build a Unified Evaluation Framework Offline Simulation: Recreate user sessions from historical logs. Feed snapshots of user state into the multi-stage pipeline and compare predicted lists with actual clicks or purchases. Metrics Tracking: Track recall@K for the retrieval stage, precision@N for the ranking stage, and end-to-end metrics like nDCG and predicted revenue at the re-ranking stage. A/B Testing Infrastructure: Implement randomized traffic splits to test new retrieval or ranking models. Log both intermediate (e.g., candidate sets, scores) and final user engagement metrics. Monitor and Iterate Logging: At each stage, log key statistics: retrieval counts, score distributions, re-ranking positions, and final engagement signals. Alerting: Set up alerts for unexpected drops in recall or spikes in latency. If the candidate generation stage suddenly drops recall, it often cascades to poor final recommendations. User Feedback Loops: Allow users to provide explicit feedback (e.g., “Not interested” clicks) and integrate this data into model updates, especially at the ranking and re-ranking stages. Reflections on Simplicity and Complexity In designing multi-stage pipelines, engineers face a tension between simple, interpretable approaches and complex, high-performing models. While it’s tempting to jump to the latest deep learning breakthroughs, simpler methods—like content-based filtering with cosine similarity and GBDT ranking—often match or exceed deep models in early stages when engineered features are strong. The principle of Occam’s razor applies: prefer the simplest solution that meets requirements, then add complexity only where it yields measurable benefit.\nMoreover, a system’s maintainability, interpretability, and debuggability often correlate inversely with complexity. Multi-stage pipelines already introduce architectural complexity; adding deeply entangled neural modules at every layer can make debugging a nightmare. By isolating complexity to the re-ranking stage—where it matters most for final user experience—engineers can maintain robustness and agility.\nThe Beauty of Layered Thinking Multi-stage recommendation systems epitomize a fundamental computing strategy: break down a huge, unwieldy problem into manageable subproblems, solve each with the right tool, and combine solutions meticulously. This layered thinking mirrors how we, as humans, process information—filter broadly, focus on promising candidates, then refine with precision. By respecting constraints of latency, scalability, and maintainability, multi-stage pipelines deliver high-quality recommendations at massive scale.\nAt each stage—candidate generation, scoring, and re-ranking—we balance conflicting objectives: recall versus speed, accuracy versus cost, personalization versus fairness. Drawing from psychology, we see parallels in cognitive load, habit formation, and the nuanced interplay between exploration and exploitation. Whether designing a new system from scratch or optimizing an existing pipeline, embracing the multi-stage mindset encourages modularity, experiment-driven improvement, and user-centered design.\nI hope this exploration has illuminated the conceptual underpinnings of multi-stage recommendation, offering both a high-level roadmap and practical pointers for implementation. As you build or refine your own systems, remember: start broad, sharpen focus, and polish the final list with care—just as one crafts an idea from rough sketch to polished essay.\nReferences and Further Reading Bello, I., Manickam, S., Li, S., Rosenberg, C., Legg, B., \u0026 Bollacker, K. (2018). Deep Interest Network for Click-Through Rate Prediction. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \u0026 Data Mining, 1059–1068. Geyik, U. A., Santos, C. N. d., Xu, Z., Grbovic, M., \u0026 Vucetic, S. (2019). Personalized Recommendation on Strengths, Weaknesses, Opportunities, Threats. Proceedings of The World Wide Web Conference, 3182–3188. Hron, P., Béres, I., \u0026 Gálik, R. (2021). Neural Cascade Ranking for Large-Scale Recommendation. SIAM International Conference on Data Mining, 454–462. Luo, J., Zhang, C., Bian, J., \u0026 Sun, G. (2020). A Survey of Hybrid Recommender Systems. ACM Computing Surveys, 52(3), 1–38. Moreira, G. d. S. P., Rabhi, S., Lee, J. M., Ak, R., \u0026 Oldridge, E. (2021). End-to-End Session-Based Recommendation on GPU. Proceedings of the ACM Symposium on Cloud Computing, 831–833. Pei, J., Yuan, S., Zhao, H., Chen, W., Wang, Q., \u0026 Li, X. (2019). Neural Multi-Task Learning for Personalized Recommendation on Taobao. ACM Transactions on Intelligent Systems and Technology, 10(5), 1–25. Wilhelm, P., Zhang, X., Liao, J., \u0026 Zhao, Y. (2018). YouTube Recommendations: Beyond K-Means. Proceedings of the 12th ACM Conference on Recommender Systems, 9–17. “Building a Multi-Stage Recommender System: A Step-by-Step Guide.” (2024). Generative AI Lab. Retrieved from https://generativeailab.org/l/machine-learning/building-a-multi-stage-recommender-system-a-step-by-step-guide/ (generativeailab.org) “Multi-Stage Recommender Systems: Concepts, Architectures, and Issues.” (2022). IJCAI. Retrieved from https://www.ijcai.org/proceedings/2022/0771.pdf (ijcai.org) “Recommendation systems overview | Machine Learning.” (2025). Google Developers. Retrieved from https://developers.google.com/machine-learning/recommendation/overview/types (developers.google.com) “Towards a Theoretical Understanding of Two-Stage Recommender Systems.” (2024). arXiv. Retrieved from https://arxiv.org/pdf/2403.00802 (arxiv.org) “Building and Deploying a Multi-Stage Recommender System with Merlin.” (2022). NVIDIA. Retrieved from https://resources.nvidia.com/en-us-merlin/bad-a-multi-stage-recommender (resources.nvidia.com, assets-global.website-files.com) “How to build a Multi-Stage Recommender System.” (2023). LinkedIn Pulse. Retrieved from https://www.linkedin.com/pulse/how-build-multi-stage-recommender-system-aayush-agrawal-djdyf (linkedin.com) “Multidimensional Insights into Recommender Systems: A Comprehensive Review.” (2025). Springer. Retrieved from https://link.springer.com/chapter/10.1007/978-3-031-70285-3_29 (link.springer.com) Schwartz, B. (2004). The Paradox of Choice: Why More Is Less. HarperCollins Publishers. Vygotsky, L. S. (1978). Mind in Society: The Development of Higher Psychological Processes. Harvard University Press. ",
  "wordCount" : "7428",
  "inLanguage": "en",
  "datePublished": "2025-06-03T13:48:45+05:30",
  "dateModified": "2025-06-03T13:48:45+05:30",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://pjainish.github.io/posts/multi-stage-recommender-systems/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jainish's Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://pjainish.github.io/assets/images/favicon.png"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://pjainish.github.io/" accesskey="h" title="Jainish&#39;s Log (Alt + H)">
                <img src="https://pjainish.github.io/assets/images/favicon.png" alt="" aria-label="logo"
                    height="30">Jainish&#39;s Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://pjainish.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://pjainish.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://pjainish.github.io/posts/films/" title="Films">
                    <span>Films</span>
                </a>
            </li>
            <li>
                <a href="https://pjainish.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://pjainish.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://pjainish.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://pjainish.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://pjainish.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Multi-Stage Approach to Building Recommender Systems
    </h1>
    <div class="post-meta"><span title='2025-06-03 13:48:45 +0530 IST'>June 3, 2025</span>&nbsp;·&nbsp;35 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction-a-personal-reflection-on-systems-of-thought" aria-label="Introduction: A Personal Reflection on Systems of Thought">Introduction: A Personal Reflection on Systems of Thought</a></li>
                <li>
                    <a href="#understanding-multi-stage-recommendation-systems" aria-label="Understanding Multi-Stage Recommendation Systems">Understanding Multi-Stage Recommendation Systems</a><ul>
                        
                <li>
                    <a href="#the-core-idea-divide-and-conquer" aria-label="The Core Idea: Divide and Conquer">The Core Idea: Divide and Conquer</a></li>
                <li>
                    <a href="#why-not-a-single-model" aria-label="Why Not a Single Model?">Why Not a Single Model?</a></li>
                <li>
                    <a href="#historical-context-from-heuristics-to-neural-pipelines" aria-label="Historical Context: From Heuristics to Neural Pipelines">Historical Context: From Heuristics to Neural Pipelines</a></li></ul>
                </li>
                <li>
                    <a href="#anatomy-of-a-multi-stage-pipeline" aria-label="Anatomy of a Multi-Stage Pipeline">Anatomy of a Multi-Stage Pipeline</a><ul>
                        
                <li>
                    <a href="#candidate-generation" aria-label="Candidate Generation">Candidate Generation</a><ul>
                        
                <li>
                    <a href="#purpose-and-intuition" aria-label="Purpose and Intuition">Purpose and Intuition</a></li>
                <li>
                    <a href="#common-techniques" aria-label="Common Techniques">Common Techniques</a></li>
                <li>
                    <a href="#design-considerations" aria-label="Design Considerations">Design Considerations</a></li></ul>
                </li>
                <li>
                    <a href="#scoring-and-ranking" aria-label="Scoring and Ranking">Scoring and Ranking</a><ul>
                        
                <li>
                    <a href="#from-thousands-to-tens" aria-label="From Thousands to Tens">From Thousands to Tens</a></li>
                <li>
                    <a href="#typical-modeling-approaches" aria-label="Typical Modeling Approaches">Typical Modeling Approaches</a></li>
                <li>
                    <a href="#practical-trade-offs" aria-label="Practical Trade-Offs">Practical Trade-Offs</a></li></ul>
                </li>
                <li>
                    <a href="#re-ranking-and-refinement" aria-label="Re-Ranking and Refinement">Re-Ranking and Refinement</a><ul>
                        
                <li>
                    <a href="#the-final-polish" aria-label="The Final Polish">The Final Polish</a></li>
                <li>
                    <a href="#key-components" aria-label="Key Components">Key Components</a></li></ul>
                </li>
                <li>
                    <a href="#additional-layers-and-enhancements" aria-label="Additional Layers and Enhancements">Additional Layers and Enhancements</a></li></ul>
                </li>
                <li>
                    <a href="#motivations-behind-layered-architectures" aria-label="Motivations Behind Layered Architectures">Motivations Behind Layered Architectures</a><ul>
                        
                <li>
                    <a href="#scalability-and-efficiency" aria-label="Scalability and Efficiency">Scalability and Efficiency</a></li>
                <li>
                    <a href="#accuracy-vs-computation-trade-off" aria-label="Accuracy vs. Computation Trade-Off">Accuracy vs. Computation Trade-Off</a></li>
                <li>
                    <a href="#system-debuggability-and-experimentation" aria-label="System Debuggability and Experimentation">System Debuggability and Experimentation</a></li>
                <li>
                    <a href="#aligning-business-objectives" aria-label="Aligning Business Objectives">Aligning Business Objectives</a></li></ul>
                </li>
                <li>
                    <a href="#analogies-and-human-centric-perspectives" aria-label="Analogies and Human-Centric Perspectives">Analogies and Human-Centric Perspectives</a><ul>
                        
                <li>
                    <a href="#the-library-research-analogy" aria-label="The Library Research Analogy">The Library Research Analogy</a></li>
                <li>
                    <a href="#human-learning-and-iterative-refinement" aria-label="Human Learning and Iterative Refinement">Human Learning and Iterative Refinement</a></li></ul>
                </li>
                <li>
                    <a href="#deep-dive-into-each-stage" aria-label="Deep Dive into Each Stage">Deep Dive into Each Stage</a><ul>
                        
                <li>
                    <a href="#candidate-generation-casting-the-wide-net" aria-label="Candidate Generation: Casting the Wide Net">Candidate Generation: Casting the Wide Net</a><ul>
                        
                <li>
                    <a href="#mathematical-formulation" aria-label="Mathematical Formulation">Mathematical Formulation</a></li>
                <li>
                    <a href="#practical-example-youtubes-candidate-generation" aria-label="Practical Example: YouTube’s “Candidate Generation”">Practical Example: YouTube’s “Candidate Generation”</a></li>
                <li>
                    <a href="#challenges-in-candidate-generation" aria-label="Challenges in Candidate Generation">Challenges in Candidate Generation</a></li></ul>
                </li>
                <li>
                    <a href="#scoring-and-ranking-separating-signal-from-noise" aria-label="Scoring and Ranking: Separating Signal from Noise">Scoring and Ranking: Separating Signal from Noise</a><ul>
                        
                <li>
                    <a href="#formalizing-the-ranking-problem" aria-label="Formalizing the Ranking Problem">Formalizing the Ranking Problem</a></li>
                <li>
                    <a href="#representational-approaches" aria-label="Representational Approaches">Representational Approaches</a></li>
                <li>
                    <a href="#engineering-considerations" aria-label="Engineering Considerations">Engineering Considerations</a></li></ul>
                </li>
                <li>
                    <a href="#re-ranking-the-art-of-final-touches" aria-label="Re-Ranking: The Art of Final Touches">Re-Ranking: The Art of Final Touches</a><ul>
                        
                <li>
                    <a href="#contextual-and-business-aware-refinements" aria-label="Contextual and Business-Aware Refinements">Contextual and Business-Aware Refinements</a></li>
                <li>
                    <a href="#learning-to-rank-approaches" aria-label="Learning-to-Rank Approaches">Learning-to-Rank Approaches</a></li>
                <li>
                    <a href="#incorporating-multi-objective-optimization" aria-label="Incorporating Multi-Objective Optimization">Incorporating Multi-Objective Optimization</a></li></ul>
                </li>
                <li>
                    <a href="#beyond-three-stages-four-or-more" aria-label="Beyond Three Stages: Four or More">Beyond Three Stages: Four or More</a></li></ul>
                </li>
                <li>
                    <a href="#challenges-and-design-trade-offs" aria-label="Challenges and Design Trade-Offs">Challenges and Design Trade-Offs</a><ul>
                        
                <li>
                    <a href="#recall-and-precision-balance" aria-label="Recall and Precision Balance">Recall and Precision Balance</a></li>
                <li>
                    <a href="#latency-and-system-complexity" aria-label="Latency and System Complexity">Latency and System Complexity</a></li>
                <li>
                    <a href="#cold-start-and-evolving-data" aria-label="Cold Start and Evolving Data">Cold Start and Evolving Data</a></li>
                <li>
                    <a href="#fairness-bias-and-ethical-considerations" aria-label="Fairness, Bias, and Ethical Considerations">Fairness, Bias, and Ethical Considerations</a></li>
                <li>
                    <a href="#evaluation-metrics-and-online-experimentation" aria-label="Evaluation Metrics and Online Experimentation">Evaluation Metrics and Online Experimentation</a></li>
                <li>
                    <a href="#maintaining-freshness-and-diversity" aria-label="Maintaining Freshness and Diversity">Maintaining Freshness and Diversity</a></li></ul>
                </li>
                <li>
                    <a href="#real-world-case-studies" aria-label="Real-World Case Studies">Real-World Case Studies</a><ul>
                        
                <li>
                    <a href="#youtubes-three-stage-pipeline" aria-label="YouTube’s Three-Stage Pipeline">YouTube’s Three-Stage Pipeline</a></li>
                <li>
                    <a href="#linkedins-news-feed-recommendations" aria-label="LinkedIn’s News Feed Recommendations">LinkedIn’s News Feed Recommendations</a></li>
                <li>
                    <a href="#taobaos-four-stage-architecture" aria-label="Taobao’s Four-Stage Architecture">Taobao’s Four-Stage Architecture</a></li></ul>
                </li>
                <li>
                    <a href="#towards-the-future-evolving-multi-stage-paradigms" aria-label="Towards the Future: Evolving Multi-Stage Paradigms">Towards the Future: Evolving Multi-Stage Paradigms</a><ul>
                        
                <li>
                    <a href="#neural-re-ranking-and-contextual-fusion" aria-label="Neural Re-Ranking and Contextual Fusion">Neural Re-Ranking and Contextual Fusion</a></li>
                <li>
                    <a href="#online-learning-and-bandit-algorithms" aria-label="Online Learning and Bandit Algorithms">Online Learning and Bandit Algorithms</a></li>
                <li>
                    <a href="#causal-inference-and-debiasing" aria-label="Causal Inference and Debiasing">Causal Inference and Debiasing</a></li>
                <li>
                    <a href="#personalized-diversity-and-multi-objective-balancing" aria-label="Personalized Diversity and Multi-Objective Balancing">Personalized Diversity and Multi-Objective Balancing</a></li></ul>
                </li>
                <li>
                    <a href="#integrating-psychological-and-human-centered-insights" aria-label="Integrating Psychological and Human-Centered Insights">Integrating Psychological and Human-Centered Insights</a><ul>
                        
                <li>
                    <a href="#cognitive-load-and-choice-overload" aria-label="Cognitive Load and Choice Overload">Cognitive Load and Choice Overload</a></li>
                <li>
                    <a href="#reinforcement-learning-and-habit-formation" aria-label="Reinforcement Learning and Habit Formation">Reinforcement Learning and Habit Formation</a></li></ul>
                </li>
                <li>
                    <a href="#a-simple-analogy-the-grocery-store" aria-label="A Simple Analogy: The Grocery Store">A Simple Analogy: The Grocery Store</a></li>
                <li>
                    <a href="#designing-your-own-multi-stage-system-practical-tips" aria-label="Designing Your Own Multi-Stage System: Practical Tips">Designing Your Own Multi-Stage System: Practical Tips</a><ul>
                        
                <li>
                    <a href="#start-with-clear-objectives" aria-label="Start with Clear Objectives">Start with Clear Objectives</a></li>
                <li>
                    <a href="#gather-and-process-data" aria-label="Gather and Process Data">Gather and Process Data</a></li>
                <li>
                    <a href="#prototype-each-stage-independently" aria-label="Prototype Each Stage Independently">Prototype Each Stage Independently</a></li>
                <li>
                    <a href="#build-a-unified-evaluation-framework" aria-label="Build a Unified Evaluation Framework">Build a Unified Evaluation Framework</a></li>
                <li>
                    <a href="#monitor-and-iterate" aria-label="Monitor and Iterate">Monitor and Iterate</a></li></ul>
                </li>
                <li>
                    <a href="#reflections-on-simplicity-and-complexity" aria-label="Reflections on Simplicity and Complexity">Reflections on Simplicity and Complexity</a></li>
                <li>
                    <a href="#the-beauty-of-layered-thinking" aria-label="The Beauty of Layered Thinking">The Beauty of Layered Thinking</a></li>
                <li>
                    <a href="#references-and-further-reading" aria-label="References and Further Reading">References and Further Reading</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Multi-stage recommendation systems break down the challenging task of matching users with relevant items into several sequential phases, each optimizing for different objectives like efficiency, accuracy, and personalization. By progressively narrowing down a vast pool of candidates, applying increasingly complex models, and refining final rankings, these systems achieve scalable and high-quality recommendations even when dealing with billions of users and items (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>). They mirror how humans might sift through information: first skimming broadly, then considering details, and finally fine-tuning choices. This blog post explores the conceptual foundations of multi-stage recommendation, the distinct roles of each phase, the motivations behind layered architectures, and the real-world trade-offs they address. Along the way, analogies to everyday decision-making, historical parallels from human learning, and references to psychology illustrate how designers balance speed, relevance, and diversity. Finally, we survey challenges such as latency constraints, fairness, and the evolution toward neural re-ranking and hybrid objectives, pointing curious readers to key research papers and practical guides for deeper study.</p>
<h2 id="introduction-a-personal-reflection-on-systems-of-thought">Introduction: A Personal Reflection on Systems of Thought<a hidden class="anchor" aria-hidden="true" href="#introduction-a-personal-reflection-on-systems-of-thought">#</a></h2>
<p>When I first encountered recommendation systems, I was struck by how they mirrored the way we navigate choices in daily life. Whether picking a movie on a streaming platform or selecting a restaurant in an unfamiliar city, we often start by skimming broad categories, then gradually focus on specific options, and finally make subtle refinements based on our mood or context. In my own journey—studying neural networks, building small-scale recommenders, and later reading about industrial-scale deployments—I realized that the most robust systems also follow a layered, multi-step process. Each stage builds on the previous one, balancing the need for speed with the quest for relevance.</p>
<p>Early in my learning, I faced the temptation to design a single, “perfect” model that could solve everything at once. But this naive approach quickly ran into practical barriers: datasets with millions of users and items, strict latency requirements, and the ever-present engineering constraints of limited compute. Over time, I discovered that breaking the problem into stages not only made systems more scalable but also allowed each subcomponent to focus on a clear objective—much like how one might draft a rough outline before writing a polished essay. This approach felt natural, almost human. It honors the way we refine our thinking: brainstorm broadly, narrow the field, then polish the final answer.</p>
<p>In this post, inspired by Andrej Karpathy’s calm, thoughtful narrative style, I want to share the conceptual palette of multi-stage recommendation systems. My aim is to offer clarity over complexity, distilling intricate algorithms into intuitive ideas and drawing parallels to broader human experiences. Whether you are a curious student, an engineer venturing into recommender research, or simply someone intrigued by how machines learn to predict our preferences, I hope this narrative resonates with your own learning journey.</p>
<h2 id="understanding-multi-stage-recommendation-systems">Understanding Multi-Stage Recommendation Systems<a hidden class="anchor" aria-hidden="true" href="#understanding-multi-stage-recommendation-systems">#</a></h2>
<h3 id="the-core-idea-divide-and-conquer">The Core Idea: Divide and Conquer<a hidden class="anchor" aria-hidden="true" href="#the-core-idea-divide-and-conquer">#</a></h3>
<p>At its simplest, a recommendation system tries to answer: “Given a user, which items will they find relevant?” When the number of potential items is enormous—often in the hundreds of millions—applying a single complex model to score every possible user-item pair quickly becomes infeasible. Multi-stage recommendation systems tackle this by splitting the problem into sequential phases, each with a different scope and computational budget (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>).</p>
<ol>
<li><strong>Candidate Generation (Retrieval):</strong> Reduce a massive corpus of items to a smaller, manageable subset—often from millions to thousands.</li>
<li><strong>Scoring (Ranking):</strong> Use a more refined model to evaluate and rank these candidates, selecting a handful (e.g., 10–50) for final consideration.</li>
<li><strong>Re-Ranking (Refinement):</strong> Apply an even richer model, possibly incorporating contextual signals, diversity constraints, or business rules, to order the final set optimally for display.</li>
</ol>
<p>Some architectures include additional phases—such as pre-filtering by broad categories or post-processing for personalization and fairness—leading to four-stage or more elaborate pipelines (<a href="https://resources.nvidia.com/en-us-merlin/bad-a-multi-stage-recommender" title="Building and Deploying a Multi-Stage Recommender System with ... - NVIDIA">resources.nvidia.com</a>). But the essential principle remains: start broad and coarse, then iteratively refine.</p>
<p>This cascade mirrors human decision-making. Imagine shopping online for a book: you might first browse top genres (candidate generation), then look at bestsellers within your chosen genre (scoring), and finally read reviews to pick the exact title (re-ranking). Each step focuses on a different level of granularity and uses different cues.</p>
<h3 id="why-not-a-single-model">Why Not a Single Model?<a hidden class="anchor" aria-hidden="true" href="#why-not-a-single-model">#</a></h3>
<p>One might ask: why not build one powerful model that directly scores every item? In theory, a deep neural network with billions of parameters could capture all signals—user preferences, item attributes, temporal trends, social context. Yet in practice:</p>
<ul>
<li><strong>Computational Cost:</strong> Scoring billions of items per user request is prohibitively expensive. Even if each prediction took a microsecond, processing a single query over 100 million items would take over a minute.</li>
<li><strong>Latency Constraints:</strong> Most user-facing systems must respond within tens to a few hundred milliseconds to maintain a fluid experience.</li>
<li><strong>Scalability:</strong> As user and item counts grow, retraining and serving a monolithic model becomes unwieldy, requiring massive hardware infrastructure.</li>
<li><strong>Flexibility:</strong> Separate stages allow engineers to swap, update, or A/B test individual components (e.g., try a new candidate generator) without rebuilding the entire system.</li>
</ul>
<p>Thus, multi-stage pipelines offer a practical compromise: coarse but fast filtering followed by progressively more accurate but slower models, ensuring that latency stays within acceptable bounds while maintaining high recommendation quality (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>).</p>
<h3 id="historical-context-from-heuristics-to-neural-pipelines">Historical Context: From Heuristics to Neural Pipelines<a hidden class="anchor" aria-hidden="true" href="#historical-context-from-heuristics-to-neural-pipelines">#</a></h3>
<p>Early recommenders—dating back to collaborative filtering in the mid-1990s—often endured all-to-all scoring within a manageable dataset size. But as platforms like Amazon, Netflix, and YouTube scaled to millions of users and items, engineers introduced multi-step processes. For instance, Netflix’s 2006 recommendation infrastructure already featured a two-tier system: a “neighborhood” retrieval step using approximate nearest neighbors, followed by a weighted hybrid model for ranking (<a href="https://natworkeffects.com/posts/multi-stage-approach-to-building-recommender-systems/" title="Multi-Stage Approach to Building Recommender Systems">natworkeffects.com</a>, <a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>).</p>
<p>Over time, as deep learning matured, architectures evolved from simple matrix factorization and linear models to complex neural networks at each stage. Today, many systems leverage separate retrieval networks (e.g., dual-tower architectures) for candidate generation, gradient-boosted or neural ranking models in the scoring phase, and transformer-based or contextual deep models for re-ranking (<a href="https://arxiv.org/pdf/2403.00802" title="Towards a Theoretical Understanding of Two-Stage Recommender Systems">arxiv.org</a>, <a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>). This layered approach reflects both the historical progression of the field and the perpetual trade-off between computation and accuracy.</p>
<h2 id="anatomy-of-a-multi-stage-pipeline">Anatomy of a Multi-Stage Pipeline<a hidden class="anchor" aria-hidden="true" href="#anatomy-of-a-multi-stage-pipeline">#</a></h2>
<h3 id="candidate-generation">Candidate Generation<a hidden class="anchor" aria-hidden="true" href="#candidate-generation">#</a></h3>
<h4 id="purpose-and-intuition">Purpose and Intuition<a hidden class="anchor" aria-hidden="true" href="#purpose-and-intuition">#</a></h4>
<p>The candidate generation stage answers: “Which items out of billions might be relevant enough to consider further?” It must be extremely fast while maintaining reasonable recall—meaning it should rarely miss items that truly match user interests. Think of it as casting a wide net before trimming it down.</p>
<p>Analogy: Imagine you’re researching scholarly articles on “graph neural networks.” You might start by searching on Google Scholar with broad keywords (“graph neural network deep learning”), pulling up thousands of results. You don’t read each paper in detail; instead, you let the search engine shortlist a few hundred of the most relevant, perhaps based on citation counts or keyword frequency. These form the candidate set for deeper review.</p>
<h4 id="common-techniques">Common Techniques<a hidden class="anchor" aria-hidden="true" href="#common-techniques">#</a></h4>
<ol>
<li>
<p><strong>Approximate Nearest Neighbors (ANN):</strong>
Users and items are embedded in a shared vector space. The system retrieves the nearest item vectors to a given user vector using methods like locality-sensitive hashing (LSH) or graph-based indexes (e.g., HNSW). This approach assumes that a user’s preference can be captured by proximity in the embedding space (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>).</p>
</li>
<li>
<p><strong>Heuristic Filtering / Content-Based Selection:</strong>
Use metadata or simple rules—for instance, filter by item category (e.g., only show “science fiction” books), geographic restrictions, or availability. These heuristics can further narrow the pool before applying more expensive methods.</p>
</li>
<li>
<p><strong>Pre-Computed User-to-Item Mappings:</strong>
Some systems maintain pre-computed lists, such as “frequently co-viewed” or “users also liked,” based on historical co-occurrence. These candidate sets can be quickly unioned and deduplicated.</p>
</li>
<li>
<p><strong>Multi-Vector Retrieval:</strong>
Instead of a single user vector, some platforms compute multiple specialized retrieval vectors—for example, one for long-term interests and another for short-term session context—and aggregate their candidate sets for higher recall (<a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>).</p>
</li>
</ol>
<p>Because candidate generation often retrieves thousands of items, these methods must operate in logarithmic or sub-linear time relative to the entire catalog size. Graph-based ANN indexes, for example, offer fast lookups even as catalogs scale to tens of millions.</p>
<h4 id="design-considerations">Design Considerations<a hidden class="anchor" aria-hidden="true" href="#design-considerations">#</a></h4>
<ul>
<li><strong>Recall vs. Latency:</strong> Aggressive pruning (retrieving fewer candidates) reduces later computation but risks losing relevant items. Conversely, broad recall increases the workload for downstream stages.</li>
<li><strong>Freshness and Exploration:</strong> Relying solely on historical co-occurrences can lead to stale recommendations. Injecting a degree of randomness or exploration can help surface new items.</li>
<li><strong>Cold Start:</strong> New users (no history) or new items (no interactions) must be handled via content-based features or hybrid heuristics.</li>
<li><strong>Budget Allocation:</strong> Systems often distribute retrieval capacity across multiple candidate sources—for instance, a fixed number from item-to-item co-visitation lists, another portion from ANN, and some from heuristic rules—to balance recall diversity.</li>
</ul>
<h3 id="scoring-and-ranking">Scoring and Ranking<a hidden class="anchor" aria-hidden="true" href="#scoring-and-ranking">#</a></h3>
<h4 id="from-thousands-to-tens">From Thousands to Tens<a hidden class="anchor" aria-hidden="true" href="#from-thousands-to-tens">#</a></h4>
<p>Once candidate generation outputs a pool (e.g., 1,000–10,000 items), the scoring stage uses a moderately complex model to assign scores reflecting the user’s likelihood of engaging with each item. The goal is to rank and select a smaller subset (often 10–100 items) for final display (<a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>, <a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>).</p>
<p>Analogy: If candidate generation is skimming the first page of Google Scholar results, scoring is akin to reading abstracts and deciding which 10–20 papers to download for deeper reading. You still work relatively quickly, but you consider more details—abstract content, co-authors, publication venue.</p>
<h4 id="typical-modeling-approaches">Typical Modeling Approaches<a hidden class="anchor" aria-hidden="true" href="#typical-modeling-approaches">#</a></h4>
<ol>
<li>
<p><strong>Gradient-Boosted Decision Trees (GBDT):</strong>
Popular for their interpretability and efficiency, GBDTs like XGBoost take a set of engineered features (user demographics, item attributes, interaction history) to produce a relevance score. They balance speed with decent accuracy and can be trained on huge offline datasets.</p>
</li>
<li>
<p><strong>Two-Tower Neural Networks (Dual-Tower):</strong>
Separate “user tower” and “item tower” networks embed users and items into vectors; their dot product estimates relevance. Because item embeddings can be pre-computed, this model supports fast online scoring with vector lookups followed by simple arithmetic (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://arxiv.org/pdf/2403.00802" title="Towards a Theoretical Understanding of Two-Stage Recommender Systems">arxiv.org</a>). Dual-tower models can incorporate features like user behavior sequences, session context, and item metadata.</p>
</li>
<li>
<p><strong>Cross-Interaction Neural Models:</strong>
More expressive than dual-tower, these models take the user and item features jointly (e.g., via concatenation) and pass them through deep layers to capture fine-grained interactions. However, they are slower and thus applied only to the reduced candidate pool. Models like Deep &amp; Cross Networks (DCN), DeepFM, or those with attention mechanisms fall into this category.</p>
</li>
<li>
<p><strong>Session-Based Models:</strong>
For domains where session context matters (e.g., news or e-commerce), recurrent neural networks (RNNs) or transformers can capture sequential patterns in user interactions. These models score candidates based on both long-term preferences and recent session behavior.</p>
</li>
</ol>
<h4 id="practical-trade-offs">Practical Trade-Offs<a hidden class="anchor" aria-hidden="true" href="#practical-trade-offs">#</a></h4>
<ul>
<li><strong>Feature Engineering vs. Representation Learning:</strong> Hand-crafted features (e.g., user age, categorical encodings) can boost GBDT performance but require significant domain knowledge. Neural models can automatically learn representations but demand more compute and careful tuning.</li>
<li><strong>Offline Training vs. Online Serving:</strong> Ranking models are often retrained daily or hourly on fresh data. Keeping model updates in sync with the real-time data pipeline (e.g., streaming user actions) is non-trivial.</li>
<li><strong>Explore/Exploit Balance:</strong> Purely optimizing click-through rate (CTR) can overemphasize already popular items. Injecting exploration (e.g., using bandit algorithms) in this stage can help promote diversity and long-tail items.</li>
</ul>
<h3 id="re-ranking-and-refinement">Re-Ranking and Refinement<a hidden class="anchor" aria-hidden="true" href="#re-ranking-and-refinement">#</a></h3>
<h4 id="the-final-polish">The Final Polish<a hidden class="anchor" aria-hidden="true" href="#the-final-polish">#</a></h4>
<p>After scoring, the top N candidates (often 10–50) are ready for final polishing. Re-ranking applies the most sophisticated models and business logic to order items precisely for display (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://assets-global.website-files.com/61398f0b3344b9d4ec0973b9/63221e027375b2aff5b35f76_recsys22_poster_final.pdf" title="Building and Deploying a Multi-Stage Recommender System with Merlin">assets-global.website-files.com</a>). This phase often considers context signals unavailable earlier—such as time of day, device type, or recent events—and optimizes for multiple objectives simultaneously.</p>
<p>Analogy: If scoring chooses 15 promising articles to read, re-ranking is carefully ordering them on your coffee table, perhaps placing groundbreaking studies that align with your current project front and center, while positioning more exploratory reads slightly lower.</p>
<h4 id="key-components">Key Components<a hidden class="anchor" aria-hidden="true" href="#key-components">#</a></h4>
<ol>
<li>
<p><strong>Contextual Signals:</strong>
Real-time context like current browsing session, geo-location, or device battery status can influence final ordering. For instance, short-form video recommendations might prioritize quick snippets if the user’s device is on low battery.</p>
</li>
<li>
<p><strong>Diversity and Fairness Constraints:</strong>
Purely greedy ranking can create echo chambers or unfairly bias against less popular content creators. Re-ranking modules may enforce diversity (e.g., ensure at least one new artist in a music playlist) or fairness (e.g., limit how often the same content provider appears) (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://assets-global.website-files.com/61398f0b3344b9d4ec0973b9/63221e027375b2aff5b35f76_recsys22_poster_final.pdf" title="Building and Deploying a Multi-Stage Recommender System with Merlin">assets-global.website-files.com</a>).</p>
</li>
<li>
<p><strong>Multi-Objective Optimization:</strong>
Beyond CTR, systems often balance metrics like dwell time, revenue, or user retention. Techniques like Pareto optimization or weighted scoring can integrate multiple objectives, with re-ranking serving as the phase to reconcile potential conflicts.</p>
</li>
<li>
<p><strong>Pairwise and Listwise Learning-to-Rank:</strong>
Instead of treating each candidate independently, re-ranking can use pairwise (e.g., RankNet) or listwise (e.g., ListNet, LambdaMART) approaches that optimize the relative ordering of candidates based on user feedback signals like click sequences or dwell times.</p>
</li>
<li>
<p><strong>Latency Buffer:</strong>
Since the re-ranking phase handles only a small number of items, it can afford deeper models (e.g., transformers, graph neural networks) while still keeping total system latency within tight deadlines.</p>
</li>
</ol>
<h3 id="additional-layers-and-enhancements">Additional Layers and Enhancements<a hidden class="anchor" aria-hidden="true" href="#additional-layers-and-enhancements">#</a></h3>
<p>Many industrial pipelines incorporate extra stages beyond the canonical three. Examples include:</p>
<ul>
<li><strong>Pre-Filtering by Coarse Attributes:</strong> Quickly exclude items based on coarse filters like age restrictions, language, or membership level before candidate generation.</li>
<li><strong>Post-Processing for Exploration:</strong> Randomly inject sponsored content or fresh items after re-ranking to avoid overconfidence in the model and encourage serendipity.</li>
<li><strong>Online A/B Testing and Logging:</strong> Between each stage, systems often log intermediate scores and decisions to feed into offline analysis or to enable rapid A/B testing of algorithmic tweaks (<a href="https://resources.nvidia.com/en-us-merlin/bad-a-multi-stage-recommender" title="Building and Deploying a Multi-Stage Recommender System with ... - NVIDIA">resources.nvidia.com</a>).</li>
<li><strong>Personalization Layers:</strong> Some platforms add user segments or clusters at various stages, ensuring that models can specialize to subpopulations without retraining entirely unique pipelines per user.</li>
</ul>
<p>By designing these layered architectures, engineers can isolate concerns—tuning candidate retrieval separately from ranking or fairness adjustments—making debugging and maintenance far more manageable.</p>
<h2 id="motivations-behind-layered-architectures">Motivations Behind Layered Architectures<a hidden class="anchor" aria-hidden="true" href="#motivations-behind-layered-architectures">#</a></h2>
<h3 id="scalability-and-efficiency">Scalability and Efficiency<a hidden class="anchor" aria-hidden="true" href="#scalability-and-efficiency">#</a></h3>
<p>When catalogs contain millions or billions of items, exhaustive scoring for each user request is impractical. Multi-stage pipelines allow early pruning of irrelevant items, ensuring that only a small subset traverses the most expensive models (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>). This design echoes divide-and-conquer algorithms in computer science, where a large problem is split into smaller subproblems that are easier to solve.</p>
<p>Consider a scenario: an e-commerce site with 100 million products. If we scored all products for each user visit, even at one microsecond per score, it would take 100 seconds—far too slow. By retrieving 1,000 candidates (taking maybe 5 milliseconds) and then scoring those with a moderately complex model (say 1 millisecond each), we reduce compute to a fraction, fitting within a 100-millisecond latency budget.</p>
<h3 id="accuracy-vs-computation-trade-off">Accuracy vs. Computation Trade-Off<a hidden class="anchor" aria-hidden="true" href="#accuracy-vs-computation-trade-off">#</a></h3>
<p>Each stage in the pipeline can use progressively more expressive models, trading off compute for accuracy only when necessary. Candidate generation might use a fast, approximate algorithm with coarse embeddings. Scoring might use gradient-boosted trees or shallow neural nets. Re-ranking can apply deep, context-rich models that consider subtle interactions. This “budgeted” approach ensures that compute resources are allocated where they yield the biggest benefit—on a small subset of high-potential items.</p>
<p>Moreover, separating concerns enables each phase to be optimized independently. If a new breakthrough emerges in dual-tower retrieval, you can update the candidate generator without touching the ranking model. Conversely, if a novel re-ranking strategy arises (e.g., graph neural networks capturing social influence), you can incorporate it at the final stage without disrupting upstream retrieval.</p>
<h3 id="system-debuggability-and-experimentation">System Debuggability and Experimentation<a hidden class="anchor" aria-hidden="true" href="#system-debuggability-and-experimentation">#</a></h3>
<p>Layered architectures naturally provide inspection points. Engineers can log candidate sets, intermediate scores, and final ranks for offline analysis. This visibility aids in diagnosing issues—did the candidate generator omit relevant items? Did the ranking model misestimate relevance? Having multiple stages allows targeted A/B tests: you might experiment with a new retrieval algorithm for half of users while keeping the ranking pipeline constant, isolating the effect of retrieval improvements on overall metrics.</p>
<p>Similarly, multi-stage pipelines support incremental rollouts. A new model can be introduced initially in the re-ranking phase, gradually moving upstream once it proves effective. This staged deployment minimizes risk compared to replacing a monolithic system all at once.</p>
<h3 id="aligning-business-objectives">Aligning Business Objectives<a hidden class="anchor" aria-hidden="true" href="#aligning-business-objectives">#</a></h3>
<p>Different phases can optimize different objectives. For example, candidate generation may prioritize diversity or novelty to avoid echo chambers, scoring may focus on CTR maximizing engagement, and re-ranking may adjust for revenue or long-term retention. By decoupling stages, systems can incorporate business rules—e.g., promoting high-margin items or fulfilling contractual obligations for sponsored content—without entangling them with fundamental retrieval logic.</p>
<h2 id="analogies-and-human-centric-perspectives">Analogies and Human-Centric Perspectives<a hidden class="anchor" aria-hidden="true" href="#analogies-and-human-centric-perspectives">#</a></h2>
<h3 id="the-library-research-analogy">The Library Research Analogy<a hidden class="anchor" aria-hidden="true" href="#the-library-research-analogy">#</a></h3>
<p>Searching for information in a digital catalog is akin to walking through a library:</p>
<ol>
<li>
<p><strong>Browsing the Stacks (Candidate Generation):</strong> You wander down aisles labeled by subject areas, pulling books that look relevant based on their spine labels. You might grab twenty books that seem promising but don’t know their exact details yet.</p>
</li>
<li>
<p><strong>Skimming Table of Contents (Scoring):</strong> At your table, you flip through these books’ tables of contents, perhaps reading a few introductory paragraphs to assess whether they deeply cover your topic.</p>
</li>
<li>
<p><strong>Reading a Chapter or Two (Re-Ranking):</strong> After narrowing to five books, you read a key chapter or two to decide which is most informative for your current research question.</p>
</li>
</ol>
<p>This process ensures efficiency—you don’t read every page of every book. Instead, you refine your scope gradually, allocating your reading time where it matters most. Multi-stage recommenders mimic this approach, trading off broad coverage with depth as the pipeline progresses.</p>
<h3 id="human-learning-and-iterative-refinement">Human Learning and Iterative Refinement<a hidden class="anchor" aria-hidden="true" href="#human-learning-and-iterative-refinement">#</a></h3>
<p>The educational psychologist Lev Vygotsky described learning as moving through a “zone of proximal development,” where zones represent tasks that a learner can complete with guidance. In recommendation pipelines, early stages guide the system to promising areas (the broad zone), while later stages apply sophisticated “guidance” (complex models and context) to refine choices. This layered attention mirrors how teachers first introduce broad concepts before diving into detailed analysis.</p>
<p>Moreover, our brains rarely process all sensory inputs deeply. We unconsciously filter peripheral stimuli (“candidate generation”), focus attention on salient objects (“scoring”), and then allocate cognitive resources to detailed examination (“re-ranking”) only when necessary. This cognitive economy principle underlies why layered sampling and enrichment work so effectively in machine systems.</p>
<h2 id="deep-dive-into-each-stage">Deep Dive into Each Stage<a hidden class="anchor" aria-hidden="true" href="#deep-dive-into-each-stage">#</a></h2>
<h3 id="candidate-generation-casting-the-wide-net">Candidate Generation: Casting the Wide Net<a hidden class="anchor" aria-hidden="true" href="#candidate-generation-casting-the-wide-net">#</a></h3>
<h4 id="mathematical-formulation">Mathematical Formulation<a hidden class="anchor" aria-hidden="true" href="#mathematical-formulation">#</a></h4>
<p>Formally, let $U$ be the set of users and $I$ the set of all items. Candidate generation seeks a function $f_{\text{gen}}: U \to 2^I$ that maps each user $u$ to a subset $C_u \subset I$ of size $k$, where $k \ll |I|$. The goal is for $C_u$ to have high <strong>recall</strong>—including most items that the final system would deem relevant—while ensuring retrieval time $T_{\text{gen}}(u)$ is minimal.</p>
<p>In practice, engineers often pre-compute user embeddings $\mathbf{e}_u \in \mathbb{R}^d$ and item embeddings $\mathbf{e}_i \in \mathbb{R}^d$ using some training signal (e.g., co-clicks or purchases). Candidate generation then solves:</p>
<p>$$
C_u = \text{TopK}\bigl{\text{sim}(\mathbf{e}_u, \mathbf{e}_i),\ i \in I\bigr},
$$</p>
<p>where $\text{sim}$ is a similarity metric (dot product or cosine similarity). To avoid $O(|I|)$ computation, approximate nearest neighbor (ANN) algorithms (e.g., HNSW, FAISS) partition or graph-index the embedding space to return approximate TopK in $O(\log |I|)$ or better (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>).</p>
<h4 id="practical-example-youtubes-candidate-generation">Practical Example: YouTube’s “Candidate Generation”<a hidden class="anchor" aria-hidden="true" href="#practical-example-youtubes-candidate-generation">#</a></h4>
<p>YouTube’s production system handles billions of videos and over two billion monthly users. Their candidate generation phase uses multiple retrieval sources: a “personalized candidate generator” (a deep neural network that outputs item vectors), “idf-based candidate generators” for rare or niche videos, and “demand generation” heuristics for fresh content. Each source retrieves thousands of candidates, which are then merged and deduplicated before feeding into the ranking stage (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>).</p>
<p>By combining diverse retrieval sources, YouTube balances high recall (including long-tail videos) with computational feasibility. The embeddings incorporate signals like watch history, search queries, and video metadata (tags, descriptions, language).</p>
<h4 id="challenges-in-candidate-generation">Challenges in Candidate Generation<a hidden class="anchor" aria-hidden="true" href="#challenges-in-candidate-generation">#</a></h4>
<ul>
<li><strong>Cold Start for Items:</strong> New items have no embeddings until they accrue interactions. Content-based attributes (text descriptions, images) can bootstrap embeddings.</li>
<li><strong>Cold Start for Users:</strong> For anonymous or new users, systems might rely on session-based signals or demographic approximations.</li>
<li><strong>Embedding Drift:</strong> As user preferences evolve, embeddings must be updated frequently. Real-time or near-real-time embedding updates can be expensive. Some systems use “approximate” embeddings that update hourly or daily.</li>
<li><strong>Recall vs. Precision:</strong> While candidate generation values recall over precision (it’s okay to include some irrelevant items), retrieving too many increases downstream costs. Engineers often tune the retrieval size $k$ based on latency budgets.</li>
</ul>
<h3 id="scoring-and-ranking-separating-signal-from-noise">Scoring and Ranking: Separating Signal from Noise<a hidden class="anchor" aria-hidden="true" href="#scoring-and-ranking-separating-signal-from-noise">#</a></h3>
<h4 id="formalizing-the-ranking-problem">Formalizing the Ranking Problem<a hidden class="anchor" aria-hidden="true" href="#formalizing-the-ranking-problem">#</a></h4>
<p>Given user $u$ and candidate set $C_u = {i_1, i_2, \dots, i_k}$, ranking seeks a scoring function $f_{\text{rank}}(u, i)$ that assigns a real-valued score to each $(u, i)$. The final ranked list is obtained by sorting $C_u$ in descending order of $f_{\text{rank}}(u, i)$. Here, the focus is on maximizing a utility metric—click-through rate (CTR), watch time, revenue—subject to constraints like computational budget and fairness policies.</p>
<h4 id="representational-approaches">Representational Approaches<a hidden class="anchor" aria-hidden="true" href="#representational-approaches">#</a></h4>
<ol>
<li>
<p><strong>Gradient-Boosted Trees (GBDT):</strong>
Features can include user demographics, item popularity, item age (freshness), session duration, historical click rates, and interactions between them. GBDT models handle heterogeneous input features and often outperform simple linear models in tabular settings. For instance, LinkedIn’s ranking models use GBDTs to process thousands of features for candidate items, balancing precision and latency (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://www.linkedin.com/pulse/how-build-multi-stage-recommender-system-aayush-agrawal-djdyf" title="How to build a Multi-Stage Recommender System | Aayush Agrawal - LinkedIn">linkedin.com</a>).</p>
</li>
<li>
<p><strong>Two-Tower Neural Networks:</strong>
These models learn embedding functions $\phi_u(\cdot)$ and $\phi_i(\cdot)$ that map user and item features to a dense vector space. The relevance score is $f_{\text{rank}}(u, i) = \phi_u(\mathbf{x}_u)^\top \phi_i(\mathbf{x}_i)$. Because item embeddings $\phi_i(\mathbf{x}_i)$ can be pre-computed offline for all items, serving involves a user embedding lookup and a nearest-neighbor search among item embeddings. While two-tower excels in retrieval, it also serves as a ranking model when run over a small candidate set (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://arxiv.org/pdf/2403.00802" title="Towards a Theoretical Understanding of Two-Stage Recommender Systems">arxiv.org</a>).</p>
</li>
<li>
<p><strong>Cross-Interaction Neural Architectures:</strong>
To capture complex interactions, models like DeepFM or Wide &amp; Deep networks combine embeddings with feature crosses and joint layers. For example, the Deep &amp; Cross Network (DCN) explicitly models polynomial feature interactions, improving ranking quality at the cost of higher inference time. Such models are viable when ranking only a limited candidate set.</p>
</li>
<li>
<p><strong>Sequence Models:</strong>
In scenarios where the user’s recent behavior is paramount (e.g., news or music recommendations), recurrent neural networks (RNNs) or transformers encode the session sequence. The model’s hidden state after processing recent clicks or listens forms $\phi_u$, which then interacts with candidate item embeddings. These sequence-aware rankers can capture trends like “if the user listened to fast-paced songs recently, recommend similar tracks” (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://dl.acm.org/doi/fullHtml/10.1145/3523227.3547372" title="Training and Deploying Multi-Stage Recommender Systems">dl.acm.org</a>).</p>
</li>
</ol>
<h4 id="engineering-considerations">Engineering Considerations<a hidden class="anchor" aria-hidden="true" href="#engineering-considerations">#</a></h4>
<ul>
<li><strong>Feature Freshness:</strong> To capture evolving user interests, some features (like recent click counts) must be updated in near real-time. Engineering streaming pipelines that supply fresh features to ranking models is a significant challenge.</li>
<li><strong>Online vs. Offline Scoring:</strong> Some ranking scores can be computed offline (e.g., item popularity), while others must be computed online given session context. Balancing pre-computation and real-time inference is key to meeting latency requirements.</li>
<li><strong>Regularization and Overfitting:</strong> Because the ranking model sees only a filtered candidate set, it risks learning biases introduced by the retrieval stage. Engineers use techniques like exploration (random candidate injections) and regularization (dropout, weight decay) to mitigate such feedback loops.</li>
</ul>
<h3 id="re-ranking-the-art-of-final-touches">Re-Ranking: The Art of Final Touches<a hidden class="anchor" aria-hidden="true" href="#re-ranking-the-art-of-final-touches">#</a></h3>
<h4 id="contextual-and-business-aware-refinements">Contextual and Business-Aware Refinements<a hidden class="anchor" aria-hidden="true" href="#contextual-and-business-aware-refinements">#</a></h4>
<p>By the time candidates reach re-ranking, they number perhaps a dozen. This reduced set enables the system to apply the most expensive and context-rich models, considering signals that were too costly earlier:</p>
<ul>
<li><strong>User’s Real-Time Context:</strong> Current weather, device type, screen size, or even network speed can influence which items make sense. For example, a video platform might demote 4K videos if the user’s bandwidth appears constrained.</li>
<li><strong>Temporal Patterns:</strong> If an item is trending due to a breaking news event, re-ranking can upweight it even if it didn’t score highest in the ranking model.</li>
</ul>
<p>Additionally, the re-ranking stage often integrates final business rules:</p>
<ul>
<li><strong>Sponsored Content and Ads:</strong> Platforms typically must display a minimum number of sponsored items or promote partners. Re-ranking can adjust scores to ensure contractual obligations are met.</li>
<li><strong>Diversity Constraints:</strong> To prevent monotony and filter bubbles, systems may enforce that top N items span multiple content categories or creators (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://assets-global.website-files.com/61398f0b3344b9d4ec0973b9/63221e027375b2aff5b35f76_recsys22_poster_final.pdf" title="Building and Deploying a Multi-Stage Recommender System with Merlin">assets-global.website-files.com</a>).</li>
<li><strong>Fairness and Ethical Safeguards:</strong> Ensuring that minority or new creators receive exposure may require explicit adjustments. For instance, a music streaming service might limit how many tracks by a single artist appear in a daily playlist, or an e-commerce site might promote ethically sourced products.</li>
</ul>
<h4 id="learning-to-rank-approaches">Learning-to-Rank Approaches<a hidden class="anchor" aria-hidden="true" href="#learning-to-rank-approaches">#</a></h4>
<p>While earlier stages often rely on pointwise prediction (predicting the utility of each item independently), re-ranking can adopt more sophisticated <strong>pairwise</strong> or <strong>listwise</strong> approaches:</p>
<ul>
<li><strong>Pairwise Ranking (e.g., RankNet, RankSVM):</strong> The model learns from pairs of items, optimizing the probability that a more relevant item is ranked above a less relevant one. This typically uses a loss function that encourages correct ordering of pairs based on user clicks or dwell times.</li>
<li><strong>Listwise Ranking (e.g., ListNet, LambdaMART):</strong> These methods consider the entire list of candidates jointly, optimizing metrics directly related to list order—such as nDCG (normalized Discounted Cumulative Gain). Listwise losses can be more aligned with final business metrics but are often harder to optimize and require careful sampling strategies.</li>
</ul>
<h4 id="incorporating-multi-objective-optimization">Incorporating Multi-Objective Optimization<a hidden class="anchor" aria-hidden="true" href="#incorporating-multi-objective-optimization">#</a></h4>
<p>In many scenarios, platforms must juggle multiple goals: user engagement (clicks or watch time), revenue (ad impressions or purchases), and long-term retention. Re-ranking offers the flexibility to integrate these objectives:</p>
<ul>
<li><strong>Scalarization:</strong> Combine multiple metrics into a single weighted score. For example, $\text{score} = \alpha \times \text{CTR} + \beta \times \text{Expected Revenue}$. Weights $\alpha, \beta$ can be tuned to match business priorities.</li>
<li><strong>Pareto Front Methods:</strong> Instead of combining objectives, identify items that lie on the Pareto frontier—meaning no other item is strictly better in all objectives. Re-ranking then selects from this frontier based on context.</li>
<li><strong>Constrained Optimization:</strong> Define primary objectives (e.g., CTR) while enforcing constraints on secondary metrics (e.g., minimum diversity or fairness thresholds). This can be formulated as linear or integer programming problems solved at re-ranking time.</li>
</ul>
<h3 id="beyond-three-stages-four-or-more">Beyond Three Stages: Four or More<a hidden class="anchor" aria-hidden="true" href="#beyond-three-stages-four-or-more">#</a></h3>
<p>Some platforms extend multi-stage pipelines further:</p>
<ol>
<li><strong>Coarse Filtering (Pre-Retrieval):</strong> Filter by extremely simple rules—e.g., language, age rating, or membership level—before computing any embeddings. This reduces both retrieval and ranking load.</li>
<li><strong>Primary Retrieval (Candidate Generation).</strong></li>
<li><strong>Secondary Retrieval (Cross-Modal or Contextual):</strong> Some systems perform a second retrieval focusing on a different signal. For instance, after retrieving general candidates from a content-based model, they may retrieve additional items based on collaborative co-click signals and then union the two sets.</li>
<li><strong>Ranking (Scoring).</strong></li>
<li><strong>Re-Ranking (Refinement).</strong></li>
<li><strong>Post-Processing (Online Exploration/Injection):</strong> Finally, inject a small fraction of random or specially curated items—like sponsored content or editorial picks—into the ranked list before display (<a href="https://resources.nvidia.com/en-us-merlin/bad-a-multi-stage-recommender" title="Building and Deploying a Multi-Stage Recommender System with ... - NVIDIA">resources.nvidia.com</a>, <a href="https://assets-global.website-files.com/61398f0b3344b9d4ec0973b9/63221e027375b2aff5b35f76_recsys22_poster_final.pdf" title="Building and Deploying a Multi-Stage Recommender System with Merlin">assets-global.website-files.com</a>).</li>
</ol>
<p>NVIDIA’s Merlin architecture outlines a four-stage pipeline where separate retrieval stages handle different signals, reflecting real-world complexities in balancing content freshness, personalization, and business rules (<a href="https://resources.nvidia.com/en-us-merlin/bad-a-multi-stage-recommender" title="Building and Deploying a Multi-Stage Recommender System with ... - NVIDIA">resources.nvidia.com</a>).</p>
<h2 id="challenges-and-design-trade-offs">Challenges and Design Trade-Offs<a hidden class="anchor" aria-hidden="true" href="#challenges-and-design-trade-offs">#</a></h2>
<h3 id="recall-and-precision-balance">Recall and Precision Balance<a hidden class="anchor" aria-hidden="true" href="#recall-and-precision-balance">#</a></h3>
<ul>
<li><strong>High Recall Need:</strong> If candidate generation misses relevant items, downstream stages cannot recover them. Low recall hurts both immediate relevance and long-term user satisfaction.</li>
<li><strong>Precision Constraints:</strong> However, retrieving too many candidates inflates computational costs. Designers must find an operating point where recall is sufficiently high while keeping the candidate set size within resource budgets.</li>
</ul>
<p>Finding this balance often involves extensive offline evaluation: sampling user queries, varying retrieval thresholds, and measuring recall of items that ultimately led to clicks or conversions. Techniques like “held-out validation” and “information retrieval metrics” (e.g., recall@K, MRR) guide engineers in tuning retrieval hyperparameters.</p>
<h3 id="latency-and-system-complexity">Latency and System Complexity<a hidden class="anchor" aria-hidden="true" href="#latency-and-system-complexity">#</a></h3>
<p>Every stage introduces latency. Even if candidate generation and ranking operate in microseconds, re-ranking complex item sets with deep models can push total response time beyond acceptable limits. Systems often target end-to-end latencies under 100–200 milliseconds for web-based recommendations (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>). To meet these SLAs:</p>
<ul>
<li><strong>Parallelization:</strong> Some stages run in parallel—e.g., Katz–Schneider retrieval that fetches both content-based and collaborative candidates simultaneously before merging.</li>
<li><strong>Caching:</strong> Popular users or items may have pre-computed candidate lists or ranking scores. However, caching fresh recommendations is tricky when user activity changes rapidly.</li>
<li><strong>Hardware Acceleration:</strong> GPUs or specialized accelerators can speed up neural inference, especially for deep re-ranking models. Yet they add operational complexity and cost.</li>
<li><strong>Graceful Degradation:</strong> Under high load, systems might skip the re-ranking phase or employ simplified ranking to ensure responsiveness, accepting a temporary drop in accuracy.</li>
</ul>
<h3 id="cold-start-and-evolving-data">Cold Start and Evolving Data<a hidden class="anchor" aria-hidden="true" href="#cold-start-and-evolving-data">#</a></h3>
<ul>
<li><strong>New Users:</strong> Without historical interactions, candidate generation struggles. Common strategies include asking onboarding questions, using demographic-based heuristics, or emphasizing popular items to collect initial data.</li>
<li><strong>New Items:</strong> Newly added content has no interaction history. Content-based features (text embeddings, image features) or editorial tagging can bootstrap embeddings. Some systems also inject fresh items randomly into candidate sets to gather user feedback quickly.</li>
<li><strong>Data Drift:</strong> User interests and item catalogs evolve. Periodic retraining—daily or hourly—helps keep models up to date, but retraining at scale can strain infrastructure. Incremental training or online learning frameworks attempt to update models continuously, though they raise concerns about model stability and feedback loops.</li>
</ul>
<h3 id="fairness-bias-and-ethical-considerations">Fairness, Bias, and Ethical Considerations<a hidden class="anchor" aria-hidden="true" href="#fairness-bias-and-ethical-considerations">#</a></h3>
<p>Multi-stage pipelines can inadvertently amplify biases:</p>
<ul>
<li><strong>Popularity Bias:</strong> Early retrieval might preferentially surface popular items, pushing niche or new content out of the pipeline entirely.</li>
<li><strong>Demographic Bias:</strong> If training data reflect societal biases—e.g., gender or racial preferences—models might perpetuate or exacerbate inequities. For instance, a music recommender might under-represent certain genres popular among minority communities.</li>
<li><strong>Feedback Loops:</strong> When users are repeatedly shown similar content, they have fewer opportunities to diversify their interests. This cyclical effect traps them in a feedback loop that reinforces initial biases.</li>
</ul>
<p>To address these issues, re-ranking often incorporates fairness constraints—e.g., ensuring a minimum representation of under-represented groups—or diversity-promoting objectives (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://assets-global.website-files.com/61398f0b3344b9d4ec0973b9/63221e027375b2aff5b35f76_recsys22_poster_final.pdf" title="Building and Deploying a Multi-Stage Recommender System with Merlin">assets-global.website-files.com</a>). Engineers may also use causal inference to disentangle correlation from true preference signals, though this remains an active research area.</p>
<h3 id="evaluation-metrics-and-online-experimentation">Evaluation Metrics and Online Experimentation<a hidden class="anchor" aria-hidden="true" href="#evaluation-metrics-and-online-experimentation">#</a></h3>
<p>Measuring success in multi-stage systems is multifaceted:</p>
<ul>
<li>
<p><strong>Offline Metrics:</strong></p>
<ul>
<li><strong>Recall@K:</strong> Fraction of truly relevant items that appear in the top K candidates (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>).</li>
<li><strong>NRMSE (Normalized Root Mean Squared Error):</strong> For predicting ratings or continuous outcomes.</li>
<li><strong>nDCG (Normalized Discounted Cumulative Gain):</strong> Accounts for position bias in ranked lists.</li>
</ul>
</li>
<li>
<p><strong>Online Metrics (A/B Testing):</strong></p>
<ul>
<li><strong>Click-Through Rate (CTR):</strong> The fraction of recommendations that lead to clicks.</li>
<li><strong>Engagement Time/Dwell Time:</strong> Time spent interacting with recommended content.</li>
<li><strong>Conversion Rate (CR):</strong> Purchases or desired downstream actions.</li>
<li><strong>Retention/Lifetime Value (LTV):</strong> Long-term impact of recommendations on user loyalty.</li>
</ul>
</li>
</ul>
<p>A/B tests are critical because offline proxies often fail to capture user behavior complexities. For example, a model that improves offline nDCG may inadvertently reduce long-term engagement if it over-emphasizes certain item types.</p>
<h3 id="maintaining-freshness-and-diversity">Maintaining Freshness and Diversity<a hidden class="anchor" aria-hidden="true" href="#maintaining-freshness-and-diversity">#</a></h3>
<p>Balancing relevance with freshness ensures that users see timely content, not stale favorites. Common techniques include:</p>
<ul>
<li><strong>Time Decay Functions:</strong> Decrease the weight of interactions as they age, ensuring that recent trending items receive higher retrieval priority.</li>
<li><strong>Dynamic Exploration Schedules:</strong> Temporarily boost undervalued content or categories, measuring user responses to decide if these should enter regular circulation.</li>
<li><strong>Diversity Constraints:</strong> Enforce constraints like “no more than two items from the same category in the top-5 recommendations” to avoid monotony (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://assets-global.website-files.com/61398f0b3344b9d4ec0973b9/63221e027375b2aff5b35f76_recsys22_poster_final.pdf" title="Building and Deploying a Multi-Stage Recommender System with Merlin">assets-global.website-files.com</a>).</li>
</ul>
<p>With rapid shifts in user interests—such as viral trends on social media—systems must adapt quickly without overreacting to noise.</p>
<h2 id="real-world-case-studies">Real-World Case Studies<a hidden class="anchor" aria-hidden="true" href="#real-world-case-studies">#</a></h2>
<h3 id="youtubes-three-stage-pipeline">YouTube’s Three-Stage Pipeline<a hidden class="anchor" aria-hidden="true" href="#youtubes-three-stage-pipeline">#</a></h3>
<p>YouTube’s recommendation engine processes over 500 hours of video uploads per minute and serves billions of daily watch sessions. Their pipeline typically comprises:</p>
<ol>
<li><strong>Candidate Generation:</strong> Several retrieval sources—embedding-based ANN, session-based heuristics, and recent trending signals—produce a combined set of 1,000–2,000 videos (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>).</li>
<li><strong>Scoring:</strong> A candidate omnivorous ranking model (COR) scores each video using a two-tower architecture supplemented by contextual features like watch history, device type, and time of day. The top ~50 videos are selected for re-ranking.</li>
<li><strong>Re-Ranking:</strong> A complex deep model (often leveraging attention mechanisms to model user-video interactions along with session context) refines the ordering, ensuring diversity and personal relevance. Business rules inject some fresh or sponsored videos at this stage (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://assets-global.website-files.com/61398f0b3344b9d4ec0973b9/63221e027375b2aff5b35f76_recsys22_poster_final.pdf" title="Building and Deploying a Multi-Stage Recommender System with Merlin">assets-global.website-files.com</a>).</li>
</ol>
<p>YouTube continuously A/B tests changes, measuring not just immediate watch time but also long-term retention and channel subscriptions. Their hierarchical approach allows them to serve highly personalized content at massive scale without exceeding latency budgets (often under 100 ms for initial retrieval and 200 ms end-to-end) (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>).</p>
<h3 id="linkedins-news-feed-recommendations">LinkedIn’s News Feed Recommendations<a hidden class="anchor" aria-hidden="true" href="#linkedins-news-feed-recommendations">#</a></h3>
<p>LinkedIn’s feed blends content recommendations (articles, posts) with job suggestions and ads. Their multi-stage system includes:</p>
<ol>
<li><strong>Pre-Filtering:</strong> Exclude posts in languages the user doesn’t understand or items violating policies.</li>
<li><strong>Candidate Generation:</strong> Retrieve posts based on user’s network interactions—e.g., posts by first-degree connections, followed influencers, or articles matching user’s interests. This stage uses graph-based traversal along the social graph and content-based retrieval for topical relevance (<a href="https://www.linkedin.com/pulse/how-build-multi-stage-recommender-system-aayush-agrawal-djdyf" title="How to build a Multi-Stage Recommender System | Aayush Agrawal - LinkedIn">linkedin.com</a>, <a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>).</li>
<li><strong>Scoring:</strong> A gradient-boosted model evaluates each post’s relevance based on hundreds of features—user’s skill tags, past engagement patterns, recency, and even inferred career stage. The model outputs a score predicting “probability of positive engagement” (like click, comment, or share).</li>
<li><strong>Re-Ranking:</strong> A pairwise learning-to-rank module refines ranking by optimizing for relative ordering. It also enforces that no more than two successive posts from the same publisher appear, promoting diversity among content creators.</li>
</ol>
<p>LinkedIn’s system must juggle diverse content formats—text articles, videos, job postings, ads—each with different engagement signals. By decoupling retrieval, ranking, and re-ranking, they can optimize specialized models for each format and then unify them under a common final re-ranker.</p>
<h3 id="taobaos-four-stage-architecture">Taobao’s Four-Stage Architecture<a hidden class="anchor" aria-hidden="true" href="#taobaos-four-stage-architecture">#</a></h3>
<p>Taobao, one of the world’s largest e-commerce platforms, serves over a billion monthly active users. Their multi-stage architecture often follows:</p>
<ol>
<li><strong>Wide &amp; Narrow Retrieval:</strong> A combination of content-based filtering (e.g., category-level retrieval) and collaborative retrieval (e.g., user–item co-click graphs) yields ~10,000 candidates.</li>
<li><strong>Coarse Ranking:</strong> A GBDT model with engineered features ranks these candidates to a shortlist of ~1,000.</li>
<li><strong>Fine Ranking:</strong> A deep neural network—often combining convolutional layers for image features, embedding layers for text attributes, and attention modules to capture user-item interactions—reduces to ~50 items.</li>
<li><strong>Re-Ranking with Business Rules:</strong> Final adjustments inject promotions, ensure seller diversity, apply dayparting rules (e.g., preferring essential goods in morning and entertainment items in evening), and optimize for multiple objectives like conversion rate, gross merchandise volume (GMV), and click yield (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://dl.acm.org/doi/fullHtml/10.1145/3523227.3547372" title="Training and Deploying Multi-Stage Recommender Systems">dl.acm.org</a>).</li>
</ol>
<p>Because Taobao’s inventory changes rapidly (with thousands of new items added hourly), their system employs robust feature pipelines to update item embeddings in near real-time. The four-stage design allows them to integrate new items into candidate pools via content-based features, then gradually gather interaction data to feed collaborative signals back into retrieval.</p>
<h2 id="towards-the-future-evolving-multi-stage-paradigms">Towards the Future: Evolving Multi-Stage Paradigms<a hidden class="anchor" aria-hidden="true" href="#towards-the-future-evolving-multi-stage-paradigms">#</a></h2>
<h3 id="neural-re-ranking-and-contextual-fusion">Neural Re-Ranking and Contextual Fusion<a hidden class="anchor" aria-hidden="true" href="#neural-re-ranking-and-contextual-fusion">#</a></h3>
<p>Recent research in neural re-ranking focuses on richer representations and contextual fusion:</p>
<ul>
<li><strong>Transformer-Based Re-Rankers:</strong> Models like BERT or its variants, finetuned for recommendation tasks, can process candidate sets jointly, capturing inter-item relationships (e.g., “these two movies are sequels”) and user context. IJCAI’s 2022 review notes that transformer-based re-rankers can significantly outperform traditional MLP or tree-based models, albeit at higher computational cost (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>).</li>
<li><strong>Multi-Modal Fusion:</strong> E-commerce and social media often benefit from combining visual, textual, and numerical features. Graph neural networks (GNNs) can propagate signals across user–item graphs, capturing higher-order interactions. Eﬀective fusion of these signals in the re-ranking stage leads to more nuanced final lists (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>, <a href="https://dl.acm.org/doi/fullHtml/10.1145/3523227.3547372" title="Training and Deploying Multi-Stage Recommender Systems">dl.acm.org</a>).</li>
<li><strong>Session-Aware Re-Ranking:</strong> In domains where session context evolves rapidly (e.g., news or music streaming), re-ranking models incorporate session sequences as part of the final scoring. Models like “Transformer4Rec” attend over both candidate items and session history, refining lists to match transient user intent.</li>
</ul>
<h3 id="online-learning-and-bandit-algorithms">Online Learning and Bandit Algorithms<a hidden class="anchor" aria-hidden="true" href="#online-learning-and-bandit-algorithms">#</a></h3>
<p>Traditionally, multi-stage pipelines train offline on historical data and then serve static models online. Emerging trends include:</p>
<ul>
<li><strong>Contextual Bandits in Ranking:</strong> Between the scoring and re-ranking stages, some systems integrate bandit algorithms that dynamically adjust item scores based on real-time click feedback, balancing exploration (showing new or uncertain items) and exploitation (showing high-confidence items).</li>
<li><strong>Continual Learning:</strong> Instead of periodic batch retraining, models update incrementally as new interactions arrive. This reduces lag between data generation and model applicability, improving responsiveness to changing user preferences.</li>
</ul>
<h3 id="causal-inference-and-debiasing">Causal Inference and Debiasing<a hidden class="anchor" aria-hidden="true" href="#causal-inference-and-debiasing">#</a></h3>
<p>Recommendation systems often suffer from biases introduced by historical data—popularity bias, presentation bias (items shown higher get more clicks), and selection bias (users only see a subset of items). Researchers are exploring causal methods:</p>
<ul>
<li><strong>Inverse Propensity Scoring (IPS):</strong> Adjusting training signals to counteract the fact that users only interact with presented items, providing unbiased estimates of user preference (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>).</li>
<li><strong>Counterfactual Learning:</strong> Simulating “what-if” scenarios—e.g., if we had shown item X instead of item Y, would the user still have clicked? These methods help in refining ranking and re-ranking models to avoid reinforcing feedback loops.</li>
</ul>
<h3 id="personalized-diversity-and-multi-objective-balancing">Personalized Diversity and Multi-Objective Balancing<a hidden class="anchor" aria-hidden="true" href="#personalized-diversity-and-multi-objective-balancing">#</a></h3>
<p>As platforms grapple with user well-being and societal impact, re-ranking increasingly accounts for:</p>
<ul>
<li><strong>Personalized Diversity:</strong> Instead of generic diversity rules (e.g., at least three different genres), models learn each user’s tolerance for variety. Some users prefer focused lists; others like exploration. Personalizing diversity constraints aligns recommendations with individual preferences.</li>
<li><strong>Ethical and Trust Metrics:</strong> Beyond clicks or watch time, metrics like “trust score” (does the user trust the platform’s suggestions?) or “user satisfaction” (measured via surveys) become part of multi-objective optimization at re-ranking time.</li>
</ul>
<h2 id="integrating-psychological-and-human-centered-insights">Integrating Psychological and Human-Centered Insights<a hidden class="anchor" aria-hidden="true" href="#integrating-psychological-and-human-centered-insights">#</a></h2>
<h3 id="cognitive-load-and-choice-overload">Cognitive Load and Choice Overload<a hidden class="anchor" aria-hidden="true" href="#cognitive-load-and-choice-overload">#</a></h3>
<p>Psychologists have long studied how presenting too many options can overwhelm decision-making. Barry Schwartz’s “Paradox of Choice” posits that consumers can become paralyzed when faced with abundant choices, ultimately reducing satisfaction. Multi-stage recommenders inherently combat choice overload by presenting a curated subset (<a href="https://natworkeffects.com/posts/multi-stage-approach-to-building-recommender-systems/" title="Multi-Stage Approach to Building Recommender Systems">natworkeffects.com</a>). But re-ranking must carefully balance narrowing the set without removing serendipity. Injecting a few unexpected items can delight users, akin to a bookstore clerk recommending a hidden gem.</p>
<h3 id="reinforcement-learning-and-habit-formation">Reinforcement Learning and Habit Formation<a hidden class="anchor" aria-hidden="true" href="#reinforcement-learning-and-habit-formation">#</a></h3>
<p>Humans form habits through repeated reinforcement. Recommendation systems, by continually suggesting similar content, can solidify user habits—for better or worse. For instance, YouTube’s suggested videos normatively prolong watch sessions; Netflix’s auto-playing of similar shows creates chain-watching behaviors. Designers must weigh engagement metrics against potential negative effects like “rabbit hole” addiction. Multi-stage pipelines can introduce “serendipity knobs” at re-ranking—slightly reducing pure relevance to nudge users toward novel experiences, promoting healthier consumption patterns.</p>
<h2 id="a-simple-analogy-the-grocery-store">A Simple Analogy: The Grocery Store<a hidden class="anchor" aria-hidden="true" href="#a-simple-analogy-the-grocery-store">#</a></h2>
<p>Consider shopping in a massive grocery store you’ve never visited:</p>
<ol>
<li>
<p><strong>Initial Walkthrough (Candidate Generation):</strong> As you enter, you scan broad signage—“Bakery,” “Produce,” “Dairy.” You pick a general aisle based on a shopping list: “I need bread, but not sure which one.” In a recommendation system, this is akin to retrieving items in the “Bread” category.</p>
</li>
<li>
<p><strong>Browsing Aisles (Scoring):</strong> In the bakery aisle, you look at multiple bread types—whole wheat, sourdough, rye. You read labels (ingredients, brand reputation, price) quickly to decide which five breads to consider.</p>
</li>
<li>
<p><strong>Reading Ingredients and Price (Re-Ranking):</strong> From those five, you pick two that fit dietary restrictions (e.g., gluten-free, low-sodium), your budget, and perhaps a new brand you want to try for variety. This reflects a final refinement, possibly balancing price (business objective) with nutrition (user objective).</p>
</li>
<li>
<p><strong>Checking Out (Post-Processing):</strong> At checkout, you might receive a coupon for cheese (cross-sell recommendation) as a post-processing step, adding unplanned but contextually relevant items.</p>
</li>
</ol>
<p>Each phase progressively focuses the shopper’s attention, balancing speed (you don’t read every crumb of every loaf) with careful consideration (you ensure dietary needs are met). Likewise, multi-stage recommender pipelines funnel large item sets into concise, well-curated lists that align with user objectives and business goals.</p>
<h2 id="designing-your-own-multi-stage-system-practical-tips">Designing Your Own Multi-Stage System: Practical Tips<a hidden class="anchor" aria-hidden="true" href="#designing-your-own-multi-stage-system-practical-tips">#</a></h2>
<h3 id="start-with-clear-objectives">Start with Clear Objectives<a hidden class="anchor" aria-hidden="true" href="#start-with-clear-objectives">#</a></h3>
<ul>
<li><strong>Define Success Metrics:</strong> Is your primary goal CTR, watch time, revenue, or long-term retention? Each objective influences model choices and evaluation strategies.</li>
<li><strong>Identify Constraints:</strong> What is your latency budget? How large is your item catalog? What hardware resources do you have? These factors guide decisions on candidate set sizes and model complexity.</li>
</ul>
<h3 id="gather-and-process-data">Gather and Process Data<a hidden class="anchor" aria-hidden="true" href="#gather-and-process-data">#</a></h3>
<ul>
<li><strong>Interaction Logs:</strong> Collect fine-grained logs of user interactions—clicks, views, dwell time, purchases. Ensure data pipelines support both batch and streaming use cases.</li>
<li><strong>Item Metadata:</strong> Harvest rich item features—text descriptions, images, categories, price, creation date. Text embeddings (e.g., BERT), image embeddings (e.g., ResNet), and structured features enhance both candidate generation and ranking.</li>
</ul>
<h3 id="prototype-each-stage-independently">Prototype Each Stage Independently<a hidden class="anchor" aria-hidden="true" href="#prototype-each-stage-independently">#</a></h3>
<ol>
<li>
<p><strong>Candidate Generation Prototype:</strong></p>
<ul>
<li>Use off-the-shelf ANN libraries (e.g., FAISS, Annoy) to retrieve items based on pre-computed embeddings.</li>
<li>Compare recall at different candidate set sizes using offline evaluation (e.g., how often does historical click appear in the top-k set?).</li>
</ul>
</li>
<li>
<p><strong>Ranking Prototype:</strong></p>
<ul>
<li>Train a simple GBDT model on candidate–user pairs. Measure ranking metrics (nDCG@10, AUC).</li>
<li>Experiment with a dual-tower neural network: pre-compute item embeddings and train user tower embeddings to maximize dot product on positive interactions.</li>
</ul>
</li>
<li>
<p><strong>Re-Ranking Prototype:</strong></p>
<ul>
<li>Implement a pairwise learning-to-rank approach (e.g., LightGBM with LambdaMART). Use full session features.</li>
<li>Incorporate simple business rules (e.g., ensure at least 10% of final recommendations are new items).</li>
</ul>
</li>
</ol>
<h3 id="build-a-unified-evaluation-framework">Build a Unified Evaluation Framework<a hidden class="anchor" aria-hidden="true" href="#build-a-unified-evaluation-framework">#</a></h3>
<ul>
<li><strong>Offline Simulation:</strong> Recreate user sessions from historical logs. Feed snapshots of user state into the multi-stage pipeline and compare predicted lists with actual clicks or purchases.</li>
<li><strong>Metrics Tracking:</strong> Track recall@K for the retrieval stage, precision@N for the ranking stage, and end-to-end metrics like nDCG and predicted revenue at the re-ranking stage.</li>
<li><strong>A/B Testing Infrastructure:</strong> Implement randomized traffic splits to test new retrieval or ranking models. Log both intermediate (e.g., candidate sets, scores) and final user engagement metrics.</li>
</ul>
<h3 id="monitor-and-iterate">Monitor and Iterate<a hidden class="anchor" aria-hidden="true" href="#monitor-and-iterate">#</a></h3>
<ul>
<li><strong>Logging:</strong> At each stage, log key statistics: retrieval counts, score distributions, re-ranking positions, and final engagement signals.</li>
<li><strong>Alerting:</strong> Set up alerts for unexpected drops in recall or spikes in latency. If the candidate generation stage suddenly drops recall, it often cascades to poor final recommendations.</li>
<li><strong>User Feedback Loops:</strong> Allow users to provide explicit feedback (e.g., “Not interested” clicks) and integrate this data into model updates, especially at the ranking and re-ranking stages.</li>
</ul>
<h2 id="reflections-on-simplicity-and-complexity">Reflections on Simplicity and Complexity<a hidden class="anchor" aria-hidden="true" href="#reflections-on-simplicity-and-complexity">#</a></h2>
<p>In designing multi-stage pipelines, engineers face a tension between simple, interpretable approaches and complex, high-performing models. While it’s tempting to jump to the latest deep learning breakthroughs, simpler methods—like content-based filtering with cosine similarity and GBDT ranking—often match or exceed deep models in early stages when engineered features are strong. The principle of Occam’s razor applies: prefer the simplest solution that meets requirements, then add complexity only where it yields measurable benefit.</p>
<p>Moreover, a system’s maintainability, interpretability, and debuggability often correlate inversely with complexity. Multi-stage pipelines already introduce architectural complexity; adding deeply entangled neural modules at every layer can make debugging a nightmare. By isolating complexity to the re-ranking stage—where it matters most for final user experience—engineers can maintain robustness and agility.</p>
<h2 id="the-beauty-of-layered-thinking">The Beauty of Layered Thinking<a hidden class="anchor" aria-hidden="true" href="#the-beauty-of-layered-thinking">#</a></h2>
<p>Multi-stage recommendation systems epitomize a fundamental computing strategy: break down a huge, unwieldy problem into manageable subproblems, solve each with the right tool, and combine solutions meticulously. This layered thinking mirrors how we, as humans, process information—filter broadly, focus on promising candidates, then refine with precision. By respecting constraints of latency, scalability, and maintainability, multi-stage pipelines deliver high-quality recommendations at massive scale.</p>
<p>At each stage—candidate generation, scoring, and re-ranking—we balance conflicting objectives: recall versus speed, accuracy versus cost, personalization versus fairness. Drawing from psychology, we see parallels in cognitive load, habit formation, and the nuanced interplay between exploration and exploitation. Whether designing a new system from scratch or optimizing an existing pipeline, embracing the multi-stage mindset encourages modularity, experiment-driven improvement, and user-centered design.</p>
<p>I hope this exploration has illuminated the conceptual underpinnings of multi-stage recommendation, offering both a high-level roadmap and practical pointers for implementation. As you build or refine your own systems, remember: start broad, sharpen focus, and polish the final list with care—just as one crafts an idea from rough sketch to polished essay.</p>
<hr>
<h2 id="references-and-further-reading">References and Further Reading<a hidden class="anchor" aria-hidden="true" href="#references-and-further-reading">#</a></h2>
<ul>
<li>Bello, I., Manickam, S., Li, S., Rosenberg, C., Legg, B., &amp; Bollacker, K. (2018). Deep Interest Network for Click-Through Rate Prediction. <em>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, 1059–1068.</li>
<li>Geyik, U. A., Santos, C. N. d., Xu, Z., Grbovic, M., &amp; Vucetic, S. (2019). Personalized Recommendation on Strengths, Weaknesses, Opportunities, Threats. <em>Proceedings of The World Wide Web Conference</em>, 3182–3188.</li>
<li>Hron, P., Béres, I., &amp; Gálik, R. (2021). Neural Cascade Ranking for Large-Scale Recommendation. <em>SIAM International Conference on Data Mining</em>, 454–462.</li>
<li>Luo, J., Zhang, C., Bian, J., &amp; Sun, G. (2020). A Survey of Hybrid Recommender Systems. <em>ACM Computing Surveys</em>, 52(3), 1–38.</li>
<li>Moreira, G. d. S. P., Rabhi, S., Lee, J. M., Ak, R., &amp; Oldridge, E. (2021). End-to-End Session-Based Recommendation on GPU. <em>Proceedings of the ACM Symposium on Cloud Computing</em>, 831–833.</li>
<li>Pei, J., Yuan, S., Zhao, H., Chen, W., Wang, Q., &amp; Li, X. (2019). Neural Multi-Task Learning for Personalized Recommendation on Taobao. <em>ACM Transactions on Intelligent Systems and Technology</em>, 10(5), 1–25.</li>
<li>Wilhelm, P., Zhang, X., Liao, J., &amp; Zhao, Y. (2018). YouTube Recommendations: Beyond K-Means. <em>Proceedings of the 12th ACM Conference on Recommender Systems</em>, 9–17.</li>
<li>“Building a Multi-Stage Recommender System: A Step-by-Step Guide.” (2024). Generative AI Lab. Retrieved from <a href="https://generativeailab.org/l/machine-learning/building-a-multi-stage-recommender-system-a-step-by-step-guide/">https://generativeailab.org/l/machine-learning/building-a-multi-stage-recommender-system-a-step-by-step-guide/</a> (<a href="https://generativeailab.org/l/machine-learning/building-a-multi-stage-recommender-system-a-step-by-step-guide/1047/" title="Building a Multi-Stage Recommender System: A Step-by-Step Guide">generativeailab.org</a>)</li>
<li>“Multi-Stage Recommender Systems: Concepts, Architectures, and Issues.” (2022). IJCAI. Retrieved from <a href="https://www.ijcai.org/proceedings/2022/0771.pdf">https://www.ijcai.org/proceedings/2022/0771.pdf</a> (<a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org</a>)</li>
<li>“Recommendation systems overview | Machine Learning.” (2025). Google Developers. Retrieved from <a href="https://developers.google.com/machine-learning/recommendation/overview/types">https://developers.google.com/machine-learning/recommendation/overview/types</a> (<a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com</a>)</li>
<li>“Towards a Theoretical Understanding of Two-Stage Recommender Systems.” (2024). arXiv. Retrieved from <a href="https://arxiv.org/pdf/2403.00802">https://arxiv.org/pdf/2403.00802</a> (<a href="https://arxiv.org/pdf/2403.00802" title="Towards a Theoretical Understanding of Two-Stage Recommender Systems">arxiv.org</a>)</li>
<li>“Building and Deploying a Multi-Stage Recommender System with Merlin.” (2022). NVIDIA. Retrieved from <a href="https://resources.nvidia.com/en-us-merlin/bad-a-multi-stage-recommender">https://resources.nvidia.com/en-us-merlin/bad-a-multi-stage-recommender</a> (<a href="https://resources.nvidia.com/en-us-merlin/bad-a-multi-stage-recommender" title="Building and Deploying a Multi-Stage Recommender System with ... - NVIDIA">resources.nvidia.com</a>, <a href="https://assets-global.website-files.com/61398f0b3344b9d4ec0973b9/63221e027375b2aff5b35f76_recsys22_poster_final.pdf" title="Building and Deploying a Multi-Stage Recommender System with Merlin">assets-global.website-files.com</a>)</li>
<li>“How to build a Multi-Stage Recommender System.” (2023). LinkedIn Pulse. Retrieved from <a href="https://www.linkedin.com/pulse/how-build-multi-stage-recommender-system-aayush-agrawal-djdyf">https://www.linkedin.com/pulse/how-build-multi-stage-recommender-system-aayush-agrawal-djdyf</a> (<a href="https://www.linkedin.com/pulse/how-build-multi-stage-recommender-system-aayush-agrawal-djdyf" title="How to build a Multi-Stage Recommender System | Aayush Agrawal - LinkedIn">linkedin.com</a>)</li>
<li>“Multidimensional Insights into Recommender Systems: A Comprehensive Review.” (2025). Springer. Retrieved from <a href="https://link.springer.com/chapter/10.1007/978-3-031-70285-3_29">https://link.springer.com/chapter/10.1007/978-3-031-70285-3_29</a> (<a href="https://link.springer.com/chapter/10.1007/978-3-031-70285-3_29" title="Multidimensional Insights into Recommender Systems: A ... - Springer">link.springer.com</a>)</li>
<li>Schwartz, B. (2004). <em>The Paradox of Choice: Why More Is Less</em>. HarperCollins Publishers.</li>
<li>Vygotsky, L. S. (1978). <em>Mind in Society: The Development of Higher Psychological Processes</em>. Harvard University Press.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi-Stage Approach to Building Recommender Systems on x"
            href="https://x.com/intent/tweet/?text=Multi-Stage%20Approach%20to%20Building%20Recommender%20Systems&amp;url=https%3a%2f%2fpjainish.github.io%2fposts%2fmulti-stage-recommender-systems%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi-Stage Approach to Building Recommender Systems on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpjainish.github.io%2fposts%2fmulti-stage-recommender-systems%2f&amp;title=Multi-Stage%20Approach%20to%20Building%20Recommender%20Systems&amp;summary=Multi-Stage%20Approach%20to%20Building%20Recommender%20Systems&amp;source=https%3a%2f%2fpjainish.github.io%2fposts%2fmulti-stage-recommender-systems%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi-Stage Approach to Building Recommender Systems on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fpjainish.github.io%2fposts%2fmulti-stage-recommender-systems%2f&title=Multi-Stage%20Approach%20to%20Building%20Recommender%20Systems">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi-Stage Approach to Building Recommender Systems on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpjainish.github.io%2fposts%2fmulti-stage-recommender-systems%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi-Stage Approach to Building Recommender Systems on whatsapp"
            href="https://api.whatsapp.com/send?text=Multi-Stage%20Approach%20to%20Building%20Recommender%20Systems%20-%20https%3a%2f%2fpjainish.github.io%2fposts%2fmulti-stage-recommender-systems%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi-Stage Approach to Building Recommender Systems on telegram"
            href="https://telegram.me/share/url?text=Multi-Stage%20Approach%20to%20Building%20Recommender%20Systems&amp;url=https%3a%2f%2fpjainish.github.io%2fposts%2fmulti-stage-recommender-systems%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Multi-Stage Approach to Building Recommender Systems on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Multi-Stage%20Approach%20to%20Building%20Recommender%20Systems&u=https%3a%2f%2fpjainish.github.io%2fposts%2fmulti-stage-recommender-systems%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><div id="disqus_thread"></div>
<script>
    

    

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://pjainish.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://pjainish.github.io/">Jainish&#39;s Log</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
