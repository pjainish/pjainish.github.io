<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Improving Search Relevance Using Large Language Models | Jainish Patel</title>
<meta name="keywords" content="">
<meta name="description" content="Search is the invisible backbone of our digital lives. Every time you type a query into Google, search through Netflix&rsquo;s catalog, or hunt for a specific product on Amazon, you&rsquo;re interacting with systems designed to understand what you really want - not just what you literally typed. But here&rsquo;s the thing: traditional search has always been a bit like playing telephone with a robot that only speaks in keywords.
Large Language Models are changing this game entirely. They&rsquo;re teaching search systems to understand language the way humans do - with context, nuance, and genuine comprehension. The transformation is so profound that we&rsquo;re witnessing the biggest shift in information retrieval since the invention of the web crawler. Let me show you how this revolution works and why it&rsquo;s reshaping everything from how we shop to how we discover knowledge.">
<meta name="author" content="Jainish Patel">
<link rel="canonical" href="http://localhost:1313/posts/improving-search-relevance-using-large-language-models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.35cae19563caab39a3c404b05a80a9c0b422b6401497238582ba86f9c908d490.css" integrity="sha256-NcrhlWPKqzmjxASwWoCpwLQitkAUlyOFgrqG&#43;ckI1JA=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/assets/images/favicon.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/assets/images/favicon.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/assets/images/favicon.png">
<link rel="apple-touch-icon" href="http://localhost:1313/assets/images/favicon.png">
<link rel="mask-icon" href="http://localhost:1313/assets/images/favicon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/improving-search-relevance-using-large-language-models/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-79V8YMLKHG"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-79V8YMLKHG');
</script><meta property="og:url" content="http://localhost:1313/posts/improving-search-relevance-using-large-language-models/">
  <meta property="og:site_name" content="Jainish Patel">
  <meta property="og:title" content="Improving Search Relevance Using Large Language Models">
  <meta property="og:description" content="Search is the invisible backbone of our digital lives. Every time you type a query into Google, search through Netflix’s catalog, or hunt for a specific product on Amazon, you’re interacting with systems designed to understand what you really want - not just what you literally typed. But here’s the thing: traditional search has always been a bit like playing telephone with a robot that only speaks in keywords.
Large Language Models are changing this game entirely. They’re teaching search systems to understand language the way humans do - with context, nuance, and genuine comprehension. The transformation is so profound that we’re witnessing the biggest shift in information retrieval since the invention of the web crawler. Let me show you how this revolution works and why it’s reshaping everything from how we shop to how we discover knowledge.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-03T13:48:45+05:30">
    <meta property="article:modified_time" content="2025-05-03T13:48:45+05:30">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving Search Relevance Using Large Language Models">
<meta name="twitter:description" content="Search is the invisible backbone of our digital lives. Every time you type a query into Google, search through Netflix&rsquo;s catalog, or hunt for a specific product on Amazon, you&rsquo;re interacting with systems designed to understand what you really want - not just what you literally typed. But here&rsquo;s the thing: traditional search has always been a bit like playing telephone with a robot that only speaks in keywords.
Large Language Models are changing this game entirely. They&rsquo;re teaching search systems to understand language the way humans do - with context, nuance, and genuine comprehension. The transformation is so profound that we&rsquo;re witnessing the biggest shift in information retrieval since the invention of the web crawler. Let me show you how this revolution works and why it&rsquo;s reshaping everything from how we shop to how we discover knowledge.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Improving Search Relevance Using Large Language Models",
      "item": "http://localhost:1313/posts/improving-search-relevance-using-large-language-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Improving Search Relevance Using Large Language Models",
  "name": "Improving Search Relevance Using Large Language Models",
  "description": "Search is the invisible backbone of our digital lives. Every time you type a query into Google, search through Netflix\u0026rsquo;s catalog, or hunt for a specific product on Amazon, you\u0026rsquo;re interacting with systems designed to understand what you really want - not just what you literally typed. But here\u0026rsquo;s the thing: traditional search has always been a bit like playing telephone with a robot that only speaks in keywords.\nLarge Language Models are changing this game entirely. They\u0026rsquo;re teaching search systems to understand language the way humans do - with context, nuance, and genuine comprehension. The transformation is so profound that we\u0026rsquo;re witnessing the biggest shift in information retrieval since the invention of the web crawler. Let me show you how this revolution works and why it\u0026rsquo;s reshaping everything from how we shop to how we discover knowledge.\n",
  "keywords": [
    
  ],
  "articleBody": "Search is the invisible backbone of our digital lives. Every time you type a query into Google, search through Netflix’s catalog, or hunt for a specific product on Amazon, you’re interacting with systems designed to understand what you really want - not just what you literally typed. But here’s the thing: traditional search has always been a bit like playing telephone with a robot that only speaks in keywords.\nLarge Language Models are changing this game entirely. They’re teaching search systems to understand language the way humans do - with context, nuance, and genuine comprehension. The transformation is so profound that we’re witnessing the biggest shift in information retrieval since the invention of the web crawler. Let me show you how this revolution works and why it’s reshaping everything from how we shop to how we discover knowledge.\nThe Fundamental Problem with Traditional Search Before we dive into LLMs, let’s understand what traditional search gets wrong - and why millions of engineering hours have been spent trying to fix it. Classic search engines rely on something called lexical matching - they look for exact word matches between your query and documents. When you search for “best Italian restaurant,” the system hunts for documents containing those exact words, like a librarian who can only find books by looking for precise title matches.\nThis approach breaks down in countless frustrating ways. What if someone wrote about “excellent authentic Italian dining” instead of using your exact words? What if you search for “fixing my car’s engine” but the relevant article talks about “automotive repair”? What if you’re looking for information about “COVID-19” but the document uses “coronavirus” or “SARS-CoV-2”? Traditional systems miss these connections because they don’t understand that different words can express the same concept.\nThe problem gets even more complex with vocabulary mismatch - the technical term for when searchers and content creators use different words for the same ideas. Studies show that two people will use the same keyword for the same concept only 20% of the time. This means traditional search systems miss 80% of potentially relevant content simply because of word choice differences.\nEven more sophisticated approaches like TF-IDF (Term Frequency-Inverse Document Frequency) and BM25, which score documents based on word importance and rarity, still operate in this keyword-matching paradigm. TF-IDF works by calculating:\nTF-IDF(t,d) = TF(t,d) × log(N/DF(t))\nWhere TF(t,d) is the frequency of term t in document d, N is the total number of documents, and DF(t) is the number of documents containing term t. This formula helps identify documents where query terms are both frequent and rare across the corpus - a clever heuristic, but still fundamentally limited by exact word matches.\nThe real-world impact is staggering. E-commerce sites lose billions in revenue annually because customers can’t find products they’re actively trying to buy. Academic researchers waste countless hours because they can’t locate papers using slightly different terminology. Enterprise search systems fail to surface critical internal documents because teams use different jargon for the same concepts.\nThe Semantic Revolution: How LLMs Transform Search Understanding Large Language Models solve this by creating semantic representations - mathematical fingerprints that capture meaning rather than just words. When an LLM processes text, it converts it into high-dimensional vectors (typically 768 to 4096 dimensions) where similar meanings naturally cluster together in this mathematical space.\nThink of it like this: imagine meaning exists in a vast landscape where concepts that are related sit close to each other. “Car” and “automobile” would be neighbors, “happy” and “joyful” would be nearby, and “Python programming” and “software development” would share the same neighborhood. But the landscape is far richer than simple synonyms - it captures relationships like “doctor” being close to “hospital,” “stethoscope,” and “patient,” even though these aren’t synonyms.\nThe mathematical foundation is surprisingly elegant. Each word or phrase becomes a vector v in this high-dimensional space, and similarity between concepts is measured using cosine similarity:\nsimilarity(A, B) = (A · B) / (||A|| × ||B||)\nWhere A · B is the dot product and ||A|| represents the vector magnitude. This simple formula captures semantic relationships that keyword matching could never find.\nBut here’s where it gets really interesting: these vectors capture not just explicit relationships but also subtle contextual nuances. The word “bank” will have different vector representations depending on whether it appears in contexts about finance (“bank account,” “loan officer”) or geography (“river bank,” “steep bank”). This contextual sensitivity is what makes LLM-based search so powerful.\nThe training process that creates these representations is fascinating. LLMs learn by predicting the next word in billions of text sequences, and through this process, they develop an internal understanding of how concepts relate to each other. Words that appear in similar contexts end up with similar vector representations - not because anyone explicitly taught the model that “happy” and “joyful” are related, but because both words tend to appear in contexts about positive emotions.\nDense Retrieval: The Core Architecture Revolution The breakthrough came with dense retrieval systems that use LLMs to encode both queries and documents into the same semantic space. This seemingly simple idea required solving numerous technical challenges and has become the foundation of modern search systems.\nHere’s how it works: When you submit a query, the system passes it through an encoder (typically a transformer model like BERT or a more recent architecture) to produce a query vector. Similarly, all documents in the search corpus have been pre-processed and converted into document vectors using the same encoder. This preprocessing step is crucial - for a large corpus like Wikipedia, this might involve encoding millions of documents, each taking milliseconds to process.\nFinding relevant documents becomes a nearest neighbor search in this vector space. Documents whose vectors are closest to your query vector - measured by cosine similarity - are the most semantically relevant results. What makes this powerful is that “best Italian restaurant” and “top-rated authentic Italian dining” will produce very similar vectors, even though they share no common words.\nBut the magic really happens when you see the system handle complex queries. Consider searching for “how to reduce anxiety before public speaking.” Traditional systems would look for exact matches of these words. A dense retrieval system understands that documents about “managing presentation nerves,” “overcoming stage fright,” or “confidence building for speeches” are all highly relevant, even though they use completely different vocabulary.\nThe technical implementation involves several sophisticated components. Query encoding must be fast since it happens in real-time when users search. Document encoding can be slower since it’s done offline, but it needs to be consistent - the same document should always produce the same vector. Vector storage requires efficient data structures since you’re storing millions of high-dimensional vectors. Similarity search needs to be optimized since comparing your query vector against millions of document vectors would be too slow without clever algorithms.\nThe Architecture Wars: Bi-encoder vs Cross-encoder The field has converged on two main architectural approaches, each with distinct trade-offs that matter enormously for real-world deployment. Understanding these trade-offs is crucial because they determine everything from search speed to accuracy to cost.\nBi-encoders process queries and documents separately, creating independent vector representations. This separation is computationally efficient because document vectors can be pre-computed and stored, making real-time search fast. When you search, only the query needs to be encoded, and then it’s just a matter of comparing the query vector against pre-computed document vectors.\nThe speed advantage is massive. A bi-encoder can search through millions of documents in milliseconds because it’s just doing vector arithmetic. This is why companies like Google and Microsoft can provide near-instantaneous search results across the entire web.\nHowever, bi-encoders miss the subtle interactions between query and document that can signal relevance. When you search for “jaguar repair manual,” a bi-encoder treats “jaguar” and “repair manual” as separate concepts. It might not fully understand that in this context, “jaguar” likely refers to the car brand rather than the animal.\nCross-encoders process query and document together, allowing the model to consider their interaction directly. They see the full context: “jaguar repair manual” as a unified concept. This produces more nuanced relevance scores because the model can reason about how the query and document relate to each other.\nThe technical difference is profound. A cross-encoder takes concatenated text like “[CLS] jaguar repair manual [SEP] This guide covers maintenance for Jaguar F-Type engines…” and processes it as a single sequence. The model’s attention mechanism can directly connect “jaguar” in the query with “Jaguar F-Type” in the document, understanding the relationship.\nBut cross-encoders come with a severe computational cost. They require computing a new representation for every query-document pair at search time. For a query against a million-document corpus, that’s a million separate model forward passes - far too slow for real-time search.\nThe elegant solution? Cascade architecture that uses bi-encoders for fast initial retrieval to narrow down candidates, then applies cross-encoders for precise re-ranking of the top results. This hybrid approach captures the best of both worlds: the speed of bi-encoders for broad retrieval and the accuracy of cross-encoders for final ranking.\nTraining LLMs for Search: The Art and Science of Relevance Teaching an LLM to excel at search requires sophisticated training strategies that go beyond standard language modeling. The key insight is that relevance is inherently comparative - knowing that document A is more relevant than document B for a given query matters more than knowing the absolute relevance of either document.\nContrastive learning has emerged as the dominant training paradigm, and it’s beautifully intuitive once you understand it. For each query, the model sees positive examples (relevant documents) and negative examples (irrelevant ones), learning to pull positive pairs closer together in vector space while pushing negative pairs apart.\nThe loss function typically looks like:\nL = -log(exp(sim(q, d+) / τ) / Σ exp(sim(q, di) / τ))\nWhere q is the query, d+ is a relevant document, di represents all documents in the batch, and τ is a temperature parameter that controls the sharpness of the distribution.\nThis mathematical formulation captures something profound about how humans think about relevance. We don’t judge documents in isolation - we compare them. When you search for “best pizza NYC,” you’re not looking for documents that meet some absolute standard of pizza-related relevance. You want the documents that are most relevant compared to all other possible documents.\nThe challenge is getting high-quality training data. Early systems used click-through data - assuming that if users clicked on a result, it was relevant. But this creates biases. Users tend to click on results that appear higher in the search rankings, regardless of actual relevance. They’re also more likely to click on familiar-looking results or those with appealing titles.\nMore sophisticated approaches use hard negative mining - deliberately including challenging negative examples that are topically related but not truly relevant. This forces the model to make finer distinctions and improves its precision. For a query about “Python programming,” easy negatives might be documents about biology or cooking. Hard negatives would be documents about other programming languages or general computer science topics.\nThe training process itself is computationally intensive. Modern search models are trained on datasets with millions of query-document pairs, using clusters of GPUs for weeks or months. The computational cost is enormous - training a competitive search model can cost hundreds of thousands of dollars in cloud computing resources.\nBut the results justify the investment. Well-trained search models can achieve 40-60% improvements in relevance metrics compared to traditional systems. More importantly, they handle the long tail of queries - the millions of unique searches that users perform daily but that traditional systems struggle with.\nMulti-Vector and Late Interaction: Beyond Single Vectors Recent innovations have moved beyond single vector representations toward more nuanced approaches that preserve fine-grained information while maintaining computational efficiency. This represents a fundamental shift in how we think about semantic search.\nColBERT (Contextualized Late Interaction over BERT) represents both queries and documents as collections of vectors - one for each token - rather than compressing everything into a single vector. This seemingly simple change solves a major problem with single-vector approaches: information loss.\nWhen you compress an entire document into a single vector, you inevitably lose details. A document about “machine learning applications in healthcare” might have its vector positioned somewhere between “machine learning” and “healthcare” in the semantic space, but important nuances about specific applications or methodologies get lost.\nColBERT preserves this information by keeping separate vectors for each token. During retrieval, it computes fine-grained interactions between query and document tokens, finding the maximum similarity between each query token and all document tokens. This approach captures term-level evidence while maintaining the semantic understanding of transformer models.\nThe scoring function becomes:\nScore(q, d) = Σ max(Eq,i · Ed,j)\nWhere Eq,i and Ed,j are token-level embeddings. This means each query token finds its best match in the document, and the overall score is the sum of these individual matches.\nThe practical impact is remarkable. ColBERT can understand that a query for “deep learning optimization techniques” matches a document discussing “neural network training algorithms” because individual query tokens find strong matches with semantically related document tokens, even when the overall phrasing is different.\nBut ColBERT introduces new challenges. Storage requirements increase dramatically since you’re storing vectors for every token in every document. A single document might require hundreds of vectors instead of just one. Search becomes more complex since you need to compute interactions between query and document token sets.\nThe engineering solutions are clever. Compression techniques reduce the storage overhead by clustering similar token vectors and storing cluster centroids. Efficient interaction algorithms speed up the max-pooling operations required for scoring. Caching strategies store frequently accessed token vectors in memory for faster retrieval.\nHandling Multi-Modal Search: Beyond Text Modern search increasingly involves multiple modalities - text, images, code, audio, and video. Users expect to search across all these content types seamlessly, and LLMs trained on multi-modal data are making this possible.\nCLIP (Contrastive Language-Image Pre-training) pioneered this approach for text-image search. The model learns joint representations where semantically related text and images occupy nearby positions in the shared vector space. This enables queries like “sunset over mountains” to retrieve relevant images, even if those images were never explicitly tagged with those words.\nThe training process for CLIP is fascinating. The model sees millions of image-text pairs scraped from the web and learns to associate images with their captions. Through this process, it develops an understanding of visual concepts that can be expressed in language. A photo of a golden retriever becomes associated not just with the text “golden retriever” but with related concepts like “dog,” “pet,” “furry,” and “friendly.”\nThis capability is transforming e-commerce search. Instead of requiring manual tagging of product images, systems can now understand visual queries. Users can search for “red dress with floral pattern” and find relevant products even if the product descriptions don’t use those exact words. The system can see the red color and floral pattern in the images and match them to the textual query.\nFor code search, models like CodeBERT apply similar principles, understanding that a query for “sort a list in Python” should match code snippets that implement sorting algorithms, regardless of variable names or specific syntax variations. The model learns that array.sort(), sorted(my_list), and custom sorting implementations are all semantically related to the concept of sorting.\nThe technical challenges are substantial. Different modalities have vastly different characteristics - images are high-dimensional pixel arrays, text is sequential tokens, code has syntactic structure, and audio has temporal dynamics. Creating unified representations requires careful architectural design and massive amounts of training data.\nVision-language models use shared transformer architectures that can process both visual and textual inputs. Multi-modal fusion techniques combine information from different modalities at various levels - early fusion concatenates raw inputs, late fusion combines processed representations, and hybrid approaches use attention mechanisms to dynamically weight different modalities.\nThe impact extends beyond search. Multi-modal understanding enables content generation (generating captions for images), cross-modal retrieval (finding images that match text descriptions), and content understanding (analyzing videos to extract searchable information).\nQuery Understanding: Parsing Intent and Context LLMs don’t just improve document matching - they transform query understanding itself. Traditional systems treated queries as bags of keywords, but LLMs can parse intent, identify entities, extract relationships, and understand context in ways that feel almost magical.\nConsider the query “Apple stock price yesterday.” This simple seven-word query contains multiple layers of meaning that an LLM-powered system can parse:\nEntity recognition: “Apple” refers to Apple Inc., the technology company, not the fruit Intent classification: This is a factual information query, specifically about financial data Temporal understanding: “Yesterday” provides specific temporal context Implicit requirements: The user wants current, accurate financial information An LLM-powered system can recognize all these elements and trigger specialized retrieval paths, combining general web search with real-time financial data APIs. It might even understand that if the query is made on a Monday, “yesterday” refers to Friday (since markets are closed on weekends).\nQuery expansion becomes far more sophisticated. Instead of simple synonym replacement, LLMs can generate semantically related terms that preserve the original intent while broadening coverage. A query about “sustainable energy” might be expanded to include “renewable power,” “clean electricity,” “green technology,” “solar panels,” “wind turbines,” and “energy efficiency.”\nBut the expansion is contextually aware. The same query in different contexts might expand differently. “Sustainable energy” in an academic context might include terms like “photovoltaic efficiency” and “grid integration,” while in a consumer context it might include “solar installation” and “energy savings.”\nAmbiguity resolution is another area where LLMs excel. The query “Java” could refer to the programming language, the Indonesian island, or the type of coffee. Traditional systems might return results for all three, forcing users to sort through irrelevant results. LLMs can use context clues to disambiguate - if the user’s previous queries were about programming, “Java” likely refers to the programming language.\nThe system might also consider user context without storing personal information. If the query comes from an IP address associated with a university computer science department, the programming language interpretation becomes more likely. If it comes from a travel website, the island interpretation gains weight.\nConversational search represents the next frontier. Instead of treating each query in isolation, systems can maintain context across multiple interactions. A user might start with “best restaurants in Paris,” then follow up with “which ones have vegetarian options?” The system understands that “ones” refers to the previously mentioned Paris restaurants.\nPersonalization Through Contextual Embeddings LLMs enable personalized search that adapts to individual users without compromising privacy. This represents a significant advancement over traditional personalization approaches that required storing detailed user profiles and behavioral histories.\nInstead of storing explicit user profiles, systems can create contextual embeddings that incorporate recent search history, location, behavioral signals, and preferences directly into the query representation. This approach keeps user data ephemeral while still providing personalized results.\nThe personalized query vector becomes:\nq_personalized = α × q_base + β × q_context + γ × q_temporal\nWhere q_base is the original query embedding, q_context captures personalization signals, q_temporal includes time-sensitive factors, and α, β, γ are learned weighting parameters that determine the influence of each component.\nThe personalization signals can be remarkably subtle yet powerful. If a user frequently searches for technical programming content, their query for “Python” will be biased toward programming-related results. If they often search for cooking recipes, the same query might lean toward food-related content.\nImplicit personalization works through behavioral signals that don’t require explicit user input. Click-through patterns, dwell time on results, query reformulations, and scrolling behavior all provide signals about user preferences and intent. LLMs can incorporate these signals without storing personally identifiable information.\nThe privacy implications are significant. Traditional personalization required building detailed user profiles that posed privacy risks and regulatory challenges. LLM-based personalization can work with ephemeral context, processing personalization signals in real-time without long-term storage.\nFederated learning approaches allow personalization models to improve from user interactions without centralizing personal data. Local models adapt to individual user patterns while contributing to global model improvements through privacy-preserving techniques like differential privacy.\nThe business impact is substantial. Personalized search improves user satisfaction, increases engagement, and drives better business outcomes. E-commerce sites see higher conversion rates when search results match user preferences. Content platforms achieve better user retention when recommendations align with individual tastes.\nReal-Time Learning and Adaptation Unlike traditional search systems that require manual tuning and periodic retraining, LLM-based search can adapt continuously to changing user behavior, emerging topics, and new content patterns. This adaptability is crucial in our rapidly evolving information landscape.\nOnline learning techniques allow models to incorporate feedback from user interactions in real-time. When users click on search results, skip over others, or reformulate queries, these signals provide training data for continuous model improvement. The challenge is updating large language models efficiently without full retraining - a computationally expensive process that can take weeks.\nTechniques like LoRA (Low-Rank Adaptation) and prefix tuning provide solutions by updating only small portions of the model parameters. LoRA works by adding low-rank matrices to the model’s weight matrices, allowing adaptation with minimal computational overhead:\nW_adapted = W_original + A × B\nWhere W_original is the original weight matrix, and A and B are small matrices whose product approximates the needed weight updates. This approach can achieve 90% of full fine-tuning performance while updating less than 1% of the model parameters.\nGradient-based meta-learning enables models to quickly adapt to new domains or query types with minimal examples. The model learns not just to perform search, but to learn how to adapt its search behavior based on new signals.\nThe feedback loop operates at multiple timescales. Immediate adaptation happens within seconds of user interactions, adjusting result rankings based on real-time signals. Short-term adaptation occurs over hours or days, incorporating patterns from recent user sessions. Long-term adaptation happens over weeks or months, capturing fundamental shifts in user behavior or content trends.\nTrending topic detection becomes automatic as the system notices unusual query patterns and content interactions. When a major news event occurs, the system can quickly identify and boost relevant content without manual intervention. This is particularly important for breaking news, viral content, and seasonal topics.\nThe technical infrastructure required is sophisticated. Streaming data processing systems handle millions of user interactions per second. Distributed training frameworks update model parameters across multiple servers. Version control systems manage model updates while ensuring consistent user experiences.\nScaling Challenges: Engineering for Internet Scale Deploying LLMs for search at internet scale presents unique engineering challenges that push the boundaries of what’s computationally feasible. The numbers are staggering - Google processes over 8 billion searches per day, each requiring millisecond response times across a corpus of trillions of documents.\nThe computational cost of encoding queries and performing vector similarity search over billions of documents requires careful optimization at every level of the stack. Query encoding must complete in single-digit milliseconds, which means careful model architecture choices and optimized inference pipelines.\nApproximate nearest neighbor (ANN) algorithms like FAISS, Annoy, and ScaNN make vector search tractable by trading small amounts of recall for dramatic speedups. These algorithms use clever data structures and approximation techniques to avoid computing exact distances between all vector pairs.\nFAISS, developed by Meta, uses techniques like locality-sensitive hashing and product quantization to achieve sub-linear search times. The key insight is that you don’t need to find the absolute nearest neighbors - you just need to find vectors that are close enough to represent the most relevant documents.\nQuantization techniques reduce memory requirements and speed up computations by representing vectors with lower precision. Instead of storing 32-bit floating-point values, systems might use 8-bit integers or even binary representations. The storage savings are enormous - 8-bit quantization reduces memory requirements by 75% while maintaining most of the search quality.\nHierarchical search architectures split large document collections into clusters, first identifying relevant clusters before searching within them. This reduces the effective search space and enables sub-linear scaling. The clustering process itself uses sophisticated algorithms to ensure that semantically similar documents end up in the same clusters.\nDistributed search spreads the workload across multiple servers, with each server handling a subset of the document corpus. Query processing becomes a distributed computing problem, requiring careful load balancing and result aggregation.\nThe caching strategies are multilayered. Popular queries are cached at the query level, frequent document vectors are cached in memory, and intermediate results are cached at various stages of the processing pipeline. Cache hit rates above 90% are common for web search workloads.\nHardware optimization plays a crucial role. Modern search systems use specialized hardware like GPUs and TPUs for vector operations, high-memory servers for storing vector indices, and fast storage systems for rapid data access. The hardware costs are substantial - a competitive web search system might require thousands of servers and millions of dollars in hardware.\nQuality Measurement and Evaluation: Beyond Traditional Metrics Measuring search quality with LLMs requires rethinking traditional evaluation approaches. The semantic understanding capabilities of LLMs create new opportunities for both better search results and more sophisticated evaluation methods.\nNDCG (Normalized Discounted Cumulative Gain) and MRR (Mean Reciprocal Rank) remain important foundational metrics, but they don’t capture the nuanced improvements that LLMs bring to search. NDCG measures the quality of ranked lists by considering both relevance and position:\nNDCG@k = DCG@k / IDCG@k\nwhere DCG@k is the discounted cumulative gain up to position k, and IDCG@k is the ideal DCG for perfect ranking.\nHowever, these metrics assume that relevance judgments are binary or based on simple relevance scales. LLM-based search systems can provide more nuanced understanding of relevance that traditional metrics miss.\nSemantic similarity between retrieved and expected results provides a new evaluation dimension. Instead of just measuring whether the correct documents were retrieved, systems can evaluate whether the retrieved documents are semantically related to the expected results. This is particularly valuable for evaluating query expansion and semantic matching capabilities.\nQuery-document relevance can be scored by separate LLMs trained specifically for relevance assessment. These models can provide more consistent and scalable relevance judgments than human annotators, especially for large-scale evaluation datasets.\nUser satisfaction metrics derived from behavioral data provide the ultimate measure of search quality. Metrics like success rate (percentage of queries that result in user satisfaction), time to success (how long users spend before finding what they need), and reformulation rate (how often users need to modify their queries) capture the real-world impact of search improvements.\nFailure analysis becomes more sophisticated with LLMs. Instead of just identifying queries that perform poorly, systems can analyze why they fail and categorize failure modes. Common categories include vocabulary mismatch (user and document use different terms), intent ambiguity (query has multiple possible interpretations), knowledge gaps (relevant information doesn’t exist in the corpus), and temporal mismatches (user wants current information but corpus is outdated).\nA/B testing remains crucial, but it’s complemented by more sophisticated analysis techniques. Interleaving experiments mix results from different systems to get more sensitive measurements of relative quality. Long-term impact studies measure how search improvements affect user behavior over weeks or months, not just immediate click-through rates.\nFairness and bias evaluation becomes critical as LLMs can perpetuate or amplify biases present in training data. Search systems need to be evaluated for demographic fairness, ensuring that results don’t systematically favor or disadvantage particular groups. This requires specialized evaluation datasets and metrics that can detect subtle forms of bias.\nEmerging Frontiers: The Future of Intelligent Search The field continues evolving at a breathtaking pace, with new developments emerging monthly that reshape what’s possible in information retrieval. The convergence of large language models with search is opening entirely new paradigms for how we interact with information.\nRetrieval-augmented generation (RAG) systems represent a fundamental shift from traditional search. Instead of returning a list of documents, these systems combine LLM-based search with generative capabilities to synthesize answers from multiple retrieved documents. Users get direct answers to their questions, backed by retrieved evidence.\nThe architecture is elegant: when you ask “What are the health benefits of regular exercise?”, the system first retrieves relevant documents from medical literature, fitness research, and health databases. Then a generative LLM synthesizes this information into a coherent answer, citing specific sources and providing a comprehensive response.\nRAG systems can handle complex queries that require synthesizing information from multiple sources. A query like “Compare the environmental impact of electric vehicles versus traditional cars, considering manufacturing, operation, and disposal” would require gathering information from multiple documents and combining it into a coherent comparison.\nNeural information retrieval is moving toward end-to-end learning where retrieval and ranking are jointly optimized. Traditional systems treat retrieval and ranking as separate problems, but end-to-end approaches learn both simultaneously, potentially achieving better overall performance.\nSparse-dense hybrid models combine the interpretability of traditional keyword matching with the semantic power of dense vectors. These systems maintain both sparse representations (traditional keyword features) and dense representations (semantic vectors), combining them through learned weighting mechanisms.\nThe hybrid approach addresses a key limitation of pure dense retrieval: the “black box” problem. With traditional keyword matching, you can understand why a document was retrieved - it contained the query terms. With dense retrieval, the reasoning is opaque - documents are retrieved based on high-dimensional vector similarities that humans can’t easily interpret.\nHybrid systems provide the best of both worlds: the recall improvements of semantic search with the interpretability of keyword matching. They can also handle queries that require exact matches (like product model numbers or specific phrases) while still providing semantic understanding for natural language queries.\nFederated search across multiple specialized corpora using shared LLM representations promises to break down silos between different search systems. Currently, users must search separately across web search engines, academic databases, internal company documents, and social media platforms. Federated search would enable unified discovery across these previously disconnected information sources.\nThe technical challenges are substantial. Different corpora have different formats, update frequencies, access controls, and relevance patterns. A unified search system needs to handle these differences while providing consistent user experiences.\nMultimodal search expansion is extending beyond text and images to include audio, video, and interactive content. Users might search for “examples of good public speaking” and retrieve not just articles about public speaking but also video examples, audio recordings of great speeches, and interactive tutorials.\nConversational search interfaces are becoming more sophisticated, supporting multi-turn interactions where users can refine their queries through natural dialogue. Instead of struggling to formulate the perfect query, users can engage in a conversation with the search system, gradually narrowing down to exactly what they need.\nPersonalized knowledge graphs combine the structured representation of knowledge graphs with the personalization capabilities of LLMs. These systems build dynamic, personalized views of information that adapt to individual user interests and expertise levels.\nReal-time search over streaming data is becoming more important as information becomes increasingly dynamic. Social media posts, news articles, stock prices, and user-generated content are constantly changing, and search systems need to index and search this information in real-time.\nThe Broader Impact: Transforming How We Interact with Information The transformation of search through LLMs extends far beyond technical improvements - it’s fundamentally changing how we discover, learn, and interact with information. The implications ripple through education, commerce, research, and daily life in ways we’re only beginning to understand.\nEducational search is becoming more like having a knowledgeable tutor. Instead of returning a list of potentially relevant documents, LLM-powered educational search can provide explanations tailored to the user’s level of understanding, suggest follow-up questions, and guide learning pathways. A student struggling with calculus can get not just links to calculus resources, but explanations that build on their existing knowledge and address their specific confusion.\nScientific research is being accelerated by LLMs that can search across vast corpora of academic literature, identify connections between disparate fields, and suggest novel research directions. Researchers can query across millions of papers using natural language, finding relevant work even when it uses different terminology or comes from unexpected disciplines.\nEnterprise search is solving the chronic problem of organizational knowledge silos. Companies often have valuable information scattered across documents, databases, wikis, and email archives, but employees can’t find what they need. LLM-powered enterprise search can understand context, navigate organizational jargon, and surface relevant information regardless of where it’s stored.\nE-commerce search is becoming more conversational and helpful. Instead of forcing users to navigate complex category hierarchies or guess the right keywords, shopping platforms can understand natural language queries like “comfortable running shoes for flat feet under $100” and provide relevant results even when product descriptions don’t use those exact terms.\nHealthcare information retrieval is improving patient outcomes by helping both healthcare providers and patients find relevant medical information more effectively. Doctors can quickly search through medical literature to find the latest treatment protocols, while patients can get reliable health information without wading through irrelevant or potentially harmful content.\nThe democratization of information access is perhaps the most profound impact. High-quality search capabilities that were once available only to companies with massive technical resources are becoming accessible to smaller organizations and individuals. This levels the playing field and enables innovation in unexpected places.\nAccessibility improvements are making information more available to users with different needs and abilities. LLM-powered search can provide results in different formats, reading levels, and languages, making information more accessible to diverse audiences.\nBut the transformation also brings challenges. Information quality becomes more important as LLMs can make low-quality information seem authoritative. Bias amplification is a concern as LLMs might perpetuate or amplify biases present in training data. Privacy implications arise as more sophisticated search requires more understanding of user context and intent.\nDigital literacy becomes more important as users need to understand how to effectively query LLM-powered systems and critically evaluate the results. The shift from keyword-based to conversational search requires new skills and mental models.\nThe ultimate vision emerging from this transformation is search that understands not just what you’re looking for, but why you’re looking for it - and can proactively surface information you didn’t even know you needed. LLMs are bringing us closer to that reality every day, creating search experiences that feel less like querying a database and more like having a conversation with a knowledgeable assistant who has read everything and can help you make sense of it all.\nSearch relevance is no longer about matching words - it’s about understanding meaning, context, and intent. This understanding is transforming how we discover, learn, and connect with information in ways that seemed like science fiction just a few years ago. The revolution is happening now, and it’s reshaping not just how we search, but how we think about information itself.\nReferences and Further Reading Karpukhin, V., et al. “Dense Passage Retrieval for Open-Domain Question Answering.” EMNLP 2020. Khattab, O., \u0026 Zaharia, M. “ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.” SIGIR 2020. Radford, A., et al. “Learning Transferable Visual Models From Natural Language Supervision.” ICML 2021. Reimers, N., \u0026 Gurevych, I. “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.” EMNLP 2019. Xiong, L., et al. “Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval.” ICLR 2021. Guo, J., et al. “A Deep Look into Neural Ranking Models for Information Retrieval.” Information Processing \u0026 Management 2020. Lin, J., et al. “Pretrained Transformers for Text Ranking: BERT and Beyond.” Journal of the American Society for Information Science and Technology 2021. Thakur, N., et al. “BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models.” NeurIPS 2021. Zhan, J., et al. “Optimizing Dense Retrieval Model Training with Hard Negatives.” SIGIR 2021. Santhanam, K., et al. “ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction.” NAACL 2022. Lewis, P., et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” NeurIPS 2020. Izacard, G., \u0026 Grave, E. “Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.” EACL 2021. Kenton, L., \u0026 Toutanova, K. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” NAACL 2019. Johnson, J., et al. “Billion-scale Similarity Search with GPUs.” IEEE Transactions on Big Data 2019. Malkov, Y., \u0026 Yashunin, D. “Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.” IEEE Transactions on Pattern Analysis and Machine Intelligence 2020. Hofstätter, S., et al. “Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling.” SIGIR 2021. Qu, Y., et al. “RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering.” NAACL 2021. Xiong, L., et al. “Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary.” NAACL 2019. Nogueira, R., \u0026 Cho, K. “Passage Re-ranking with BERT.” arXiv preprint arXiv:1901.04085 2019. Dai, Z., \u0026 Callan, J. “Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval.” arXiv preprint arXiv:1910.10687 2019. Formal, T., et al. “SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking.” SIGIR 2021. Lin, S., et al. “Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term Importance Estimation and Neural Query Rewriting.” SIGIR 2021. ",
  "wordCount" : "6159",
  "inLanguage": "en",
  "datePublished": "2025-05-03T13:48:45+05:30",
  "dateModified": "2025-05-03T13:48:45+05:30",
  "author":{
    "@type": "Person",
    "name": "Jainish Patel"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/improving-search-relevance-using-large-language-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Jainish Patel",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/assets/images/favicon.png"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Jainish Patel (Alt + H)">
                <img src="http://localhost:1313/assets/images/favicon.png" alt="" aria-label="logo"
                    height="30">Jainish Patel</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/case-study/" title="Case Studies">
                    <span>Case Studies</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Improving Search Relevance Using Large Language Models
    </h1>
    <div class="post-meta"><span title='2025-05-03 13:48:45 +0530 IST'>May 3, 2025</span>&nbsp;·&nbsp;29 min&nbsp;·&nbsp;Jainish Patel

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-fundamental-problem-with-traditional-search" aria-label="The Fundamental Problem with Traditional Search">The Fundamental Problem with Traditional Search</a></li>
                <li>
                    <a href="#the-semantic-revolution-how-llms-transform-search-understanding" aria-label="The Semantic Revolution: How LLMs Transform Search Understanding">The Semantic Revolution: How LLMs Transform Search Understanding</a></li>
                <li>
                    <a href="#dense-retrieval-the-core-architecture-revolution" aria-label="Dense Retrieval: The Core Architecture Revolution">Dense Retrieval: The Core Architecture Revolution</a></li>
                <li>
                    <a href="#the-architecture-wars-bi-encoder-vs-cross-encoder" aria-label="The Architecture Wars: Bi-encoder vs Cross-encoder">The Architecture Wars: Bi-encoder vs Cross-encoder</a></li>
                <li>
                    <a href="#training-llms-for-search-the-art-and-science-of-relevance" aria-label="Training LLMs for Search: The Art and Science of Relevance">Training LLMs for Search: The Art and Science of Relevance</a></li>
                <li>
                    <a href="#multi-vector-and-late-interaction-beyond-single-vectors" aria-label="Multi-Vector and Late Interaction: Beyond Single Vectors">Multi-Vector and Late Interaction: Beyond Single Vectors</a></li>
                <li>
                    <a href="#handling-multi-modal-search-beyond-text" aria-label="Handling Multi-Modal Search: Beyond Text">Handling Multi-Modal Search: Beyond Text</a></li>
                <li>
                    <a href="#query-understanding-parsing-intent-and-context" aria-label="Query Understanding: Parsing Intent and Context">Query Understanding: Parsing Intent and Context</a></li>
                <li>
                    <a href="#personalization-through-contextual-embeddings" aria-label="Personalization Through Contextual Embeddings">Personalization Through Contextual Embeddings</a></li>
                <li>
                    <a href="#real-time-learning-and-adaptation" aria-label="Real-Time Learning and Adaptation">Real-Time Learning and Adaptation</a></li>
                <li>
                    <a href="#scaling-challenges-engineering-for-internet-scale" aria-label="Scaling Challenges: Engineering for Internet Scale">Scaling Challenges: Engineering for Internet Scale</a></li>
                <li>
                    <a href="#quality-measurement-and-evaluation-beyond-traditional-metrics" aria-label="Quality Measurement and Evaluation: Beyond Traditional Metrics">Quality Measurement and Evaluation: Beyond Traditional Metrics</a></li>
                <li>
                    <a href="#emerging-frontiers-the-future-of-intelligent-search" aria-label="Emerging Frontiers: The Future of Intelligent Search">Emerging Frontiers: The Future of Intelligent Search</a></li>
                <li>
                    <a href="#the-broader-impact-transforming-how-we-interact-with-information" aria-label="The Broader Impact: Transforming How We Interact with Information">The Broader Impact: Transforming How We Interact with Information</a></li>
                <li>
                    <a href="#references-and-further-reading" aria-label="References and Further Reading">References and Further Reading</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Search is the invisible backbone of our digital lives. Every time you type a query into Google, search through Netflix&rsquo;s catalog, or hunt for a specific product on Amazon, you&rsquo;re interacting with systems designed to understand what you really want - not just what you literally typed. But here&rsquo;s the thing: traditional search has always been a bit like playing telephone with a robot that only speaks in keywords.</p>
<p>Large Language Models are changing this game entirely. They&rsquo;re teaching search systems to understand language the way humans do - with context, nuance, and genuine comprehension. The transformation is so profound that we&rsquo;re witnessing the biggest shift in information retrieval since the invention of the web crawler. Let me show you how this revolution works and why it&rsquo;s reshaping everything from how we shop to how we discover knowledge.</p>
<h2 id="the-fundamental-problem-with-traditional-search">The Fundamental Problem with Traditional Search<a hidden class="anchor" aria-hidden="true" href="#the-fundamental-problem-with-traditional-search">#</a></h2>
<p>Before we dive into LLMs, let&rsquo;s understand what traditional search gets wrong - and why millions of engineering hours have been spent trying to fix it. Classic search engines rely on something called <strong>lexical matching</strong> - they look for exact word matches between your query and documents. When you search for &ldquo;best Italian restaurant,&rdquo; the system hunts for documents containing those exact words, like a librarian who can only find books by looking for precise title matches.</p>
<p>This approach breaks down in countless frustrating ways. What if someone wrote about &ldquo;excellent authentic Italian dining&rdquo; instead of using your exact words? What if you search for &ldquo;fixing my car&rsquo;s engine&rdquo; but the relevant article talks about &ldquo;automotive repair&rdquo;? What if you&rsquo;re looking for information about &ldquo;COVID-19&rdquo; but the document uses &ldquo;coronavirus&rdquo; or &ldquo;SARS-CoV-2&rdquo;? Traditional systems miss these connections because they don&rsquo;t understand that different words can express the same concept.</p>
<p>The problem gets even more complex with <strong>vocabulary mismatch</strong> - the technical term for when searchers and content creators use different words for the same ideas. Studies show that two people will use the same keyword for the same concept only 20% of the time. This means traditional search systems miss 80% of potentially relevant content simply because of word choice differences.</p>
<p>Even more sophisticated approaches like TF-IDF (Term Frequency-Inverse Document Frequency) and BM25, which score documents based on word importance and rarity, still operate in this keyword-matching paradigm. TF-IDF works by calculating:</p>
<p><strong>TF-IDF(t,d) = TF(t,d) × log(N/DF(t))</strong></p>
<p>Where TF(t,d) is the frequency of term t in document d, N is the total number of documents, and DF(t) is the number of documents containing term t. This formula helps identify documents where query terms are both frequent and rare across the corpus - a clever heuristic, but still fundamentally limited by exact word matches.</p>
<p>The real-world impact is staggering. E-commerce sites lose billions in revenue annually because customers can&rsquo;t find products they&rsquo;re actively trying to buy. Academic researchers waste countless hours because they can&rsquo;t locate papers using slightly different terminology. Enterprise search systems fail to surface critical internal documents because teams use different jargon for the same concepts.</p>
<h2 id="the-semantic-revolution-how-llms-transform-search-understanding">The Semantic Revolution: How LLMs Transform Search Understanding<a hidden class="anchor" aria-hidden="true" href="#the-semantic-revolution-how-llms-transform-search-understanding">#</a></h2>
<p>Large Language Models solve this by creating <strong>semantic representations</strong> - mathematical fingerprints that capture meaning rather than just words. When an LLM processes text, it converts it into high-dimensional vectors (typically 768 to 4096 dimensions) where similar meanings naturally cluster together in this mathematical space.</p>
<p>Think of it like this: imagine meaning exists in a vast landscape where concepts that are related sit close to each other. &ldquo;Car&rdquo; and &ldquo;automobile&rdquo; would be neighbors, &ldquo;happy&rdquo; and &ldquo;joyful&rdquo; would be nearby, and &ldquo;Python programming&rdquo; and &ldquo;software development&rdquo; would share the same neighborhood. But the landscape is far richer than simple synonyms - it captures relationships like &ldquo;doctor&rdquo; being close to &ldquo;hospital,&rdquo; &ldquo;stethoscope,&rdquo; and &ldquo;patient,&rdquo; even though these aren&rsquo;t synonyms.</p>
<p>The mathematical foundation is surprisingly elegant. Each word or phrase becomes a vector <strong>v</strong> in this high-dimensional space, and similarity between concepts is measured using cosine similarity:</p>
<p><strong>similarity(A, B) = (A · B) / (||A|| × ||B||)</strong></p>
<p>Where A · B is the dot product and ||A|| represents the vector magnitude. This simple formula captures semantic relationships that keyword matching could never find.</p>
<p>But here&rsquo;s where it gets really interesting: these vectors capture not just explicit relationships but also subtle contextual nuances. The word &ldquo;bank&rdquo; will have different vector representations depending on whether it appears in contexts about finance (&ldquo;bank account,&rdquo; &ldquo;loan officer&rdquo;) or geography (&ldquo;river bank,&rdquo; &ldquo;steep bank&rdquo;). This <strong>contextual sensitivity</strong> is what makes LLM-based search so powerful.</p>
<p>The training process that creates these representations is fascinating. LLMs learn by predicting the next word in billions of text sequences, and through this process, they develop an internal understanding of how concepts relate to each other. Words that appear in similar contexts end up with similar vector representations - not because anyone explicitly taught the model that &ldquo;happy&rdquo; and &ldquo;joyful&rdquo; are related, but because both words tend to appear in contexts about positive emotions.</p>
<h2 id="dense-retrieval-the-core-architecture-revolution">Dense Retrieval: The Core Architecture Revolution<a hidden class="anchor" aria-hidden="true" href="#dense-retrieval-the-core-architecture-revolution">#</a></h2>
<p>The breakthrough came with <strong>dense retrieval</strong> systems that use LLMs to encode both queries and documents into the same semantic space. This seemingly simple idea required solving numerous technical challenges and has become the foundation of modern search systems.</p>
<p>Here&rsquo;s how it works: When you submit a query, the system passes it through an encoder (typically a transformer model like BERT or a more recent architecture) to produce a query vector. Similarly, all documents in the search corpus have been pre-processed and converted into document vectors using the same encoder. This preprocessing step is crucial - for a large corpus like Wikipedia, this might involve encoding millions of documents, each taking milliseconds to process.</p>
<p>Finding relevant documents becomes a nearest neighbor search in this vector space. Documents whose vectors are closest to your query vector - measured by cosine similarity - are the most semantically relevant results. What makes this powerful is that &ldquo;best Italian restaurant&rdquo; and &ldquo;top-rated authentic Italian dining&rdquo; will produce very similar vectors, even though they share no common words.</p>
<p>But the magic really happens when you see the system handle complex queries. Consider searching for &ldquo;how to reduce anxiety before public speaking.&rdquo; Traditional systems would look for exact matches of these words. A dense retrieval system understands that documents about &ldquo;managing presentation nerves,&rdquo; &ldquo;overcoming stage fright,&rdquo; or &ldquo;confidence building for speeches&rdquo; are all highly relevant, even though they use completely different vocabulary.</p>
<p>The technical implementation involves several sophisticated components. <strong>Query encoding</strong> must be fast since it happens in real-time when users search. <strong>Document encoding</strong> can be slower since it&rsquo;s done offline, but it needs to be consistent - the same document should always produce the same vector. <strong>Vector storage</strong> requires efficient data structures since you&rsquo;re storing millions of high-dimensional vectors. <strong>Similarity search</strong> needs to be optimized since comparing your query vector against millions of document vectors would be too slow without clever algorithms.</p>
<h2 id="the-architecture-wars-bi-encoder-vs-cross-encoder">The Architecture Wars: Bi-encoder vs Cross-encoder<a hidden class="anchor" aria-hidden="true" href="#the-architecture-wars-bi-encoder-vs-cross-encoder">#</a></h2>
<p>The field has converged on two main architectural approaches, each with distinct trade-offs that matter enormously for real-world deployment. Understanding these trade-offs is crucial because they determine everything from search speed to accuracy to cost.</p>
<p><strong>Bi-encoders</strong> process queries and documents separately, creating independent vector representations. This separation is computationally efficient because document vectors can be pre-computed and stored, making real-time search fast. When you search, only the query needs to be encoded, and then it&rsquo;s just a matter of comparing the query vector against pre-computed document vectors.</p>
<p>The speed advantage is massive. A bi-encoder can search through millions of documents in milliseconds because it&rsquo;s just doing vector arithmetic. This is why companies like Google and Microsoft can provide near-instantaneous search results across the entire web.</p>
<p>However, bi-encoders miss the subtle interactions between query and document that can signal relevance. When you search for &ldquo;jaguar repair manual,&rdquo; a bi-encoder treats &ldquo;jaguar&rdquo; and &ldquo;repair manual&rdquo; as separate concepts. It might not fully understand that in this context, &ldquo;jaguar&rdquo; likely refers to the car brand rather than the animal.</p>
<p><strong>Cross-encoders</strong> process query and document together, allowing the model to consider their interaction directly. They see the full context: &ldquo;jaguar repair manual&rdquo; as a unified concept. This produces more nuanced relevance scores because the model can reason about how the query and document relate to each other.</p>
<p>The technical difference is profound. A cross-encoder takes concatenated text like &ldquo;[CLS] jaguar repair manual [SEP] This guide covers maintenance for Jaguar F-Type engines&hellip;&rdquo; and processes it as a single sequence. The model&rsquo;s attention mechanism can directly connect &ldquo;jaguar&rdquo; in the query with &ldquo;Jaguar F-Type&rdquo; in the document, understanding the relationship.</p>
<p>But cross-encoders come with a severe computational cost. They require computing a new representation for every query-document pair at search time. For a query against a million-document corpus, that&rsquo;s a million separate model forward passes - far too slow for real-time search.</p>
<p>The elegant solution? <strong>Cascade architecture</strong> that uses bi-encoders for fast initial retrieval to narrow down candidates, then applies cross-encoders for precise re-ranking of the top results. This hybrid approach captures the best of both worlds: the speed of bi-encoders for broad retrieval and the accuracy of cross-encoders for final ranking.</p>
<h2 id="training-llms-for-search-the-art-and-science-of-relevance">Training LLMs for Search: The Art and Science of Relevance<a hidden class="anchor" aria-hidden="true" href="#training-llms-for-search-the-art-and-science-of-relevance">#</a></h2>
<p>Teaching an LLM to excel at search requires sophisticated training strategies that go beyond standard language modeling. The key insight is that relevance is inherently comparative - knowing that document A is more relevant than document B for a given query matters more than knowing the absolute relevance of either document.</p>
<p><strong>Contrastive learning</strong> has emerged as the dominant training paradigm, and it&rsquo;s beautifully intuitive once you understand it. For each query, the model sees positive examples (relevant documents) and negative examples (irrelevant ones), learning to pull positive pairs closer together in vector space while pushing negative pairs apart.</p>
<p>The loss function typically looks like:</p>
<p><strong>L = -log(exp(sim(q, d+) / τ) / Σ exp(sim(q, di) / τ))</strong></p>
<p>Where q is the query, d+ is a relevant document, di represents all documents in the batch, and τ is a temperature parameter that controls the sharpness of the distribution.</p>
<p>This mathematical formulation captures something profound about how humans think about relevance. We don&rsquo;t judge documents in isolation - we compare them. When you search for &ldquo;best pizza NYC,&rdquo; you&rsquo;re not looking for documents that meet some absolute standard of pizza-related relevance. You want the documents that are most relevant compared to all other possible documents.</p>
<p>The challenge is getting high-quality training data. Early systems used click-through data - assuming that if users clicked on a result, it was relevant. But this creates biases. Users tend to click on results that appear higher in the search rankings, regardless of actual relevance. They&rsquo;re also more likely to click on familiar-looking results or those with appealing titles.</p>
<p>More sophisticated approaches use <strong>hard negative mining</strong> - deliberately including challenging negative examples that are topically related but not truly relevant. This forces the model to make finer distinctions and improves its precision. For a query about &ldquo;Python programming,&rdquo; easy negatives might be documents about biology or cooking. Hard negatives would be documents about other programming languages or general computer science topics.</p>
<p>The training process itself is computationally intensive. Modern search models are trained on datasets with millions of query-document pairs, using clusters of GPUs for weeks or months. The computational cost is enormous - training a competitive search model can cost hundreds of thousands of dollars in cloud computing resources.</p>
<p>But the results justify the investment. Well-trained search models can achieve 40-60% improvements in relevance metrics compared to traditional systems. More importantly, they handle the long tail of queries - the millions of unique searches that users perform daily but that traditional systems struggle with.</p>
<h2 id="multi-vector-and-late-interaction-beyond-single-vectors">Multi-Vector and Late Interaction: Beyond Single Vectors<a hidden class="anchor" aria-hidden="true" href="#multi-vector-and-late-interaction-beyond-single-vectors">#</a></h2>
<p>Recent innovations have moved beyond single vector representations toward more nuanced approaches that preserve fine-grained information while maintaining computational efficiency. This represents a fundamental shift in how we think about semantic search.</p>
<p><strong>ColBERT</strong> (Contextualized Late Interaction over BERT) represents both queries and documents as collections of vectors - one for each token - rather than compressing everything into a single vector. This seemingly simple change solves a major problem with single-vector approaches: information loss.</p>
<p>When you compress an entire document into a single vector, you inevitably lose details. A document about &ldquo;machine learning applications in healthcare&rdquo; might have its vector positioned somewhere between &ldquo;machine learning&rdquo; and &ldquo;healthcare&rdquo; in the semantic space, but important nuances about specific applications or methodologies get lost.</p>
<p>ColBERT preserves this information by keeping separate vectors for each token. During retrieval, it computes fine-grained interactions between query and document tokens, finding the maximum similarity between each query token and all document tokens. This approach captures term-level evidence while maintaining the semantic understanding of transformer models.</p>
<p>The scoring function becomes:</p>
<p><strong>Score(q, d) = Σ max(Eq,i · Ed,j)</strong></p>
<p>Where Eq,i and Ed,j are token-level embeddings. This means each query token finds its best match in the document, and the overall score is the sum of these individual matches.</p>
<p>The practical impact is remarkable. ColBERT can understand that a query for &ldquo;deep learning optimization techniques&rdquo; matches a document discussing &ldquo;neural network training algorithms&rdquo; because individual query tokens find strong matches with semantically related document tokens, even when the overall phrasing is different.</p>
<p>But ColBERT introduces new challenges. Storage requirements increase dramatically since you&rsquo;re storing vectors for every token in every document. A single document might require hundreds of vectors instead of just one. Search becomes more complex since you need to compute interactions between query and document token sets.</p>
<p>The engineering solutions are clever. <strong>Compression techniques</strong> reduce the storage overhead by clustering similar token vectors and storing cluster centroids. <strong>Efficient interaction algorithms</strong> speed up the max-pooling operations required for scoring. <strong>Caching strategies</strong> store frequently accessed token vectors in memory for faster retrieval.</p>
<h2 id="handling-multi-modal-search-beyond-text">Handling Multi-Modal Search: Beyond Text<a hidden class="anchor" aria-hidden="true" href="#handling-multi-modal-search-beyond-text">#</a></h2>
<p>Modern search increasingly involves multiple modalities - text, images, code, audio, and video. Users expect to search across all these content types seamlessly, and LLMs trained on multi-modal data are making this possible.</p>
<p><strong>CLIP</strong> (Contrastive Language-Image Pre-training) pioneered this approach for text-image search. The model learns joint representations where semantically related text and images occupy nearby positions in the shared vector space. This enables queries like &ldquo;sunset over mountains&rdquo; to retrieve relevant images, even if those images were never explicitly tagged with those words.</p>
<p>The training process for CLIP is fascinating. The model sees millions of image-text pairs scraped from the web and learns to associate images with their captions. Through this process, it develops an understanding of visual concepts that can be expressed in language. A photo of a golden retriever becomes associated not just with the text &ldquo;golden retriever&rdquo; but with related concepts like &ldquo;dog,&rdquo; &ldquo;pet,&rdquo; &ldquo;furry,&rdquo; and &ldquo;friendly.&rdquo;</p>
<p>This capability is transforming e-commerce search. Instead of requiring manual tagging of product images, systems can now understand visual queries. Users can search for &ldquo;red dress with floral pattern&rdquo; and find relevant products even if the product descriptions don&rsquo;t use those exact words. The system can see the red color and floral pattern in the images and match them to the textual query.</p>
<p>For code search, models like <strong>CodeBERT</strong> apply similar principles, understanding that a query for &ldquo;sort a list in Python&rdquo; should match code snippets that implement sorting algorithms, regardless of variable names or specific syntax variations. The model learns that <code>array.sort()</code>, <code>sorted(my_list)</code>, and custom sorting implementations are all semantically related to the concept of sorting.</p>
<p>The technical challenges are substantial. Different modalities have vastly different characteristics - images are high-dimensional pixel arrays, text is sequential tokens, code has syntactic structure, and audio has temporal dynamics. Creating unified representations requires careful architectural design and massive amounts of training data.</p>
<p><strong>Vision-language models</strong> use shared transformer architectures that can process both visual and textual inputs. <strong>Multi-modal fusion</strong> techniques combine information from different modalities at various levels - early fusion concatenates raw inputs, late fusion combines processed representations, and hybrid approaches use attention mechanisms to dynamically weight different modalities.</p>
<p>The impact extends beyond search. Multi-modal understanding enables <strong>content generation</strong> (generating captions for images), <strong>cross-modal retrieval</strong> (finding images that match text descriptions), and <strong>content understanding</strong> (analyzing videos to extract searchable information).</p>
<h2 id="query-understanding-parsing-intent-and-context">Query Understanding: Parsing Intent and Context<a hidden class="anchor" aria-hidden="true" href="#query-understanding-parsing-intent-and-context">#</a></h2>
<p>LLMs don&rsquo;t just improve document matching - they transform query understanding itself. Traditional systems treated queries as bags of keywords, but LLMs can parse intent, identify entities, extract relationships, and understand context in ways that feel almost magical.</p>
<p>Consider the query &ldquo;Apple stock price yesterday.&rdquo; This simple seven-word query contains multiple layers of meaning that an LLM-powered system can parse:</p>
<ol>
<li><strong>Entity recognition</strong>: &ldquo;Apple&rdquo; refers to Apple Inc., the technology company, not the fruit</li>
<li><strong>Intent classification</strong>: This is a factual information query, specifically about financial data</li>
<li><strong>Temporal understanding</strong>: &ldquo;Yesterday&rdquo; provides specific temporal context</li>
<li><strong>Implicit requirements</strong>: The user wants current, accurate financial information</li>
</ol>
<p>An LLM-powered system can recognize all these elements and trigger specialized retrieval paths, combining general web search with real-time financial data APIs. It might even understand that if the query is made on a Monday, &ldquo;yesterday&rdquo; refers to Friday (since markets are closed on weekends).</p>
<p><strong>Query expansion</strong> becomes far more sophisticated. Instead of simple synonym replacement, LLMs can generate semantically related terms that preserve the original intent while broadening coverage. A query about &ldquo;sustainable energy&rdquo; might be expanded to include &ldquo;renewable power,&rdquo; &ldquo;clean electricity,&rdquo; &ldquo;green technology,&rdquo; &ldquo;solar panels,&rdquo; &ldquo;wind turbines,&rdquo; and &ldquo;energy efficiency.&rdquo;</p>
<p>But the expansion is contextually aware. The same query in different contexts might expand differently. &ldquo;Sustainable energy&rdquo; in an academic context might include terms like &ldquo;photovoltaic efficiency&rdquo; and &ldquo;grid integration,&rdquo; while in a consumer context it might include &ldquo;solar installation&rdquo; and &ldquo;energy savings.&rdquo;</p>
<p><strong>Ambiguity resolution</strong> is another area where LLMs excel. The query &ldquo;Java&rdquo; could refer to the programming language, the Indonesian island, or the type of coffee. Traditional systems might return results for all three, forcing users to sort through irrelevant results. LLMs can use context clues to disambiguate - if the user&rsquo;s previous queries were about programming, &ldquo;Java&rdquo; likely refers to the programming language.</p>
<p>The system might also consider <strong>user context</strong> without storing personal information. If the query comes from an IP address associated with a university computer science department, the programming language interpretation becomes more likely. If it comes from a travel website, the island interpretation gains weight.</p>
<p><strong>Conversational search</strong> represents the next frontier. Instead of treating each query in isolation, systems can maintain context across multiple interactions. A user might start with &ldquo;best restaurants in Paris,&rdquo; then follow up with &ldquo;which ones have vegetarian options?&rdquo; The system understands that &ldquo;ones&rdquo; refers to the previously mentioned Paris restaurants.</p>
<h2 id="personalization-through-contextual-embeddings">Personalization Through Contextual Embeddings<a hidden class="anchor" aria-hidden="true" href="#personalization-through-contextual-embeddings">#</a></h2>
<p>LLMs enable personalized search that adapts to individual users without compromising privacy. This represents a significant advancement over traditional personalization approaches that required storing detailed user profiles and behavioral histories.</p>
<p>Instead of storing explicit user profiles, systems can create <strong>contextual embeddings</strong> that incorporate recent search history, location, behavioral signals, and preferences directly into the query representation. This approach keeps user data ephemeral while still providing personalized results.</p>
<p>The personalized query vector becomes:</p>
<p><strong>q_personalized = α × q_base + β × q_context + γ × q_temporal</strong></p>
<p>Where q_base is the original query embedding, q_context captures personalization signals, q_temporal includes time-sensitive factors, and α, β, γ are learned weighting parameters that determine the influence of each component.</p>
<p>The personalization signals can be remarkably subtle yet powerful. If a user frequently searches for technical programming content, their query for &ldquo;Python&rdquo; will be biased toward programming-related results. If they often search for cooking recipes, the same query might lean toward food-related content.</p>
<p><strong>Implicit personalization</strong> works through behavioral signals that don&rsquo;t require explicit user input. Click-through patterns, dwell time on results, query reformulations, and scrolling behavior all provide signals about user preferences and intent. LLMs can incorporate these signals without storing personally identifiable information.</p>
<p>The privacy implications are significant. Traditional personalization required building detailed user profiles that posed privacy risks and regulatory challenges. LLM-based personalization can work with ephemeral context, processing personalization signals in real-time without long-term storage.</p>
<p><strong>Federated learning</strong> approaches allow personalization models to improve from user interactions without centralizing personal data. Local models adapt to individual user patterns while contributing to global model improvements through privacy-preserving techniques like differential privacy.</p>
<p>The business impact is substantial. Personalized search improves user satisfaction, increases engagement, and drives better business outcomes. E-commerce sites see higher conversion rates when search results match user preferences. Content platforms achieve better user retention when recommendations align with individual tastes.</p>
<h2 id="real-time-learning-and-adaptation">Real-Time Learning and Adaptation<a hidden class="anchor" aria-hidden="true" href="#real-time-learning-and-adaptation">#</a></h2>
<p>Unlike traditional search systems that require manual tuning and periodic retraining, LLM-based search can adapt continuously to changing user behavior, emerging topics, and new content patterns. This adaptability is crucial in our rapidly evolving information landscape.</p>
<p><strong>Online learning</strong> techniques allow models to incorporate feedback from user interactions in real-time. When users click on search results, skip over others, or reformulate queries, these signals provide training data for continuous model improvement. The challenge is updating large language models efficiently without full retraining - a computationally expensive process that can take weeks.</p>
<p>Techniques like <strong>LoRA</strong> (Low-Rank Adaptation) and <strong>prefix tuning</strong> provide solutions by updating only small portions of the model parameters. LoRA works by adding low-rank matrices to the model&rsquo;s weight matrices, allowing adaptation with minimal computational overhead:</p>
<p><strong>W_adapted = W_original + A × B</strong></p>
<p>Where W_original is the original weight matrix, and A and B are small matrices whose product approximates the needed weight updates. This approach can achieve 90% of full fine-tuning performance while updating less than 1% of the model parameters.</p>
<p><strong>Gradient-based meta-learning</strong> enables models to quickly adapt to new domains or query types with minimal examples. The model learns not just to perform search, but to learn how to adapt its search behavior based on new signals.</p>
<p>The feedback loop operates at multiple timescales. <strong>Immediate adaptation</strong> happens within seconds of user interactions, adjusting result rankings based on real-time signals. <strong>Short-term adaptation</strong> occurs over hours or days, incorporating patterns from recent user sessions. <strong>Long-term adaptation</strong> happens over weeks or months, capturing fundamental shifts in user behavior or content trends.</p>
<p><strong>Trending topic detection</strong> becomes automatic as the system notices unusual query patterns and content interactions. When a major news event occurs, the system can quickly identify and boost relevant content without manual intervention. This is particularly important for breaking news, viral content, and seasonal topics.</p>
<p>The technical infrastructure required is sophisticated. <strong>Streaming data processing</strong> systems handle millions of user interactions per second. <strong>Distributed training</strong> frameworks update model parameters across multiple servers. <strong>Version control</strong> systems manage model updates while ensuring consistent user experiences.</p>
<h2 id="scaling-challenges-engineering-for-internet-scale">Scaling Challenges: Engineering for Internet Scale<a hidden class="anchor" aria-hidden="true" href="#scaling-challenges-engineering-for-internet-scale">#</a></h2>
<p>Deploying LLMs for search at internet scale presents unique engineering challenges that push the boundaries of what&rsquo;s computationally feasible. The numbers are staggering - Google processes over 8 billion searches per day, each requiring millisecond response times across a corpus of trillions of documents.</p>
<p>The computational cost of encoding queries and performing vector similarity search over billions of documents requires careful optimization at every level of the stack. <strong>Query encoding</strong> must complete in single-digit milliseconds, which means careful model architecture choices and optimized inference pipelines.</p>
<p><strong>Approximate nearest neighbor</strong> (ANN) algorithms like FAISS, Annoy, and ScaNN make vector search tractable by trading small amounts of recall for dramatic speedups. These algorithms use clever data structures and approximation techniques to avoid computing exact distances between all vector pairs.</p>
<p>FAISS, developed by Meta, uses techniques like <strong>locality-sensitive hashing</strong> and <strong>product quantization</strong> to achieve sub-linear search times. The key insight is that you don&rsquo;t need to find the absolute nearest neighbors - you just need to find vectors that are close enough to represent the most relevant documents.</p>
<p><strong>Quantization</strong> techniques reduce memory requirements and speed up computations by representing vectors with lower precision. Instead of storing 32-bit floating-point values, systems might use 8-bit integers or even binary representations. The storage savings are enormous - 8-bit quantization reduces memory requirements by 75% while maintaining most of the search quality.</p>
<p><strong>Hierarchical search</strong> architectures split large document collections into clusters, first identifying relevant clusters before searching within them. This reduces the effective search space and enables sub-linear scaling. The clustering process itself uses sophisticated algorithms to ensure that semantically similar documents end up in the same clusters.</p>
<p><strong>Distributed search</strong> spreads the workload across multiple servers, with each server handling a subset of the document corpus. Query processing becomes a distributed computing problem, requiring careful load balancing and result aggregation.</p>
<p>The <strong>caching strategies</strong> are multilayered. Popular queries are cached at the query level, frequent document vectors are cached in memory, and intermediate results are cached at various stages of the processing pipeline. Cache hit rates above 90% are common for web search workloads.</p>
<p><strong>Hardware optimization</strong> plays a crucial role. Modern search systems use specialized hardware like GPUs and TPUs for vector operations, high-memory servers for storing vector indices, and fast storage systems for rapid data access. The hardware costs are substantial - a competitive web search system might require thousands of servers and millions of dollars in hardware.</p>
<h2 id="quality-measurement-and-evaluation-beyond-traditional-metrics">Quality Measurement and Evaluation: Beyond Traditional Metrics<a hidden class="anchor" aria-hidden="true" href="#quality-measurement-and-evaluation-beyond-traditional-metrics">#</a></h2>
<p>Measuring search quality with LLMs requires rethinking traditional evaluation approaches. The semantic understanding capabilities of LLMs create new opportunities for both better search results and more sophisticated evaluation methods.</p>
<p><strong>NDCG</strong> (Normalized Discounted Cumulative Gain) and <strong>MRR</strong> (Mean Reciprocal Rank) remain important foundational metrics, but they don&rsquo;t capture the nuanced improvements that LLMs bring to search. NDCG measures the quality of ranked lists by considering both relevance and position:</p>
<p><strong>NDCG@k = DCG@k / IDCG@k</strong></p>
<p>where DCG@k is the discounted cumulative gain up to position k, and IDCG@k is the ideal DCG for perfect ranking.</p>
<p>However, these metrics assume that relevance judgments are binary or based on simple relevance scales. LLM-based search systems can provide more nuanced understanding of relevance that traditional metrics miss.</p>
<p><strong>Semantic similarity</strong> between retrieved and expected results provides a new evaluation dimension. Instead of just measuring whether the correct documents were retrieved, systems can evaluate whether the retrieved documents are semantically related to the expected results. This is particularly valuable for evaluating query expansion and semantic matching capabilities.</p>
<p><strong>Query-document relevance</strong> can be scored by separate LLMs trained specifically for relevance assessment. These models can provide more consistent and scalable relevance judgments than human annotators, especially for large-scale evaluation datasets.</p>
<p><strong>User satisfaction</strong> metrics derived from behavioral data provide the ultimate measure of search quality. Metrics like <strong>success rate</strong> (percentage of queries that result in user satisfaction), <strong>time to success</strong> (how long users spend before finding what they need), and <strong>reformulation rate</strong> (how often users need to modify their queries) capture the real-world impact of search improvements.</p>
<p><strong>Failure analysis</strong> becomes more sophisticated with LLMs. Instead of just identifying queries that perform poorly, systems can analyze why they fail and categorize failure modes. Common categories include <strong>vocabulary mismatch</strong> (user and document use different terms), <strong>intent ambiguity</strong> (query has multiple possible interpretations), <strong>knowledge gaps</strong> (relevant information doesn&rsquo;t exist in the corpus), and <strong>temporal mismatches</strong> (user wants current information but corpus is outdated).</p>
<p><strong>A/B testing</strong> remains crucial, but it&rsquo;s complemented by more sophisticated analysis techniques. <strong>Interleaving experiments</strong> mix results from different systems to get more sensitive measurements of relative quality. <strong>Long-term impact studies</strong> measure how search improvements affect user behavior over weeks or months, not just immediate click-through rates.</p>
<p><strong>Fairness and bias evaluation</strong> becomes critical as LLMs can perpetuate or amplify biases present in training data. Search systems need to be evaluated for demographic fairness, ensuring that results don&rsquo;t systematically favor or disadvantage particular groups. This requires specialized evaluation datasets and metrics that can detect subtle forms of bias.</p>
<h2 id="emerging-frontiers-the-future-of-intelligent-search">Emerging Frontiers: The Future of Intelligent Search<a hidden class="anchor" aria-hidden="true" href="#emerging-frontiers-the-future-of-intelligent-search">#</a></h2>
<p>The field continues evolving at a breathtaking pace, with new developments emerging monthly that reshape what&rsquo;s possible in information retrieval. The convergence of large language models with search is opening entirely new paradigms for how we interact with information.</p>
<p><strong>Retrieval-augmented generation</strong> (RAG) systems represent a fundamental shift from traditional search. Instead of returning a list of documents, these systems combine LLM-based search with generative capabilities to synthesize answers from multiple retrieved documents. Users get direct answers to their questions, backed by retrieved evidence.</p>
<p>The architecture is elegant: when you ask &ldquo;What are the health benefits of regular exercise?&rdquo;, the system first retrieves relevant documents from medical literature, fitness research, and health databases. Then a generative LLM synthesizes this information into a coherent answer, citing specific sources and providing a comprehensive response.</p>
<p>RAG systems can handle complex queries that require synthesizing information from multiple sources. A query like &ldquo;Compare the environmental impact of electric vehicles versus traditional cars, considering manufacturing, operation, and disposal&rdquo; would require gathering information from multiple documents and combining it into a coherent comparison.</p>
<p><strong>Neural information retrieval</strong> is moving toward end-to-end learning where retrieval and ranking are jointly optimized. Traditional systems treat retrieval and ranking as separate problems, but end-to-end approaches learn both simultaneously, potentially achieving better overall performance.</p>
<p><strong>Sparse-dense hybrid</strong> models combine the interpretability of traditional keyword matching with the semantic power of dense vectors. These systems maintain both sparse representations (traditional keyword features) and dense representations (semantic vectors), combining them through learned weighting mechanisms.</p>
<p>The hybrid approach addresses a key limitation of pure dense retrieval: the &ldquo;black box&rdquo; problem. With traditional keyword matching, you can understand why a document was retrieved - it contained the query terms. With dense retrieval, the reasoning is opaque - documents are retrieved based on high-dimensional vector similarities that humans can&rsquo;t easily interpret.</p>
<p>Hybrid systems provide the best of both worlds: the recall improvements of semantic search with the interpretability of keyword matching. They can also handle queries that require exact matches (like product model numbers or specific phrases) while still providing semantic understanding for natural language queries.</p>
<p><strong>Federated search</strong> across multiple specialized corpora using shared LLM representations promises to break down silos between different search systems. Currently, users must search separately across web search engines, academic databases, internal company documents, and social media platforms. Federated search would enable unified discovery across these previously disconnected information sources.</p>
<p>The technical challenges are substantial. Different corpora have different formats, update frequencies, access controls, and relevance patterns. A unified search system needs to handle these differences while providing consistent user experiences.</p>
<p><strong>Multimodal search expansion</strong> is extending beyond text and images to include audio, video, and interactive content. Users might search for &ldquo;examples of good public speaking&rdquo; and retrieve not just articles about public speaking but also video examples, audio recordings of great speeches, and interactive tutorials.</p>
<p><strong>Conversational search interfaces</strong> are becoming more sophisticated, supporting multi-turn interactions where users can refine their queries through natural dialogue. Instead of struggling to formulate the perfect query, users can engage in a conversation with the search system, gradually narrowing down to exactly what they need.</p>
<p><strong>Personalized knowledge graphs</strong> combine the structured representation of knowledge graphs with the personalization capabilities of LLMs. These systems build dynamic, personalized views of information that adapt to individual user interests and expertise levels.</p>
<p><strong>Real-time search</strong> over streaming data is becoming more important as information becomes increasingly dynamic. Social media posts, news articles, stock prices, and user-generated content are constantly changing, and search systems need to index and search this information in real-time.</p>
<h2 id="the-broader-impact-transforming-how-we-interact-with-information">The Broader Impact: Transforming How We Interact with Information<a hidden class="anchor" aria-hidden="true" href="#the-broader-impact-transforming-how-we-interact-with-information">#</a></h2>
<p>The transformation of search through LLMs extends far beyond technical improvements - it&rsquo;s fundamentally changing how we discover, learn, and interact with information. The implications ripple through education, commerce, research, and daily life in ways we&rsquo;re only beginning to understand.</p>
<p><strong>Educational search</strong> is becoming more like having a knowledgeable tutor. Instead of returning a list of potentially relevant documents, LLM-powered educational search can provide explanations tailored to the user&rsquo;s level of understanding, suggest follow-up questions, and guide learning pathways. A student struggling with calculus can get not just links to calculus resources, but explanations that build on their existing knowledge and address their specific confusion.</p>
<p><strong>Scientific research</strong> is being accelerated by LLMs that can search across vast corpora of academic literature, identify connections between disparate fields, and suggest novel research directions. Researchers can query across millions of papers using natural language, finding relevant work even when it uses different terminology or comes from unexpected disciplines.</p>
<p><strong>Enterprise search</strong> is solving the chronic problem of organizational knowledge silos. Companies often have valuable information scattered across documents, databases, wikis, and email archives, but employees can&rsquo;t find what they need. LLM-powered enterprise search can understand context, navigate organizational jargon, and surface relevant information regardless of where it&rsquo;s stored.</p>
<p><strong>E-commerce search</strong> is becoming more conversational and helpful. Instead of forcing users to navigate complex category hierarchies or guess the right keywords, shopping platforms can understand natural language queries like &ldquo;comfortable running shoes for flat feet under $100&rdquo; and provide relevant results even when product descriptions don&rsquo;t use those exact terms.</p>
<p><strong>Healthcare information retrieval</strong> is improving patient outcomes by helping both healthcare providers and patients find relevant medical information more effectively. Doctors can quickly search through medical literature to find the latest treatment protocols, while patients can get reliable health information without wading through irrelevant or potentially harmful content.</p>
<p>The <strong>democratization of information access</strong> is perhaps the most profound impact. High-quality search capabilities that were once available only to companies with massive technical resources are becoming accessible to smaller organizations and individuals. This levels the playing field and enables innovation in unexpected places.</p>
<p><strong>Accessibility improvements</strong> are making information more available to users with different needs and abilities. LLM-powered search can provide results in different formats, reading levels, and languages, making information more accessible to diverse audiences.</p>
<p>But the transformation also brings challenges. <strong>Information quality</strong> becomes more important as LLMs can make low-quality information seem authoritative. <strong>Bias amplification</strong> is a concern as LLMs might perpetuate or amplify biases present in training data. <strong>Privacy implications</strong> arise as more sophisticated search requires more understanding of user context and intent.</p>
<p><strong>Digital literacy</strong> becomes more important as users need to understand how to effectively query LLM-powered systems and critically evaluate the results. The shift from keyword-based to conversational search requires new skills and mental models.</p>
<p>The ultimate vision emerging from this transformation is search that understands not just what you&rsquo;re looking for, but why you&rsquo;re looking for it - and can proactively surface information you didn&rsquo;t even know you needed. LLMs are bringing us closer to that reality every day, creating search experiences that feel less like querying a database and more like having a conversation with a knowledgeable assistant who has read everything and can help you make sense of it all.</p>
<p>Search relevance is no longer about matching words - it&rsquo;s about understanding meaning, context, and intent. This understanding is transforming how we discover, learn, and connect with information in ways that seemed like science fiction just a few years ago. The revolution is happening now, and it&rsquo;s reshaping not just how we search, but how we think about information itself.</p>
<h2 id="references-and-further-reading">References and Further Reading<a hidden class="anchor" aria-hidden="true" href="#references-and-further-reading">#</a></h2>
<ul>
<li>Karpukhin, V., et al. &ldquo;Dense Passage Retrieval for Open-Domain Question Answering.&rdquo; EMNLP 2020.</li>
<li>Khattab, O., &amp; Zaharia, M. &ldquo;ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.&rdquo; SIGIR 2020.</li>
<li>Radford, A., et al. &ldquo;Learning Transferable Visual Models From Natural Language Supervision.&rdquo; ICML 2021.</li>
<li>Reimers, N., &amp; Gurevych, I. &ldquo;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.&rdquo; EMNLP 2019.</li>
<li>Xiong, L., et al. &ldquo;Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval.&rdquo; ICLR 2021.</li>
<li>Guo, J., et al. &ldquo;A Deep Look into Neural Ranking Models for Information Retrieval.&rdquo; Information Processing &amp; Management 2020.</li>
<li>Lin, J., et al. &ldquo;Pretrained Transformers for Text Ranking: BERT and Beyond.&rdquo; Journal of the American Society for Information Science and Technology 2021.</li>
<li>Thakur, N., et al. &ldquo;BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models.&rdquo; NeurIPS 2021.</li>
<li>Zhan, J., et al. &ldquo;Optimizing Dense Retrieval Model Training with Hard Negatives.&rdquo; SIGIR 2021.</li>
<li>Santhanam, K., et al. &ldquo;ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction.&rdquo; NAACL 2022.</li>
<li>Lewis, P., et al. &ldquo;Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.&rdquo; NeurIPS 2020.</li>
<li>Izacard, G., &amp; Grave, E. &ldquo;Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.&rdquo; EACL 2021.</li>
<li>Kenton, L., &amp; Toutanova, K. &ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.&rdquo; NAACL 2019.</li>
<li>Johnson, J., et al. &ldquo;Billion-scale Similarity Search with GPUs.&rdquo; IEEE Transactions on Big Data 2019.</li>
<li>Malkov, Y., &amp; Yashunin, D. &ldquo;Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.&rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence 2020.</li>
<li>Hofstätter, S., et al. &ldquo;Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling.&rdquo; SIGIR 2021.</li>
<li>Qu, Y., et al. &ldquo;RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering.&rdquo; NAACL 2021.</li>
<li>Xiong, L., et al. &ldquo;Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary.&rdquo; NAACL 2019.</li>
<li>Nogueira, R., &amp; Cho, K. &ldquo;Passage Re-ranking with BERT.&rdquo; arXiv preprint arXiv:1901.04085 2019.</li>
<li>Dai, Z., &amp; Callan, J. &ldquo;Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval.&rdquo; arXiv preprint arXiv:1910.10687 2019.</li>
<li>Formal, T., et al. &ldquo;SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking.&rdquo; SIGIR 2021.</li>
<li>Lin, S., et al. &ldquo;Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term Importance Estimation and Neural Query Rewriting.&rdquo; SIGIR 2021.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Search Relevance Using Large Language Models on x"
            href="https://x.com/intent/tweet/?text=Improving%20Search%20Relevance%20Using%20Large%20Language%20Models&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fimproving-search-relevance-using-large-language-models%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Search Relevance Using Large Language Models on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fimproving-search-relevance-using-large-language-models%2f&amp;title=Improving%20Search%20Relevance%20Using%20Large%20Language%20Models&amp;summary=Improving%20Search%20Relevance%20Using%20Large%20Language%20Models&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fimproving-search-relevance-using-large-language-models%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Search Relevance Using Large Language Models on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fimproving-search-relevance-using-large-language-models%2f&title=Improving%20Search%20Relevance%20Using%20Large%20Language%20Models">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Search Relevance Using Large Language Models on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fimproving-search-relevance-using-large-language-models%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Search Relevance Using Large Language Models on whatsapp"
            href="https://api.whatsapp.com/send?text=Improving%20Search%20Relevance%20Using%20Large%20Language%20Models%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fimproving-search-relevance-using-large-language-models%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Search Relevance Using Large Language Models on telegram"
            href="https://telegram.me/share/url?text=Improving%20Search%20Relevance%20Using%20Large%20Language%20Models&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fimproving-search-relevance-using-large-language-models%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Improving Search Relevance Using Large Language Models on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Improving%20Search%20Relevance%20Using%20Large%20Language%20Models&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fimproving-search-relevance-using-large-language-models%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><div id="disqus_thread"></div>
<script>
    

    

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://pjainish.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Jainish Patel</a></span> · 

    <span>
        <a href="https://pjainish.github.io/sitemap.xml">Sitemap</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script></body>

</html>
