<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Jainish's Log</title><link>https://pjainish.github.io/posts/</link><description>Recent content in Posts on Jainish's Log</description><generator>Hugo -- 0.134.3</generator><language>en-us</language><lastBuildDate>Mon, 09 Jun 2025 13:00:00 +0530</lastBuildDate><atom:link href="https://pjainish.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Incorporating Ads into Large Language Models: The Hidden Economy of AI Responses</title><link>https://pjainish.github.io/posts/incorporating-ads-into-llms/</link><pubDate>Mon, 09 Jun 2025 13:00:00 +0530</pubDate><guid>https://pjainish.github.io/posts/incorporating-ads-into-llms/</guid><description>Discover how in-response advertising unlocks a hidden AI revenue stream - balancing seamless brand integration with user trust and privacy.</description></item><item><title>Multi-Stage Approach to Building Recommender Systems</title><link>https://pjainish.github.io/posts/multi-stage-recommender-systems/</link><pubDate>Tue, 03 Jun 2025 13:48:45 +0530</pubDate><guid>https://pjainish.github.io/posts/multi-stage-recommender-systems/</guid><description>&lt;p>Multi-stage recommendation systems break down the challenging task of matching users with relevant items into several sequential phases, each optimizing for different objectives like efficiency, accuracy, and personalization. By progressively narrowing down a vast pool of candidates, applying increasingly complex models, and refining final rankings, these systems achieve scalable and high-quality recommendations even when dealing with billions of users and items (&lt;a href="https://www.ijcai.org/proceedings/2022/0771.pdf" title="Neural Re-ranking in Multi-stage Recommender Systems: A Review - IJCAI">ijcai.org&lt;/a>, &lt;a href="https://developers.google.com/machine-learning/recommendation/overview/types" title="Recommendation systems overview | Machine Learning - Google Developers">developers.google.com&lt;/a>). They mirror how humans might sift through information: first skimming broadly, then considering details, and finally fine-tuning choices. This blog post explores the conceptual foundations of multi-stage recommendation, the distinct roles of each phase, the motivations behind layered architectures, and the real-world trade-offs they address. Along the way, analogies to everyday decision-making, historical parallels from human learning, and references to psychology illustrate how designers balance speed, relevance, and diversity. Finally, we survey challenges such as latency constraints, fairness, and the evolution toward neural re-ranking and hybrid objectives, pointing curious readers to key research papers and practical guides for deeper study.&lt;/p></description></item><item><title>Improving Search Relevance Using Large Language Models</title><link>https://pjainish.github.io/posts/improving-search-relevance-using-large-language-models/</link><pubDate>Sat, 03 May 2025 13:48:45 +0530</pubDate><guid>https://pjainish.github.io/posts/improving-search-relevance-using-large-language-models/</guid><description>&lt;p>Search is the invisible backbone of our digital lives. Every time you type a query into Google, search through Netflix&amp;rsquo;s catalog, or hunt for a specific product on Amazon, you&amp;rsquo;re interacting with systems designed to understand what you really want - not just what you literally typed. But here&amp;rsquo;s the thing: traditional search has always been a bit like playing telephone with a robot that only speaks in keywords.&lt;/p>
&lt;p>Large Language Models are changing this game entirely. They&amp;rsquo;re teaching search systems to understand language the way humans do - with context, nuance, and genuine comprehension. The transformation is so profound that we&amp;rsquo;re witnessing the biggest shift in information retrieval since the invention of the web crawler. Let me show you how this revolution works and why it&amp;rsquo;s reshaping everything from how we shop to how we discover knowledge.&lt;/p></description></item><item><title>BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers</title><link>https://pjainish.github.io/posts/bert4rec-sequential-recommendation/</link><pubDate>Fri, 03 Jan 2025 17:23:15 +0530</pubDate><guid>https://pjainish.github.io/posts/bert4rec-sequential-recommendation/</guid><description>&lt;p>BERT4Rec is a sequential recommendation model that leverages the bidirectional Transformer architecture, originally designed for language tasks, to capture users’ evolving preferences by jointly considering both past and future items in a sequence (&lt;a href="https://arxiv.org/abs/1904.06690" title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org&lt;/a>, &lt;a href="https://github.com/FeiSun/BERT4Rec" title="GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...">github.com&lt;/a>). Unlike earlier unidirectional models that predict the next item only from previous ones, BERT4Rec uses a Cloze-style masking objective to predict missing items anywhere in the sequence, enabling richer context modeling (&lt;a href="https://arxiv.org/abs/1904.06690" title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org&lt;/a>, &lt;a href="https://github.com/FeiSun/BERT4Rec" title="GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...">github.com&lt;/a>). Empirical evaluations on multiple benchmark datasets demonstrate that BERT4Rec often surpasses state-of-the-art sequential models like SASRec, though its performance can depend on careful training schedules and hyperparameter choices (&lt;a href="https://arxiv.org/abs/2207.07483" title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org&lt;/a>, &lt;a href="https://arxiv.org/abs/2308.07192" title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org&lt;/a>). This post traces the journey from early recommendation methods to the Transformer revolution and the rise of BERT, explains the core ideas behind BERT4Rec, connects them to cognitive analogies of Cloze tests, and discusses experiments, limitations, and future directions. By understanding BERT4Rec’s design and its place in the broader landscape of recommendation, readers can appreciate both its technical elegance and its conceptual roots in language modeling and human learning.&lt;/p></description></item><item><title>Films</title><link>https://pjainish.github.io/posts/films/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pjainish.github.io/posts/films/</guid><description>&lt;h3 id="movies-i-have-watched-and-loved--not-in-order">Movies I have watched and Loved !!! Not in order.&lt;/h3>
&lt;blockquote>
&lt;p>We live in a box of space and time. Movies are windows in its walls.&lt;/p>
&lt;/blockquote>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">&lt;/th>
&lt;th style="text-align: left">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">&lt;img alt="image" loading="lazy" src="https://image.tmdb.org/t/p/w185/uS9m8OBk1A8eM9I042bx8XXpqAq.jpg">&lt;/td>
&lt;td style="text-align: left">&lt;strong>The Silence of the Lambs&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;img alt="image" loading="lazy" src="https://image.tmdb.org/t/p/w185/3bhkrj58Vtu7enYsRolD1fZdja1.jpg">&lt;/td>
&lt;td style="text-align: left">&lt;strong>The Godfather&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;img alt="image" loading="lazy" src="https://image.tmdb.org/t/p/w185/vQWk5YBFWF4bZaofAbv0tShwBvQ.jpg">&lt;/td>
&lt;td style="text-align: left">&lt;strong>Pulp Fiction&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;img alt="image" loading="lazy" src="https://image.tmdb.org/t/p/w185/a07wLy4ONfpsjnBqMwhlWTJTcm.jpg">&lt;/td>
&lt;td style="text-align: left">&lt;strong>Troy&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;img alt="image" loading="lazy" src="https://image.tmdb.org/t/p/w185/cPB3ZMM4UdsSAhNdS4c7ps5nypY.jpg">&lt;/td>
&lt;td style="text-align: left">&lt;strong>The Terminal&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;img alt="image" loading="lazy" src="https://image.tmdb.org/t/p/w185/2hFvxCCWrTmCYwfy7yum0GKRi3Y.jpg">&lt;/td>
&lt;td style="text-align: left">&lt;strong>The Pianist&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;img alt="image" loading="lazy" src="https://image.tmdb.org/t/p/w185/gEU2QniE6E77NI6lCU6MxlNBvIx.jpg">&lt;/td>
&lt;td style="text-align: left">&lt;strong>Interstellar&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;img alt="image" loading="lazy" src="https://image.tmdb.org/t/p/w185/x2FJsf1ElAgr63Y3PNPtJrcmpoe.jpg">&lt;/td>
&lt;td style="text-align: left">&lt;strong>Arrival&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">&lt;img alt="image" loading="lazy" src="https://image.tmdb.org/t/p/w185/7IiTTgloJzvGI1TAYymCfbfl3vT.jpg">&lt;/td>
&lt;td style="text-align: left">&lt;strong>Parasite&lt;/strong>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item></channel></rss>