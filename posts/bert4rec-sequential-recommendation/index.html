<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers | Jainish's Log</title>
<meta name=keywords content><meta name=description content="BERT4Rec is a sequential recommendation model that leverages the bidirectional Transformer architecture, originally designed for language tasks, to capture users’ evolving preferences by jointly considering both past and future items in a sequence (arxiv.org, github.com). Unlike earlier unidirectional models that predict the next item only from previous ones, BERT4Rec uses a Cloze-style masking objective to predict missing items anywhere in the sequence, enabling richer context modeling (arxiv.org, github.com). Empirical evaluations on multiple benchmark datasets demonstrate that BERT4Rec often surpasses state-of-the-art sequential models like SASRec, though its performance can depend on careful training schedules and hyperparameter choices (arxiv.org, arxiv.org). This post traces the journey from early recommendation methods to the Transformer revolution and the rise of BERT, explains the core ideas behind BERT4Rec, connects them to cognitive analogies of Cloze tests, and discusses experiments, limitations, and future directions. By understanding BERT4Rec’s design and its place in the broader landscape of recommendation, readers can appreciate both its technical elegance and its conceptual roots in language modeling and human learning."><meta name=author content="Jainish Patel"><link rel=canonical href=https://pjainish.github.io/posts/bert4rec-sequential-recommendation/><link crossorigin=anonymous href=/assets/css/stylesheet.163b6e82d7f85e271528a3b1c49ce5baa5282a84c7fb459244677fa271eccbc2.css rel="preload stylesheet" as=style><link rel=icon href=https://pjainish.github.io/assets/images/favicon.png><link rel=icon type=image/png sizes=16x16 href=https://pjainish.github.io/assets/images/favicon.png><link rel=icon type=image/png sizes=32x32 href=https://pjainish.github.io/assets/images/favicon.png><link rel=apple-touch-icon href=https://pjainish.github.io/assets/images/favicon.png><link rel=mask-icon href=https://pjainish.github.io/assets/images/favicon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://pjainish.github.io/posts/bert4rec-sequential-recommendation/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-79V8YMLKHG"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-79V8YMLKHG")</script><meta property="og:url" content="https://pjainish.github.io/posts/bert4rec-sequential-recommendation/"><meta property="og:site_name" content="Jainish's Log"><meta property="og:title" content="BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers"><meta property="og:description" content="BERT4Rec is a sequential recommendation model that leverages the bidirectional Transformer architecture, originally designed for language tasks, to capture users’ evolving preferences by jointly considering both past and future items in a sequence (arxiv.org, github.com). Unlike earlier unidirectional models that predict the next item only from previous ones, BERT4Rec uses a Cloze-style masking objective to predict missing items anywhere in the sequence, enabling richer context modeling (arxiv.org, github.com). Empirical evaluations on multiple benchmark datasets demonstrate that BERT4Rec often surpasses state-of-the-art sequential models like SASRec, though its performance can depend on careful training schedules and hyperparameter choices (arxiv.org, arxiv.org). This post traces the journey from early recommendation methods to the Transformer revolution and the rise of BERT, explains the core ideas behind BERT4Rec, connects them to cognitive analogies of Cloze tests, and discusses experiments, limitations, and future directions. By understanding BERT4Rec’s design and its place in the broader landscape of recommendation, readers can appreciate both its technical elegance and its conceptual roots in language modeling and human learning."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-03T17:23:15+05:30"><meta property="article:modified_time" content="2025-01-03T17:23:15+05:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers"><meta name=twitter:description content="BERT4Rec is a sequential recommendation model that leverages the bidirectional Transformer architecture, originally designed for language tasks, to capture users’ evolving preferences by jointly considering both past and future items in a sequence (arxiv.org, github.com). Unlike earlier unidirectional models that predict the next item only from previous ones, BERT4Rec uses a Cloze-style masking objective to predict missing items anywhere in the sequence, enabling richer context modeling (arxiv.org, github.com). Empirical evaluations on multiple benchmark datasets demonstrate that BERT4Rec often surpasses state-of-the-art sequential models like SASRec, though its performance can depend on careful training schedules and hyperparameter choices (arxiv.org, arxiv.org). This post traces the journey from early recommendation methods to the Transformer revolution and the rise of BERT, explains the core ideas behind BERT4Rec, connects them to cognitive analogies of Cloze tests, and discusses experiments, limitations, and future directions. By understanding BERT4Rec’s design and its place in the broader landscape of recommendation, readers can appreciate both its technical elegance and its conceptual roots in language modeling and human learning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://pjainish.github.io/posts/"},{"@type":"ListItem","position":2,"name":"BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers","item":"https://pjainish.github.io/posts/bert4rec-sequential-recommendation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers","name":"BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers","description":"BERT4Rec is a sequential recommendation model that leverages the bidirectional Transformer architecture, originally designed for language tasks, to capture users’ evolving preferences by jointly considering both past and future items in a sequence (arxiv.org, github.com). Unlike earlier unidirectional models that predict the next item only from previous ones, BERT4Rec uses a Cloze-style masking objective to predict missing items anywhere in the sequence, enabling richer context modeling (arxiv.org, github.com). Empirical evaluations on multiple benchmark datasets demonstrate that BERT4Rec often surpasses state-of-the-art sequential models like SASRec, though its performance can depend on careful training schedules and hyperparameter choices (arxiv.org, arxiv.org). This post traces the journey from early recommendation methods to the Transformer revolution and the rise of BERT, explains the core ideas behind BERT4Rec, connects them to cognitive analogies of Cloze tests, and discusses experiments, limitations, and future directions. By understanding BERT4Rec’s design and its place in the broader landscape of recommendation, readers can appreciate both its technical elegance and its conceptual roots in language modeling and human learning.\n","keywords":[],"articleBody":"BERT4Rec is a sequential recommendation model that leverages the bidirectional Transformer architecture, originally designed for language tasks, to capture users’ evolving preferences by jointly considering both past and future items in a sequence (arxiv.org, github.com). Unlike earlier unidirectional models that predict the next item only from previous ones, BERT4Rec uses a Cloze-style masking objective to predict missing items anywhere in the sequence, enabling richer context modeling (arxiv.org, github.com). Empirical evaluations on multiple benchmark datasets demonstrate that BERT4Rec often surpasses state-of-the-art sequential models like SASRec, though its performance can depend on careful training schedules and hyperparameter choices (arxiv.org, arxiv.org). This post traces the journey from early recommendation methods to the Transformer revolution and the rise of BERT, explains the core ideas behind BERT4Rec, connects them to cognitive analogies of Cloze tests, and discusses experiments, limitations, and future directions. By understanding BERT4Rec’s design and its place in the broader landscape of recommendation, readers can appreciate both its technical elegance and its conceptual roots in language modeling and human learning.\nIntroduction: A Learning Journey I still remember the first time I tried to build a recommendation system. It was during my undergraduate years, and I wanted to create a small app that suggested books to my friends based on what they had read before. At that time, I naively believed that simply counting co-occurrences of books would be enough. I soon realized that user preferences change over time, and static co-occurrence matrices felt too rigid. That curiosity led me to explore sequential recommendation—models that treat a user’s history as an evolving narrative rather than a single static snapshot.\nFast forward a few years, and I found myself diving into deep learning approaches for recommendation during my PhD. Each step felt like peeling another layer of understanding: starting with simple Markov chains, moving to recurrent neural networks, then witnessing the Transformer revolution in natural language processing (NLP) with papers like “Attention Is All You Need” (arxiv.org, papers.nips.cc) and “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” (arxiv.org, aclanthology.org). In language tasks, these models treated sentences as dynamic sequences of words; in recommendation, sequences of items could be handled similarly.\nJust as the alphabet and grammar form the foundation of language, the sequence of user interactions—clicks, views, purchases—forms the grammar of recommendation. When I first encountered BERT4Rec, I saw a bridge between these worlds: a model designed for language Cloze tasks, applied to sequences of items. In this post, I want to share that journey—why the shift from unidirectional to bidirectional models matters, how the Cloze objective parallels human tests, the design choices behind BERT4Rec, and what we can learn both technically and conceptually from it. My hope is that, by the end, you’ll see BERT4Rec not just as another state-of-the-art model, but as part of a broader narrative connecting human cognition, language, and recommendation.\nBackground: From Static to Sequential Recommendation The Early Days: Collaborative Filtering and Beyond Recommender systems began with collaborative filtering approaches that treat users and items symmetrically, often using matrix factorization to uncover latent factors (link.springer.com). These methods assume static preferences: a user has fixed tastes, and items have fixed attributes. For example, if Alice liked “The Hobbit” and “The Lord of the Rings,” a static model would continue recommending similar fantasy books without considering that she might have grown more interested in science fiction recently.\nPsychologically, this is akin to assuming that a person’s personality never changes—an oversimplification. In reality, tastes evolve. Just as our moods and interests shift from week to week, user interactions in an online setting reflect changing preferences. Recognizing this, researchers started looking at temporal dynamics: assigning more weight to recent interactions (link.springer.com, arxiv.org). However, these adjustments were often heuristic rather than deeply integrated into the model’s structure.\nSequential Recommendation: Capturing the Flow To better model evolving preferences, sequential recommendation treats a user’s history as an ordered list of events. Two main families of approaches emerged:\nMarkov Chain-based Models: These assume that the next action depends on a limited window of previous actions, often just the last one or two (cseweb.ucsd.edu). While simple and effective in sparse settings, they struggle to capture longer-term patterns. It’s like predicting the next word in a sentence by looking only at the immediately preceding word—sometimes okay, but often missing broader context.\nRecurrent Neural Networks (RNNs): With the rise of deep learning, RNNs (e.g., GRU4Rec) became popular for sequential recommendation tasks. They process one item at a time, updating a hidden state that summarizes the history (link.springer.com, arxiv.org). While theoretically capable of capturing long-range dependencies, RNNs can suffer from vanishing gradients and can be slow to train, especially when sequences get long.\nThese methods moved beyond static views of users, but they still relied on unidirectional modeling: either Markov chains always look backward a fixed number of steps, and RNNs process sequences from left (oldest) to right (newest). In human terms, it’s like reading a story only forward—never knowing how the ending influences the interpretation of earlier chapters.\nSelf-Attention and SASRec: A Step Towards Flexible Context In August 2018, Kang and McAuley introduced SASRec, a self-attentive sequential model that borrowed ideas from the Transformer’s self-attention mechanism to balance long-term and short-term context (arxiv.org, cseweb.ucsd.edu). Instead of processing item sequences strictly left-to-right, SASRec computes attention weights over all previous items at each step, allowing the model to focus on the most relevant past actions when predicting the next one (arxiv.org, arxiv.org). Mechanically, it applies multi-head self-attention layers over item embeddings, followed by pointwise feed-forward layers, similar to each encoder block in the original Transformer (arxiv.org, export.arxiv.org).\nSASRec offered two main advantages:\nEfficiency: By parallelizing self-attention computations across positions, SASRec can be trained faster than RNN-based models on modern hardware. Adaptive Context: Attention weights allow the model to decide which past items matter most, rather than forcing it to use a fixed window or hidden state sequence. However, SASRec remains unidirectional in its attention: at each time step, it only attends to items that come before that position. This means it still cannot consider potential “future” items, even if they would be known at test time when scoring multiple candidate items. In language terms, it’s like understanding a sentence by reading it left to right—never knowing what words come later in the sentence.\nThe Transformer Revolution: Background and Impact The Birth of the Transformer (Vaswani et al., 2017) In June 2017, Vaswani et al. published “Attention Is All You Need,” a paper that fundamentally changed NLP and sequence modeling (arxiv.org, papers.nips.cc). They introduced the Transformer, which replaced recurrence with multi-head self-attention and simple feed-forward networks. The key insights were:\nSelf-Attention Layers: These compute weighted sums of all positions’ embeddings for each position, allowing direct modeling of pairwise dependencies regardless of distance. Positional Encoding: Since attention layers by themselves lack inherent order, they added sinusoidal positional encodings to inject sequence information. Parallelization: Unlike RNNs, Transformers can process all positions in parallel, making training significantly faster on GPUs. By discarding recurrence and convolutions, the Transformer demonstrated state-of-the-art performance on machine translation tasks, achieving BLEU scores surpassing previous best models on WMT English-German and English-French benchmarks (arxiv.org, scispace.com). This architecture quickly became the de facto backbone for a wide range of NLP tasks, from translation to summarization to question answering (huggingface.co, arxiv.org).\nAnalogy: Before Transformers, sequence models were like cars with only one speed—reverse (recurrence) or forward (convolutions/attention with constraints). Transformers were like multi-gear vehicles that could shift seamlessly, giving models flexibility to access information anywhere in the sequence, much like looking up any chapter in a book instantly rather than reading every page sequentially.\nBERT: Deep Bidirectional Language Representation (Devlin et al., 2018) Building on the Transformer’s encoder, Devlin et al. introduced BERT (Bidirectional Encoder Representations from Transformers) in October 2018 (arxiv.org, aclanthology.org). BERT’s main contributions were:\nBidirectional Context: By jointly attending to both left and right context in all layers (rather than only attending to previous tokens), BERT can learn richer representations. Masked Language Modeling (MLM): To enable bidirectionality, they used a Cloze-like task: randomly mask some tokens in the input and train the model to predict them based on surrounding context. Next Sentence Prediction (NSP): As a secondary task, BERT predicts whether two sentences follow each other, helping capture inter-sentence relationships. BERT was pre-trained on massive corpora (BooksCorpus and English Wikipedia), achieving state-of-the-art results across a variety of NLP benchmarks, such as GLUE, SQuAD, and others (arxiv.org, export.arxiv.org). Its bidirectional design unlocked new capabilities: while unidirectional language models (e.g., OpenAI GPT) process text left-to-right, BERT’s MLM allowed it to encode context from both sides, akin to reading a sentence and filling in missing words anywhere in it.\nAnalogy: Imagine reading a paragraph with some words hidden and having to guess them using the rest of the paragraph. This Cloze-style task is exactly how BERT learns. In human tests, teachers often use fill-in-the-blank exercises to gauge comprehension—similarly, BERT’s MLM forces the model to deeply understand context.\nThe impact of BERT extended beyond NLP. Researchers began to ask: if bidirectional Transformers can learn from masked words in a sentence, could a similar idea work for sequences of user interactions? Enter BERT4Rec.\nBERT4Rec: Core Ideas and Design Motivation: Why Bidirectional Modeling Matters In sequential recommendation, we often care about predicting the next item given past history. Unidirectional models like SASRec attend only to prior items when making a prediction (arxiv.org, cseweb.ucsd.edu). However, at evaluation or inference time, we typically score multiple candidate items to find the most likely next item. Those candidates can be seen as “future” items once we inject them into the sequence. If the model can attend to both past items and the candidate item itself (as if it were masked during training), it can form a richer representation that uses information from the full sequence context.\nBERT4Rec reframes sequential recommendation as a Cloze task: randomly mask items in the user’s history and train the model to predict them based on both left and right context, which may include items that occur after them in the sequence (arxiv.org, github.com). This bidirectional conditioning helps the model learn how items co-occur in different parts of the sequence, not just in a strict left-to-right chain.\nAnalogy: In a detective novel, clues about who committed the crime may appear early and later in the story. A unidirectional reader would only use clues from the beginning up to the current chapter. A bidirectional reader, knowing the ending, can reinterpret earlier clues in light of later revelations. Similarly, BERT4Rec’s bidirectional attention allows the model to reinterpret earlier interactions when considering missing items.\nArchitecture Overview At a high level, BERT4Rec follows the encoder architecture from the original Transformer with two major changes:\nCloze-style Masking: A certain percentage of items in a user’s sequence are randomly masked (replaced with a special [MASK] token). The model’s task is to predict the identity of each masked item using bidirectional attention over the unmasked items (arxiv.org, researchgate.net). Item Embeddings with Positional Encodings: Each item in the sequence is mapped to a learned embedding. Since the Transformer has no inherent sense of order, sinusoidal or learned positional encodings are added to each item embedding to encode its position in the sequence (arxiv.org, ar5iv.labs.arxiv.org). Concretely:\nInput: A user history of length n (e.g., [i₁, i₂, …, iₙ]). We randomly choose a subset of positions (usually 15%) and replace them with [MASK] tokens. For example, if the original sequence is [A, B, C, D, E] and positions 2 and 4 are masked, the input becomes [A, [MASK], C, [MASK], E]. Embedding Layer: Each position t has an embedding E_item(iₜ) (for item iₜ) plus a positional embedding E_pos(t). So, the initial input to the Transformer is the sum E_item + E_pos for each position, with masked positions using a special mask embedding. Transformer Encoder Stack: Typically 2 to 4 layers (depending on hyperparameters) of multi-head self-attention and feed-forward layers. Since we want bidirectional context, the self-attention is “full” (not masked), allowing each position to attend to all other positions in the sequence. Output Heads: For each masked position, the final hidden state vector is passed through a linear projection followed by a softmax over the item vocabulary to predict which item was masked. Loss Function: Cross-entropy loss is computed only over the masked positions, summing (or averaging) across them. During inference, to predict the next item, one can append a [MASK] token to the end of a user’s sequence and feed it through the model. The model’s output distribution at that position indicates the probabilities of all possible items being the next interaction.\nTechnical Note: Because BERT4Rec conditions on bidirectional context, it avoids what is known as “exposure bias” often found in left-to-right models, where during training the model sees only ground-truth history, but during inference it must rely on its own predictions. BERT4Rec’s Cloze objective alleviates this by mixing masked ground truth with unmasked items, making the model robust to masked or unknown future items.\nTraining as a Cloze Task: Deeper Explanation The term Cloze comes from psycholinguistics and educational testing: learners fill in missing words in a text passage (arxiv.org, kdnuggets.com). This is not a new idea. In fact, BERT borrowed it directly from earlier NLP work, such as the Cloze tests used by educators to measure student comprehension (kdnuggets.com). In the context of recommendation:\nMasked Item Prediction (MIP): Analogous to masked language modeling (MLM) in BERT, BERT4Rec’s MIP randomly selects a subset of positions in a user’s interaction sequence, hides each item, and asks the model to fill it in based on both past and future interactions. Sampling Strategy: Typically, 15% of items are chosen for masking. Of those, 80% are replaced with [MASK], 10% with a random item (to encourage robustness), and 10% are left unchanged but still counted in the loss as if they were masked (to mitigate training/test mismatch) (arxiv.org, github.com). Advantages: By predicting items anywhere in the sequence, the model learns co-occurrence patterns in all contexts, not just predicting the next item. This generates more training samples per sequence (since each masked position is a training example), potentially improving data efficiency (arxiv.org, arxiv.org). Analogy: When learning a language, filling in blank words anywhere in a paragraph helps both reading comprehension and vocabulary acquisition. Similarly, by practicing predicting missing items anywhere in their history, the model builds a more flexible representation of user preferences.\nComparison with Unidirectional Models (e.g., SASRec) Context Scope\nUnidirectional (SASRec): At position t, the model attends only to items 1 through t–1. Bidirectional (BERT4Rec): At each masked position t, the model attends to all items except those that are also masked. When predicting the next item (by placing a [MASK] at n+1), it attends to items 1 through n and vice versa for other masked positions. Training Objective\nUnidirectional: Usually uses next-item prediction with cross-entropy loss at each time step. Bidirectional: Uses Cloze objective, predicting multiple masked positions per sequence. Data Efficiency\nUnidirectional: Generates one training sample per time step (predict next item). Bidirectional: Generates as many training samples as there are masked positions (typically ~15% of sequence length), often leading to more gradient updates per sequence. Inference\nUnidirectional: Directly predicts the next item based on history. Bidirectional: Appends a [MASK] to the end to predict next item, or can mask any position for in-sequence imputation. Several empirical studies have shown that BERT4Rec often outperforms SASRec, especially when long-range dependencies are important (arxiv.org, arxiv.org). However, this performance advantage can require longer training times and careful hyperparameter tuning, as later work has pointed out (arxiv.org, arxiv.org).\nDrawing Analogies: Cloze Tests, Human Learning, and Recommendation The Psychology of Masked Tests Cloze tests, introduced by W. L. Taylor in 1953, are exercises where learners fill in blanks in a passage of text, gauging language comprehension and vocabulary knowledge (kdnuggets.com). Educational psychologists have found that Cloze tasks encourage active recall and semantic inference, as learners must use both local and global context to guess missing words correctly. Similarly, BERT’s MLM and BERT4Rec’s MIP require the model to infer missing tokens (words or items) from all available context, reinforcing rich contextual understanding.\nIn human terms:\nLocal Context: To guess a masked word in a sentence, you use nearby words. Global Context: Often, clues spread across the paragraph or entire document guide you toward the right answer. BERT4Rec’s masked items play the role of blank spaces in a text. The model, like a student in a Cloze test, must use all known interactions (both before and after the blank) to infer the missing preference. This leads to representations that capture not only pairwise item relationships but also how items co-occur across entire sessions.\nHistorical Perspective: From Prediction to Comprehension Early recommendation models focused on prediction: given past clicks, what happens next? This is analogous to a fill-in-the-blank exercise where only the next word is blank. In mathematics, this is like knowing all terms of a sequence except the next one and trying to guess it from a recurrence relation. But modern language teaching emphasizes comprehension, teaching students to understand entire texts, not just predict the next word. BERT4Rec embodies that shift: from predicting sequentially to understanding a user’s entire session.\nConsider reading Hamlet: if you only focus on predicting the next line, you might miss the broader themes. If you think about themes and motifs across the play, you get a richer understanding. BERT4Rec, by predicting masked items anywhere, learns themes and motifs in interaction sequences as well.\nReal-World Analogy: Playlist Shuffling Imagine you’re curating a playlist of songs you’ll listen to on a road trip. Instead of putting them in a fixed order (e.g., chronological from your latest favorites), you shuffle them but still want the transitions to feel coherent. A unidirectional model ensures each song transitions well from the previous one, like ensuring each next word makes sense after the last. A bidirectional approach would allow you to also consider the song that comes after when choosing a song for a particular slot, creating smooth transitions both forward and backward. In BERT4Rec, masked songs correspond to shuffled or missing approximate transitions, and the model learns what fits best given both neighbors.\nTechnical Deep Dive: BERT4Rec’s Mechanics Input Representation Given a user’s historical sequence of item interactions $i₁, i₂, …, iₙ$, BERT4Rec prepares inputs as follows (arxiv.org, researchgate.net):\nMasking Strategy\nRandomly select 15% of positions for masking.\nOf those positions:\n80% are replaced with [MASK]. 10% are replaced with a random item ID from the vocabulary (to encourage robustness). 10% remain unchanged (but are still counted in the loss). This strategy mirrors BERT’s design to prevent the model from relying too heavily on the [MASK] token (arxiv.org, export.arxiv.org). Item Embeddings\nEach item ID has a learned embedding vector of dimension d. A special embedding E_mask is used for [MASK] tokens. Positional Embeddings\nSince the Transformer has no notion of sequence order, add a learned positional embedding E_pos(t) for each position t ∈ {1,…,n}. The sum E_item(iₜ) + E_pos(t) forms the input embedding at position t. Sequence Length and Padding\nFor computational efficiency, fix a maximum sequence length L (e.g., 200). If a user’s history has fewer than L interactions, pad the sequence with [PAD] tokens at the front or back. [PAD] tokens have embeddings but are ignored in attention computations (i.e., their attention weights are set to zero). Embedding Dropout\nOptional dropout can be applied to the sum of item and positional embeddings to regularize training. Mathematically, let\n$$ xₜ = E_{item}(iₜ) + E_{pos}(t), \\quad t = 1,\\dots,n. $$\nMasked positions use\n$$ xₜ = E_{mask} + E_{pos}(t). $$\nTransformer Encoder Stack BERT4Rec typically uses a stack of N encoder layers (e.g., N = 2 or 3 for smaller datasets, up to N = 6 for larger ones), each consisting of:\nMulti-Head Self-Attention\nFor layer l, each position t has queries, keys, and values computed as linear projections of the input from the previous layer.\nAttention weights are computed as scaled dot products between queries and keys, followed by softmax.\nWeighted sums of values produce the attention output for each head.\nThe outputs of all heads are concatenated and linearly projected back to dimension d.\nResidual connection and layer normalization are applied:\n$$ \\text{SA}_l(X) = \\text{LayerNorm}(X + \\text{MultiHeadAttn}(X)). $$\nPosition-Wise Feed-Forward Network\nA two-layer feed-forward network with a GELU or ReLU activation:\n$$ \\text{FFN}_l(Y) = \\text{LayerNorm}(Y + W₂ ,\\phi(W₁ Y + b₁) + b₂), $$\nwhere $\\phi$ is an activation (often GELU).\nLayerNorm and Residual Connections\nAs in the original Transformer, each sub-layer has a residual (skip) connection followed by layer normalization, ensuring stable training and gradient flow (arxiv.org, scispace.com). Because the self-attention is full (no masking of future positions), each position’s representation at each layer can incorporate information from any other unmasked position in the sequence.\nOutput and Loss Computation After N encoder layers, we obtain final hidden representations ${h₁, h₂, \\dots, hₙ}$ ∈ ℝ^{n×d}. For each position t that was masked during input preparation, we compute:\nItem Prediction Scores\n$$ sₜ = W_{output} , hₜ + b_{output}, \\quad sₜ ∈ ℝ^{|V|}, $$\nwhere |V| is the size of the item vocabulary, and $W_{output} ∈ ℝ^{|V|×d}$.\nSoftmax and Cross-Entropy Loss\nApply softmax to $sₜ$ to get predicted probability distribution $\\hat{y}_t$.\nIf the true item ID at position t is $iₜ^*$, the cross-entropy loss for that position is:\n$$ \\mathcal{L}t = -\\log\\bigl(\\hat{y}{t}[ iₜ^* ]\\bigr). $$\nAggregate loss across all masked positions in the batch, typically averaging over them:\n$$ \\mathcal{L} = \\frac{1}{\\sum_t mₜ} \\sum_{t=1}^n mₜ , \\mathcal{L}_t, $$\nwhere $mₜ = 1$ if position t was masked, else 0.\nBecause multiple positions are masked per sequence, each training example yields several prediction targets, improving data efficiency.\nInference: Predicting the Next Item To recommend the next item for a user:\nExtend the Sequence\nGiven the user’s last n interactions, append a [MASK] token at position n+1 (if n+1 ≤ L). If n = L, one could remove the oldest item or use sliding window techniques. Feed Through Model\nThe [MASK] at position n+1 participates in bidirectional attention, attending to all positions 1 through n. Conversely, positions 1 through n attend to the [MASK] if full self‐attention is used. Obtain Scores\nCompute $s_{n+1} ∈ ℝ^{|V|}$ from the final hidden state $h_{n+1}$. The highest-scoring items in $s_{n+1}$ are the top-K recommendations. Because BERT4Rec’s training objective was to predict masked items given both left and right context, placing the [MASK] at the end simulates one masked position with only left context. While strictly speaking this isn’t bidirectional (the [MASK] at the end has no right context), it still benefits from richer item co-occurrence patterns learned during training. Empirically, this approach yields strong next-item recommendation accuracy.\nExperimental Results and Analysis Datasets and Evaluation Protocols In the original BERT4Rec paper, Sun et al. evaluated the model on four public benchmark datasets:\nMovieLens-1M (ML-1M): 1 million ratings from ~6000 users on ~3900 movies. YooChoose: Click logs from the RecSys Challenge 2015, with ~8.6 million events. Steam: Game purchase and play logs from the Steam platform. Amazon Beauty: Reviews and ratings in the beauty product category from the Amazon Reviews dataset. For each user, interactions were chronologically ordered. The last interaction was used as the test item, the second last as validation, and earlier interactions for training. Performance metrics included Hit Rate (HR@K) and Normalized Discounted Cumulative Gain (NDCG@K) at various cut-offs (e.g., K = 5, 10) (arxiv.org, arxiv.org).\nBaselines Compared Sun et al. compared BERT4Rec against several state-of-the-art sequential recommendation methods:\nGRU4Rec: RNN (GRU) based model with pairwise ranking loss. Casual Convolutional (CasualConv): Convolutional neural network model for sequences. SASRec: Self-attention based unidirectional model. Caser: Convolutional sequence embedding model (vertical + horizontal convolution). NextItNet: Dilated residual network for sequential recommendation. Key Findings BERT4Rec vs. SASRec\nAcross ML-1M and YooChoose, BERT4Rec improved HR@10 by ≈2–3% and NDCG@10 by ≈1–2% relative to SASRec (arxiv.org, arxiv.org). On sparser datasets like Steam, the advantage increased, indicating that bidirectional context can better handle data sparsity by leveraging co-occurrence patterns across entire sessions. Model Depth and Hidden Size\nDeeper (more layers) or wider (larger d) BERT4Rec variants performed better on large datasets but risked overfitting on smaller ones. Typical configurations: 2 layers, hidden size 64 for ML-1M; 3–4 layers for larger datasets. Masking Ratio\nMasking ~15% of items per sequence yielded a good trade-off. Masking too many positions reduced signal per position; masking too few yielded fewer training samples. Training Time\nBERT4Rec required more compute than SASRec due to larger parameter counts and Cloze objective. Subsequent research (Petrov \u0026 Macdonald, 2022) noted that default training schedules in the original implementations were too short to fully converge on some datasets; when trained longer, BERT4Rec’s performance became more consistent (arxiv.org, arxiv.org). Replicability and Training Considerations Petrov and Macdonald (2022) conducted a systematic review and replicability study of BERT4Rec, finding:\nTraining Time Sensitivity: Default hyperparameters often led to under-trained models. Training 10–30× longer was sometimes necessary to reproduce reported results (arxiv.org, arxiv.org). Batch Size and Learning Rates: Smaller batch sizes with warm-up steps and linear decay of learning rates yielded more stable convergence. Alternative Architectures: Implementations using Hugging Face’s Transformers library, incorporating variants like DeBERTa’s disentangled attention, matched or exceeded original results with significantly less training time (arxiv.org, arxiv.org). Another study by Petrov \u0026 Macdonald (2023) introduced gSASRec, which showed that SASRec could outperform BERT4Rec when properly addressing overconfidence arising from negative sampling (arxiv.org). They argued that BERT4Rec’s bidirectional mechanism alone did not guarantee superiority; rather, loss formulations and training strategies play a crucial role.\nComparative Strengths and Weaknesses Strengths\nRich Context Modeling: By conditioning on both sides of a position, BERT4Rec captures intricate co-occurrence patterns. Data Efficiency: Masked positions generate more supervision signals per sequence. Flexibility: Can predict items at arbitrary positions, enabling applications like sequential imputation or session completion beyond next-item recommendation. Weaknesses\nCompute and Memory: More parameters and bidirectional attention make it more expensive in both training and inference compared to unidirectional models. Training Sensitivity: Requires careful hyperparameter tuning and longer training times to reach optimal performance. Inference Unidirectionality for Next-Item: Although trained bidirectionally, predicting the next item requires inserting a [MASK] with no right context, effectively making inference unidirectional, possibly leaving some benefits unused. Conceptual Insights: Why BERT4Rec Works Learning Co-Occurrence vs. Sequential Order Unlike unidirectional models that focus on ordering—item t predicts item t+1—BERT4Rec learns from co-occurrence patterns across sessions:\nItems A and B that consistently appear together in sessions might have high mutual information. If A often precedes B and also often follows B, unidirectional models only see one direction; BERT4Rec sees both, learning a symmetric association. In recommendation, co-occurrence is often more informative than strict ordering. For example, if many users watch “The Matrix” and “Inception” in any order, a bidirectional model picks up that association, regardless of which came first.\nOvercoming Exposure Bias Unidirectional models train to predict the next item given ground-truth history. During inference, they must use predicted items (or no items) to form history, leading to exposure bias—errors compound as the model has never seen its own mistakes. In contrast, BERT4Rec’s masking randomly hides items during training, exposing the model to situations where parts of the sequence are unknown, resulting in more robust representations when some interactions are missing or noisy (arxiv.org, arxiv.org).\nAnalogous to Autoencoders BERT4Rec’s training resembles an autoencoder: it corrupts (masks) parts of the input and learns to reconstruct them. This formulation encourages the model to learn latent representations capturing holistic session semantics. In collaborative filtering, denoising autoencoders (e.g., CDAE) have been used for recommendation, where randomly corrupted user vectors are reconstructed (arxiv.org, researchgate.net). BERT4Rec extends that idea to sequences of interactions with the Transformer’s bidirectional power.\nBroader Context: From Language to Recommendation Transfer of Ideas Across Domains BERT4Rec is an instance of cross-pollination between NLP and recommendation research. Historically, many breakthroughs in one field find applications in others:\nWord2Vec (2013): Initially for word embeddings, later adapted for graph embeddings, collaborative filtering, and more. Convolutional Neural Networks (1995–2012): Developed for image tasks, later adapted for text (CNNs for sentence classification) and recommendation (Caser uses convolution to model user-item sequences). Attention Mechanisms (2014–2017): Originating in machine translation, now used in recommendation (e.g., SASRec, BERT4Rec, and many variants). The flow of ideas mirrors human creativity: when we learn a concept in one context, we often find analogous patterns in another.\nAnalogy: Leonardo da Vinci studied bird flight to design flying machines. Similarly, BERT4Rec studies how Transformers learn from language sequences to design better user modeling systems.\nHistorical Perspective: The Rise of Pre-Training In both language and recommendation, there is a shift from task-specific training to pre-training + fine-tuning:\nIn NLP, models like ELMo (2018), GPT (2018), and BERT (2018–2019) introduced large-scale pre-training on massive unlabeled corpora, followed by fine-tuning on downstream tasks (arxiv.org, aclanthology.org). In recommendation, early models trained from scratch on each dataset. Now, researchers explore pre-training on large interaction logs to learn general user behavior patterns, then fine-tune on specific domains (e.g., news, movies). BERT4Rec’s Cloze objective could be viewed as a form of self-supervised pre-training, although in the original work they trained on the target dataset from scratch (arxiv.org, arxiv.org). This trend reflects a broader movement in AI: capturing general knowledge from large data and adapting it to specific tasks, mirroring human learning—children first learn language generally, then apply it to specialized domains like mathematics or science.\nLimitations and Challenges Computational Complexity BERT4Rec’s bidirectional attention has quadratic time and memory complexity with respect to sequence length. In long sessions (e.g., browsing logs with hundreds of items), this becomes a bottleneck. Several strategies mitigate this:\nTruncated Histories: Only consider the last L items (e.g., L = 200). Segmented or Sliding Windows: Process overlapping windows of fixed length rather than the entire history. Efficient Attention Variants: Use sparse attention (e.g., Linformer, Performer) to reduce complexity from O(L²) to O(L log L) or O(L) (arxiv.org). Nonetheless, these require extra engineering and can affect performance if important interactions get truncated.\nTraining Sensitivity and Hyperparameters As noted by Petrov and Macdonald (2022), BERT4Rec’s performance is sensitive to:\nNumber of Training Epochs: Standard schedules may under-train the model. Learning Rate Schedules: Warm-up steps followed by linear decay often yield stable performance. Batch Size and Mask Ratio: Larger batches and masking too many positions can hinder learning. Negative Sampling Effects: Overconfidence in ranking due to unbalanced positive/negative sampling can lead to suboptimal results; alternative loss functions (e.g., gBCE) can mitigate this (arxiv.org, arxiv.org). This contrasts with smaller unidirectional models like SASRec, which often converge faster and require fewer tuning efforts.\nCold-Start and Long-Tail Items Like many collaborative filtering methods, BERT4Rec struggles with:\nCold-Start Users: Users with very short or no interaction history. Masked predictions require context—if there’s no context, predictions degrade. Cold-Start Items: Items with very few interactions. Their embeddings are not well trained, making them less likely to be predicted. Long-Tail Distribution: Most items appear infrequently; BERT4Rec can overfit popular items seen many times in training, biasing recommendations. Mitigations include:\nIncorporating content features (e.g., item metadata, text descriptions) through hybrid models. Using meta-learning to quickly adapt to new items or users. Employing data augmentation (e.g., synthetic interactions) to enrich representations. Interpretability Transformers are often regarded as “black boxes.” While attention weights can sometimes be visualized to show which items influence predictions, they do not guarantee human-interpretable explanations. Efforts to explain recommendation via attention often reveal that attention scores do not always align with intuitive importance (arxiv.org). For stakeholders demanding transparency, additional interpretability methods (e.g., counterfactual explanations, post-hoc analysis) may be needed.\nVariants and Extensions Incorporating Side Information BERT4Rec can be extended to use side features:\nUser Features: Demographics, location, device, etc. Item Features: Category, price, textual description, images. Session Context: Time gaps, device changes, location transitions. One approach is to concatenate side feature embeddings with item embeddings at each position, then feed the combined vector into the Transformer (arxiv.org). Alternatively, one can use separate Transformer streams for different modalities and then merge them (e.g., multi-modality Transformers).\nPre-Training on Large-Scale Logs Instead of training BERT4Rec from scratch on a target dataset, it can be pre-trained on massive generic interaction logs (e.g., clicks across many categories) and fine-tuned on a domain-specific dataset (e.g., music). Pre-training tasks might include:\nMasked Item Prediction (as usual). Segment Prediction: Predict whether a sequence segment belongs to the same user. Next Session Prediction: Predict which next session a user will have. After pre-training, the model adapts faster to downstream tasks, especially in data-sparse domains. This mimics BERT’s success in NLP.\nCombining with Contrastive Learning Recent trends in self-supervised learning for recommendation incorporate contrastive objectives, encouraging similar user sequences or items to have similar representations. One can combine BERT4Rec’s Cloze objective with contrastive losses (e.g., SimCLR, MoCo) to further improve generalization:\nSequence-Level Contrast: Represent a user session by pooling BERT4Rec’s hidden states; contrast similar sessions against dissimilar ones. Item-Level Contrast: Encourage items co-occurring frequently to have similar embeddings. Contrastive learning can mitigate representation collapse and improve robustness.\nEfficient Transformer Variants To handle long sequences more efficiently:\nLinformer: Projects keys and values to a lower dimension before computing attention, reducing complexity from O(L²) to O(L) (arxiv.org). Performer: Uses kernel methods to approximate softmax attention linearly in sequence length. Longformer: Employs sliding window (local) attention and global tokens. Reformer: Uses locality-sensitive hashing to reduce attention costs. These variants can be plugged into BERT4Rec’s framework to handle longer sessions while retaining bidirectional context.\nFuture Directions Personalization and Diversity While BERT4Rec focuses on accuracy metrics like HR@K and NDCG@K, real-world systems must balance personalization with diversity to avoid echo chambers. Future work could:\nInclude diversity-aware objectives, penalizing recommendations that are too similar to each other. Integrate exploration strategies, e.g., adding randomness to top-K predictions to surface niche items. Leverage reinforcement learning to optimize long-term engagement rather than immediate next click. Adaptation to Multi-Objective Settings E-commerce platforms care about metrics beyond clicks—revenues, lifetime value, churn reduction. Extensions of BERT4Rec could incorporate:\nMulti-Task Learning: Jointly predict next item and other objectives (e.g., purchase probability, churn risk). Bandit Feedback: Combine BERT4Rec embeddings with contextual bandit algorithms to dynamically adapt to user feedback. Causal Inference: Adjust for selection bias in logged interactions, using inverse propensity scoring with BERT4Rec representations. Explainability and Trust Building user trust in recommendations requires transparency. Research could focus on:\nAttention-Based Explanations: Visualizing attention maps to show which past items influenced a recommendation. Counterfactual Explanations: Explaining “if you hadn’t clicked on item A, you might not see item B recommended.” User-Friendly Summaries: Summarizing session themes (e.g., “Because you watched yoga videos, we recommend this fitness product”). Cross-Seat and Cross-Device Scenarios Users often switch between devices (phone, laptop, TV) and contexts (work, home). Modeling these cross-seat patterns requires:\nHierarchical Transformers: One level encodes per-device sequences; another encodes cross-device transitions. Time-Aware Modeling: Incorporate temporal embeddings for time gaps between interactions, using continuous time Transformers. Hybrid with Knowledge Graphs Many platforms maintain knowledge graphs linking items to attributes, categories, and external entities. Integrating BERT4Rec embeddings with graph neural networks (GNNs) can enrich representations:\nGraph-Enhanced Embeddings: Use GNNs to initialize item embeddings based on their neighbors in the knowledge graph. Joint Attention over Sequences and Graphs: Attend over historical interactions and relevant graph nodes. Personal Reflections and Closing Thoughts Building BERT4Rec felt like standing on the shoulders of giants: from Markov models that taught me the basics of transitions, to RNNs that showed me how to carry hidden state, to attention mechanisms that revealed the power of flexible context, to BERT’s bidirectional pre-training that inspired me to look at user sequences holistically. Each step deepened my understanding of how to model dynamic preferences, echoing my own journey of learning and exploration.\nI’ve always believed that technical advancements in AI should be connected to human-centered insights. When I see masked language models predicting words, I think of a student piecing together meaning. When I see masked item tasks predicting products, I imagine someone reconstructing their shopping trajectory, filling in forgotten steps. These analogies bridge the gap between cold mathematics and living experiences, reminding me that behind each click or purchase is a person with evolving interests, context, and purpose.\nBERT4Rec is not the final word in sequential recommendation. It represents a milestone—a demonstration that ideas from language modeling can transform how we think about recommendation. But as we push forward, we must keep asking: How can we make models more efficient without sacrificing nuance? How can we ensure diversity and fairness? How can we respect privacy while learning from behavior? I hope this post not only explains BERT4Rec’s mechanics but also sparks your own curiosity to explore these questions further.\nReferences and Further Reading Devlin, J., Chang, M.-W., Lee, K., \u0026 Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT (pp. 4171–4186). (arxiv.org, aclanthology.org) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., \u0026 Polosukhin, I. (2017). Attention Is All You Need. In NeurIPS (pp. 5998–6008). (arxiv.org, papers.nips.cc) Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., \u0026 Jiang, P. (2019). BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In CIKM (pp. 1441–1450). (arxiv.org, github.com) Kang, W.-C., \u0026 McAuley, J. (2018). Self-Attentive Sequential Recommendation. In ICDM (pp. 197–206). (arxiv.org, cseweb.ucsd.edu) Petrov, A., \u0026 Macdonald, C. (2022). A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation. arXiv:2207.07483. (arxiv.org, arxiv.org) Petrov, A., \u0026 Macdonald, C. (2023). gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling. arXiv:2308.07192. (arxiv.org) Devlin, J., Chang, M.-W., Lee, K., \u0026 Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805. (arxiv.org, export.arxiv.org) Devlin, J., Chang, M.-W., Lee, K., \u0026 Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805v1. (eecs.csuohio.edu, ar5iv.labs.arxiv.org) Kang, W.-C., \u0026 McAuley, J. (2018). Self-Attentive Sequential Recommendation. arXiv:1808.09781. (arxiv.org, ar5iv.labs.arxiv.org) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., \u0026 Polosukhin, I. (2017). Attention Is All You Need. arXiv:1706.03762. (export.arxiv.org, en.wikipedia.org) Petrov, A., \u0026 Macdonald, C. (2022). A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation. arXiv:2207.07483. Petrov, A., \u0026 Macdonald, C. (2023). gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling. arXiv:2308.07192. Hu, Y., Zhang, Y., Sun, N., Murai, M., Li, M., \u0026 King, I. (2018). Utilizing Long- and Short-Term Structure for Memory-Based Sequential Recommendation. In WWW (pp. 1281–1290). Wu, L., Sun, X., Wang, Y., \u0026 Wu, J. (2020). S3-Rec: Self-Supervised Seq2Seq Autoregressive Reconstruction for Sequential Recommendation. In KDD (pp. 1267–1277). Tan, Y. K., \u0026 Yang, J. (2021). Light-BERT4Rec: Accelerating BERT4Rec via Knowledge Distillation for Sequential Recommendation. In CIKM. Yang, N., Wang, W., \u0026 Zhao, J. (2021). TransRec: Learning User and Item Representations for Sequential Recommendation with Multi-Head Self-Attention. In Sarnoff Symposium. Bi, W., Zhu, X., Lv, H., \u0026 Wang, W. (2021). AdaSAS: Adaptive User Interest Modeling with Multi-Hop Self-Attention for Sequential Recommendation. In RecSys. Ying, C., Fei, K., Wang, X., Wei, F., Mao, J., \u0026 Gao, J. (2018). Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In KDD. (Used as analogy for combining graph structures with sequence modeling.) He, R., \u0026 McAuley, J. (2016). VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback. In AAAI. (Illustrates use of side information in recommendation.) Wang, X., He, X., Cao, Y., Liu, M., \u0026 Chua, T.-S. (2019). KGAT: Knowledge Graph Attention Network for Recommendation. In KDD. (Shows integration of knowledge graphs for richer item representations.) ","wordCount":"6524","inLanguage":"en","datePublished":"2025-01-03T17:23:15+05:30","dateModified":"2025-01-03T17:23:15+05:30","author":{"@type":"Person","name":"Jainish Patel"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://pjainish.github.io/posts/bert4rec-sequential-recommendation/"},"publisher":{"@type":"Organization","name":"Jainish's Log","logo":{"@type":"ImageObject","url":"https://pjainish.github.io/assets/images/favicon.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://pjainish.github.io/ accesskey=h title="Jainish's Log (Alt + H)"><img src=https://pjainish.github.io/assets/images/favicon.png alt aria-label=logo height=30>Jainish's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://pjainish.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://pjainish.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://pjainish.github.io/posts/films/ title=Films><span>Films</span></a></li><li><a href=https://pjainish.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://pjainish.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://pjainish.github.io/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://pjainish.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://pjainish.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers</h1><div class=post-meta><span title='2025-01-03 17:23:15 +0530 IST'>January 3, 2025</span>&nbsp;·&nbsp;31 min&nbsp;·&nbsp;Jainish Patel</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction-a-learning-journey aria-label="Introduction: A Learning Journey">Introduction: A Learning Journey</a></li><li><a href=#background-from-static-to-sequential-recommendation aria-label="Background: From Static to Sequential Recommendation">Background: From Static to Sequential Recommendation</a><ul><li><a href=#the-early-days-collaborative-filtering-and-beyond aria-label="The Early Days: Collaborative Filtering and Beyond">The Early Days: Collaborative Filtering and Beyond</a></li><li><a href=#sequential-recommendation-capturing-the-flow aria-label="Sequential Recommendation: Capturing the Flow">Sequential Recommendation: Capturing the Flow</a></li><li><a href=#self-attention-and-sasrec-a-step-towards-flexible-context aria-label="Self-Attention and SASRec: A Step Towards Flexible Context">Self-Attention and SASRec: A Step Towards Flexible Context</a></li></ul></li><li><a href=#the-transformer-revolution-background-and-impact aria-label="The Transformer Revolution: Background and Impact">The Transformer Revolution: Background and Impact</a><ul><li><a href=#the-birth-of-the-transformer-vaswani-et-al-2017 aria-label="The Birth of the Transformer (Vaswani et al., 2017)">The Birth of the Transformer (Vaswani et al., 2017)</a></li><li><a href=#bert-deep-bidirectional-language-representation-devlin-et-al-2018 aria-label="BERT: Deep Bidirectional Language Representation (Devlin et al., 2018)">BERT: Deep Bidirectional Language Representation (Devlin et al., 2018)</a></li></ul></li><li><a href=#bert4rec-core-ideas-and-design aria-label="BERT4Rec: Core Ideas and Design">BERT4Rec: Core Ideas and Design</a><ul><li><a href=#motivation-why-bidirectional-modeling-matters aria-label="Motivation: Why Bidirectional Modeling Matters">Motivation: Why Bidirectional Modeling Matters</a></li><li><a href=#architecture-overview aria-label="Architecture Overview">Architecture Overview</a></li><li><a href=#training-as-a-cloze-task-deeper-explanation aria-label="Training as a Cloze Task: Deeper Explanation">Training as a Cloze Task: Deeper Explanation</a></li><li><a href=#comparison-with-unidirectional-models-eg-sasrec aria-label="Comparison with Unidirectional Models (e.g., SASRec)">Comparison with Unidirectional Models (e.g., SASRec)</a></li></ul></li><li><a href=#drawing-analogies-cloze-tests-human-learning-and-recommendation aria-label="Drawing Analogies: Cloze Tests, Human Learning, and Recommendation">Drawing Analogies: Cloze Tests, Human Learning, and Recommendation</a><ul><li><a href=#the-psychology-of-masked-tests aria-label="The Psychology of Masked Tests">The Psychology of Masked Tests</a></li><li><a href=#historical-perspective-from-prediction-to-comprehension aria-label="Historical Perspective: From Prediction to Comprehension">Historical Perspective: From Prediction to Comprehension</a></li><li><a href=#real-world-analogy-playlist-shuffling aria-label="Real-World Analogy: Playlist Shuffling">Real-World Analogy: Playlist Shuffling</a></li></ul></li><li><a href=#technical-deep-dive-bert4recs-mechanics aria-label="Technical Deep Dive: BERT4Rec’s Mechanics">Technical Deep Dive: BERT4Rec’s Mechanics</a><ul><li><a href=#input-representation aria-label="Input Representation">Input Representation</a></li><li><a href=#transformer-encoder-stack aria-label="Transformer Encoder Stack">Transformer Encoder Stack</a></li><li><a href=#output-and-loss-computation aria-label="Output and Loss Computation">Output and Loss Computation</a></li><li><a href=#inference-predicting-the-next-item aria-label="Inference: Predicting the Next Item">Inference: Predicting the Next Item</a></li></ul></li><li><a href=#experimental-results-and-analysis aria-label="Experimental Results and Analysis">Experimental Results and Analysis</a><ul><li><a href=#datasets-and-evaluation-protocols aria-label="Datasets and Evaluation Protocols">Datasets and Evaluation Protocols</a></li><li><a href=#baselines-compared aria-label="Baselines Compared">Baselines Compared</a></li><li><a href=#key-findings aria-label="Key Findings">Key Findings</a></li><li><a href=#replicability-and-training-considerations aria-label="Replicability and Training Considerations">Replicability and Training Considerations</a></li><li><a href=#comparative-strengths-and-weaknesses aria-label="Comparative Strengths and Weaknesses">Comparative Strengths and Weaknesses</a></li></ul></li><li><a href=#conceptual-insights-why-bert4rec-works aria-label="Conceptual Insights: Why BERT4Rec Works">Conceptual Insights: Why BERT4Rec Works</a><ul><li><a href=#learning-co-occurrence-vs-sequential-order aria-label="Learning Co-Occurrence vs. Sequential Order">Learning Co-Occurrence vs. Sequential Order</a></li><li><a href=#overcoming-exposure-bias aria-label="Overcoming Exposure Bias">Overcoming Exposure Bias</a></li><li><a href=#analogous-to-autoencoders aria-label="Analogous to Autoencoders">Analogous to Autoencoders</a></li></ul></li><li><a href=#broader-context-from-language-to-recommendation aria-label="Broader Context: From Language to Recommendation">Broader Context: From Language to Recommendation</a><ul><li><a href=#transfer-of-ideas-across-domains aria-label="Transfer of Ideas Across Domains">Transfer of Ideas Across Domains</a></li><li><a href=#historical-perspective-the-rise-of-pre-training aria-label="Historical Perspective: The Rise of Pre-Training">Historical Perspective: The Rise of Pre-Training</a></li></ul></li><li><a href=#limitations-and-challenges aria-label="Limitations and Challenges">Limitations and Challenges</a><ul><li><a href=#computational-complexity aria-label="Computational Complexity">Computational Complexity</a></li><li><a href=#training-sensitivity-and-hyperparameters aria-label="Training Sensitivity and Hyperparameters">Training Sensitivity and Hyperparameters</a></li><li><a href=#cold-start-and-long-tail-items aria-label="Cold-Start and Long-Tail Items">Cold-Start and Long-Tail Items</a></li><li><a href=#interpretability aria-label=Interpretability>Interpretability</a></li></ul></li><li><a href=#variants-and-extensions aria-label="Variants and Extensions">Variants and Extensions</a><ul><li><a href=#incorporating-side-information aria-label="Incorporating Side Information">Incorporating Side Information</a></li><li><a href=#pre-training-on-large-scale-logs aria-label="Pre-Training on Large-Scale Logs">Pre-Training on Large-Scale Logs</a></li><li><a href=#combining-with-contrastive-learning aria-label="Combining with Contrastive Learning">Combining with Contrastive Learning</a></li><li><a href=#efficient-transformer-variants aria-label="Efficient Transformer Variants">Efficient Transformer Variants</a></li></ul></li><li><a href=#future-directions aria-label="Future Directions">Future Directions</a><ul><li><a href=#personalization-and-diversity aria-label="Personalization and Diversity">Personalization and Diversity</a></li><li><a href=#adaptation-to-multi-objective-settings aria-label="Adaptation to Multi-Objective Settings">Adaptation to Multi-Objective Settings</a></li><li><a href=#explainability-and-trust aria-label="Explainability and Trust">Explainability and Trust</a></li><li><a href=#cross-seat-and-cross-device-scenarios aria-label="Cross-Seat and Cross-Device Scenarios">Cross-Seat and Cross-Device Scenarios</a></li><li><a href=#hybrid-with-knowledge-graphs aria-label="Hybrid with Knowledge Graphs">Hybrid with Knowledge Graphs</a></li></ul></li><li><a href=#personal-reflections-and-closing-thoughts aria-label="Personal Reflections and Closing Thoughts">Personal Reflections and Closing Thoughts</a></li><li><a href=#references-and-further-reading aria-label="References and Further Reading">References and Further Reading</a></li></ul></div></details></div><div class=post-content><p>BERT4Rec is a sequential recommendation model that leverages the bidirectional Transformer architecture, originally designed for language tasks, to capture users’ evolving preferences by jointly considering both past and future items in a sequence (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://github.com/FeiSun/BERT4Rec title="GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...">github.com</a>). Unlike earlier unidirectional models that predict the next item only from previous ones, BERT4Rec uses a Cloze-style masking objective to predict missing items anywhere in the sequence, enabling richer context modeling (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://github.com/FeiSun/BERT4Rec title="GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...">github.com</a>). Empirical evaluations on multiple benchmark datasets demonstrate that BERT4Rec often surpasses state-of-the-art sequential models like SASRec, though its performance can depend on careful training schedules and hyperparameter choices (<a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>, <a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>). This post traces the journey from early recommendation methods to the Transformer revolution and the rise of BERT, explains the core ideas behind BERT4Rec, connects them to cognitive analogies of Cloze tests, and discusses experiments, limitations, and future directions. By understanding BERT4Rec’s design and its place in the broader landscape of recommendation, readers can appreciate both its technical elegance and its conceptual roots in language modeling and human learning.</p><h2 id=introduction-a-learning-journey>Introduction: A Learning Journey<a hidden class=anchor aria-hidden=true href=#introduction-a-learning-journey>#</a></h2><p>I still remember the first time I tried to build a recommendation system. It was during my undergraduate years, and I wanted to create a small app that suggested books to my friends based on what they had read before. At that time, I naively believed that simply counting co-occurrences of books would be enough. I soon realized that user preferences change over time, and static co-occurrence matrices felt too rigid. That curiosity led me to explore sequential recommendation—models that treat a user’s history as an evolving narrative rather than a single static snapshot.</p><p>Fast forward a few years, and I found myself diving into deep learning approaches for recommendation during my PhD. Each step felt like peeling another layer of understanding: starting with simple Markov chains, moving to recurrent neural networks, then witnessing the Transformer revolution in natural language processing (NLP) with papers like “Attention Is All You Need” (<a href=https://arxiv.org/abs/1706.03762 title="Attention Is All You Need">arxiv.org</a>, <a href=https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf title="Attention is All you Need - NIPS">papers.nips.cc</a>) and “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” (<a href=https://arxiv.org/abs/1810.04805 title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">arxiv.org</a>, <a href=https://aclanthology.org/N19-1423/ title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">aclanthology.org</a>). In language tasks, these models treated sentences as dynamic sequences of words; in recommendation, sequences of items could be handled similarly.</p><p>Just as the alphabet and grammar form the foundation of language, the sequence of user interactions—clicks, views, purchases—forms the grammar of recommendation. When I first encountered BERT4Rec, I saw a bridge between these worlds: a model designed for language Cloze tasks, applied to sequences of items. In this post, I want to share that journey—why the shift from unidirectional to bidirectional models matters, how the Cloze objective parallels human tests, the design choices behind BERT4Rec, and what we can learn both technically and conceptually from it. My hope is that, by the end, you’ll see BERT4Rec not just as another state-of-the-art model, but as part of a broader narrative connecting human cognition, language, and recommendation.</p><h2 id=background-from-static-to-sequential-recommendation>Background: From Static to Sequential Recommendation<a hidden class=anchor aria-hidden=true href=#background-from-static-to-sequential-recommendation>#</a></h2><h3 id=the-early-days-collaborative-filtering-and-beyond>The Early Days: Collaborative Filtering and Beyond<a hidden class=anchor aria-hidden=true href=#the-early-days-collaborative-filtering-and-beyond>#</a></h3><p>Recommender systems began with collaborative filtering approaches that treat users and items symmetrically, often using matrix factorization to uncover latent factors (<a href=https://link.springer.com/chapter/10.1007/978-981-16-5348-3_48 title="Cross-domain Self-attentive Sequential Recommendations">link.springer.com</a>). These methods assume static preferences: a user has fixed tastes, and items have fixed attributes. For example, if Alice liked “The Hobbit” and “The Lord of the Rings,” a static model would continue recommending similar fantasy books without considering that she might have grown more interested in science fiction recently.</p><p>Psychologically, this is akin to assuming that a person’s personality never changes—an oversimplification. In reality, tastes evolve. Just as our moods and interests shift from week to week, user interactions in an online setting reflect changing preferences. Recognizing this, researchers started looking at temporal dynamics: assigning more weight to recent interactions (<a href=https://link.springer.com/chapter/10.1007/978-981-16-5348-3_48 title="Cross-domain Self-attentive Sequential Recommendations">link.springer.com</a>, <a href=https://arxiv.org/abs/1808.09781 title="Self-Attentive Sequential Recommendation">arxiv.org</a>). However, these adjustments were often heuristic rather than deeply integrated into the model’s structure.</p><h3 id=sequential-recommendation-capturing-the-flow>Sequential Recommendation: Capturing the Flow<a hidden class=anchor aria-hidden=true href=#sequential-recommendation-capturing-the-flow>#</a></h3><p>To better model evolving preferences, sequential recommendation treats a user’s history as an ordered list of events. Two main families of approaches emerged:</p><ul><li><p><strong>Markov Chain-based Models</strong>: These assume that the next action depends on a limited window of previous actions, often just the last one or two (<a href=https://cseweb.ucsd.edu/~jmcauley/pdfs/icdm18.pdf title="Self-Attentive Sequential Recommendation - University of California ...">cseweb.ucsd.edu</a>). While simple and effective in sparse settings, they struggle to capture longer-term patterns. It’s like predicting the next word in a sentence by looking only at the immediately preceding word—sometimes okay, but often missing broader context.</p></li><li><p><strong>Recurrent Neural Networks (RNNs)</strong>: With the rise of deep learning, RNNs (e.g., GRU4Rec) became popular for sequential recommendation tasks. They process one item at a time, updating a hidden state that summarizes the history (<a href=https://link.springer.com/chapter/10.1007/978-981-16-5348-3_48 title="Cross-domain Self-attentive Sequential Recommendations">link.springer.com</a>, <a href=https://arxiv.org/abs/1808.09781 title="Self-Attentive Sequential Recommendation">arxiv.org</a>). While theoretically capable of capturing long-range dependencies, RNNs can suffer from vanishing gradients and can be slow to train, especially when sequences get long.</p></li></ul><p>These methods moved beyond static views of users, but they still relied on <em>unidirectional</em> modeling: either Markov chains always look backward a fixed number of steps, and RNNs process sequences from left (oldest) to right (newest). In human terms, it’s like reading a story only forward—never knowing how the ending influences the interpretation of earlier chapters.</p><h3 id=self-attention-and-sasrec-a-step-towards-flexible-context>Self-Attention and SASRec: A Step Towards Flexible Context<a hidden class=anchor aria-hidden=true href=#self-attention-and-sasrec-a-step-towards-flexible-context>#</a></h3><p>In August 2018, Kang and McAuley introduced SASRec, a self-attentive sequential model that borrowed ideas from the Transformer’s self-attention mechanism to balance long-term and short-term context (<a href=https://arxiv.org/abs/1808.09781 title="Self-Attentive Sequential Recommendation">arxiv.org</a>, <a href=https://cseweb.ucsd.edu/~jmcauley/pdfs/icdm18.pdf title="Self-Attentive Sequential Recommendation - University of California ...">cseweb.ucsd.edu</a>). Instead of processing item sequences strictly left-to-right, SASRec computes attention weights over all previous items at each step, allowing the model to focus on the most relevant past actions when predicting the next one (<a href=https://arxiv.org/abs/1808.09781 title="Self-Attentive Sequential Recommendation">arxiv.org</a>, <a href=https://arxiv.org/pdf/1808.09781 title="arXiv.org e-Print archive">arxiv.org</a>). Mechanically, it applies multi-head self-attention layers over item embeddings, followed by pointwise feed-forward layers, similar to each encoder block in the original Transformer (<a href=https://arxiv.org/abs/1706.03762 title="Attention Is All You Need">arxiv.org</a>, <a href=https://export.arxiv.org/abs/1706.03762v5 title="[1706.03762v5] Attention Is All You Need - arXiv">export.arxiv.org</a>).</p><p>SASRec offered two main advantages:</p><ol><li><strong>Efficiency</strong>: By parallelizing self-attention computations across positions, SASRec can be trained faster than RNN-based models on modern hardware.</li><li><strong>Adaptive Context</strong>: Attention weights allow the model to decide which past items matter most, rather than forcing it to use a fixed window or hidden state sequence.</li></ol><p>However, SASRec remains <em>unidirectional</em> in its attention: at each time step, it only attends to items that come before that position. This means it still cannot consider potential “future” items, even if they would be known at test time when scoring multiple candidate items. In language terms, it’s like understanding a sentence by reading it left to right—never knowing what words come later in the sentence.</p><h2 id=the-transformer-revolution-background-and-impact>The Transformer Revolution: Background and Impact<a hidden class=anchor aria-hidden=true href=#the-transformer-revolution-background-and-impact>#</a></h2><h3 id=the-birth-of-the-transformer-vaswani-et-al-2017>The Birth of the Transformer (Vaswani et al., 2017)<a hidden class=anchor aria-hidden=true href=#the-birth-of-the-transformer-vaswani-et-al-2017>#</a></h3><p>In June 2017, Vaswani et al. published “Attention Is All You Need,” a paper that fundamentally changed NLP and sequence modeling (<a href=https://arxiv.org/abs/1706.03762 title="Attention Is All You Need">arxiv.org</a>, <a href=https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf title="Attention is All you Need - NIPS">papers.nips.cc</a>). They introduced the <strong>Transformer</strong>, which replaced recurrence with multi-head self-attention and simple feed-forward networks. The key insights were:</p><ul><li><strong>Self-Attention Layers</strong>: These compute weighted sums of all positions’ embeddings for each position, allowing direct modeling of pairwise dependencies regardless of distance.</li><li><strong>Positional Encoding</strong>: Since attention layers by themselves lack inherent order, they added sinusoidal positional encodings to inject sequence information.</li><li><strong>Parallelization</strong>: Unlike RNNs, Transformers can process all positions in parallel, making training significantly faster on GPUs.</li></ul><p>By discarding recurrence and convolutions, the Transformer demonstrated state-of-the-art performance on machine translation tasks, achieving BLEU scores surpassing previous best models on WMT English-German and English-French benchmarks (<a href=https://arxiv.org/abs/1706.03762 title="Attention Is All You Need">arxiv.org</a>, <a href=https://scispace.com/papers/attention-is-all-you-need-1hodz0wcqb title="(PDF) Attention is All you Need (2017) | Ashish Vaswani - Typeset">scispace.com</a>). This architecture quickly became the de facto backbone for a wide range of NLP tasks, from translation to summarization to question answering (<a href=https://huggingface.co/papers/1706.03762 title="Paper page - Attention Is All You Need - Hugging Face">huggingface.co</a>, <a href=https://arxiv.org/pdf/1706.03762 title="arXiv.org e-Print archive">arxiv.org</a>).</p><p>Analogy: Before Transformers, sequence models were like cars with only one speed—reverse (recurrence) or forward (convolutions/attention with constraints). Transformers were like multi-gear vehicles that could shift seamlessly, giving models flexibility to access information anywhere in the sequence, much like looking up any chapter in a book instantly rather than reading every page sequentially.</p><h3 id=bert-deep-bidirectional-language-representation-devlin-et-al-2018>BERT: Deep Bidirectional Language Representation (Devlin et al., 2018)<a hidden class=anchor aria-hidden=true href=#bert-deep-bidirectional-language-representation-devlin-et-al-2018>#</a></h3><p>Building on the Transformer’s encoder, Devlin et al. introduced <strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) in October 2018 (<a href=https://arxiv.org/abs/1810.04805 title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">arxiv.org</a>, <a href=https://aclanthology.org/N19-1423/ title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">aclanthology.org</a>). BERT’s main contributions were:</p><ul><li><strong>Bidirectional Context</strong>: By jointly attending to both left and right context in all layers (rather than only attending to previous tokens), BERT can learn richer representations.</li><li><strong>Masked Language Modeling (MLM)</strong>: To enable bidirectionality, they used a Cloze-like task: randomly mask some tokens in the input and train the model to predict them based on surrounding context.</li><li><strong>Next Sentence Prediction (NSP)</strong>: As a secondary task, BERT predicts whether two sentences follow each other, helping capture inter-sentence relationships.</li></ul><p>BERT was pre-trained on massive corpora (BooksCorpus and English Wikipedia), achieving state-of-the-art results across a variety of NLP benchmarks, such as GLUE, SQuAD, and others (<a href=https://arxiv.org/abs/1810.04805 title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">arxiv.org</a>, <a href=https://export.arxiv.org/abs/1810.04805 title="[1810.04805] BERT: Pre-training of Deep Bidirectional ... - arXiv">export.arxiv.org</a>). Its bidirectional design unlocked new capabilities: while unidirectional language models (e.g., OpenAI GPT) process text left-to-right, BERT’s MLM allowed it to encode context from both sides, akin to reading a sentence and filling in missing words anywhere in it.</p><p>Analogy: Imagine reading a paragraph with some words hidden and having to guess them using the rest of the paragraph. This Cloze-style task is exactly how BERT learns. In human tests, teachers often use fill-in-the-blank exercises to gauge comprehension—similarly, BERT’s MLM forces the model to deeply understand context.</p><p>The impact of BERT extended beyond NLP. Researchers began to ask: if bidirectional Transformers can learn from masked words in a sentence, could a similar idea work for sequences of user interactions? Enter BERT4Rec.</p><h2 id=bert4rec-core-ideas-and-design>BERT4Rec: Core Ideas and Design<a hidden class=anchor aria-hidden=true href=#bert4rec-core-ideas-and-design>#</a></h2><h3 id=motivation-why-bidirectional-modeling-matters>Motivation: Why Bidirectional Modeling Matters<a hidden class=anchor aria-hidden=true href=#motivation-why-bidirectional-modeling-matters>#</a></h3><p>In sequential recommendation, we often care about predicting the next item given past history. Unidirectional models like SASRec attend only to prior items when making a prediction (<a href=https://arxiv.org/abs/1808.09781 title="Self-Attentive Sequential Recommendation">arxiv.org</a>, <a href=https://cseweb.ucsd.edu/~jmcauley/pdfs/icdm18.pdf title="Self-Attentive Sequential Recommendation - University of California ...">cseweb.ucsd.edu</a>). However, at evaluation or inference time, we typically score multiple candidate items to find the most likely next item. Those candidates can be seen as “future” items once we inject them into the sequence. If the model can attend to both past items and the candidate item itself (as if it were masked during training), it can form a richer representation that uses information from the full sequence context.</p><p>BERT4Rec reframes sequential recommendation as a Cloze task: randomly mask items in the user’s history and train the model to predict them based on both left and right context, which may include items that occur after them in the sequence (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://github.com/FeiSun/BERT4Rec title="GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...">github.com</a>). This bidirectional conditioning helps the model learn how items co-occur in different parts of the sequence, not just in a strict left-to-right chain.</p><p>Analogy: In a detective novel, clues about who committed the crime may appear early and later in the story. A unidirectional reader would only use clues from the beginning up to the current chapter. A bidirectional reader, knowing the ending, can reinterpret earlier clues in light of later revelations. Similarly, BERT4Rec’s bidirectional attention allows the model to reinterpret earlier interactions when considering missing items.</p><h3 id=architecture-overview>Architecture Overview<a hidden class=anchor aria-hidden=true href=#architecture-overview>#</a></h3><p>At a high level, BERT4Rec follows the encoder architecture from the original Transformer with two major changes:</p><ol><li><strong>Cloze-style Masking</strong>: A certain percentage of items in a user’s sequence are randomly masked (replaced with a special [MASK] token). The model’s task is to predict the identity of each masked item using bidirectional attention over the unmasked items (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://www.researchgate.net/profile/Fei-Sun-41/publication/332438773_BERT4Rec_Sequential_Recommendation_with_Bidirectional_Encoder_Representations_from_Transformer/links/6047567d299bf1e0786667eb/BERT4Rec-Sequential-Recommendation-with-Bidirectional-Encoder-Representations-from-Transformer.pdf title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder ...">researchgate.net</a>).</li><li><strong>Item Embeddings with Positional Encodings</strong>: Each item in the sequence is mapped to a learned embedding. Since the Transformer has no inherent sense of order, sinusoidal or learned positional encodings are added to each item embedding to encode its position in the sequence (<a href=https://arxiv.org/abs/1706.03762 title="Attention Is All You Need">arxiv.org</a>, <a href=https://ar5iv.labs.arxiv.org/html/1706.03762 title="[1706.03762] Attention Is All You Need - ar5iv">ar5iv.labs.arxiv.org</a>).</li></ol><p>Concretely:</p><ul><li><strong>Input</strong>: A user history of length <em>n</em> (e.g., [i₁, i₂, …, iₙ]). We randomly choose a subset of positions (usually 15%) and replace them with [MASK] tokens. For example, if the original sequence is [A, B, C, D, E] and positions 2 and 4 are masked, the input becomes [A, [MASK], C, [MASK], E].</li><li><strong>Embedding Layer</strong>: Each position <em>t</em> has an embedding <code>E_item(iₜ)</code> (for item <em>iₜ</em>) plus a positional embedding <code>E_pos(t)</code>. So, the initial input to the Transformer is the sum <code>E_item + E_pos</code> for each position, with masked positions using a special mask embedding.</li><li><strong>Transformer Encoder Stack</strong>: Typically 2 to 4 layers (depending on hyperparameters) of multi-head self-attention and feed-forward layers. Since we want bidirectional context, the self-attention is “full” (not masked), allowing each position to attend to all other positions in the sequence.</li><li><strong>Output Heads</strong>: For each masked position, the final hidden state vector is passed through a linear projection followed by a softmax over the item vocabulary to predict which item was masked.</li><li><strong>Loss Function</strong>: Cross-entropy loss is computed only over the masked positions, summing (or averaging) across them.</li></ul><p>During inference, to predict the next item, one can append a [MASK] token to the end of a user’s sequence and feed it through the model. The model’s output distribution at that position indicates the probabilities of all possible items being the next interaction.</p><p>Technical Note: Because BERT4Rec conditions on bidirectional context, it avoids what is known as <strong>“exposure bias”</strong> often found in left-to-right models, where during training the model sees only ground-truth history, but during inference it must rely on its own predictions. BERT4Rec’s Cloze objective alleviates this by mixing masked ground truth with unmasked items, making the model robust to masked or unknown future items.</p><h3 id=training-as-a-cloze-task-deeper-explanation>Training as a Cloze Task: Deeper Explanation<a hidden class=anchor aria-hidden=true href=#training-as-a-cloze-task-deeper-explanation>#</a></h3><p>The term <strong>Cloze</strong> comes from psycholinguistics and educational testing: learners fill in missing words in a text passage (<a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>, <a href=https://www.kdnuggets.com/2019/07/pre-training-transformers-bi-directionality.html title="Pre-training, Transformers, and Bi-directionality - KDnuggets">kdnuggets.com</a>). This is not a new idea. In fact, BERT borrowed it directly from earlier NLP work, such as the Cloze tests used by educators to measure student comprehension (<a href=https://www.kdnuggets.com/2019/07/pre-training-transformers-bi-directionality.html title="Pre-training, Transformers, and Bi-directionality - KDnuggets">kdnuggets.com</a>). In the context of recommendation:</p><ul><li><strong>Masked Item Prediction (MIP)</strong>: Analogous to masked language modeling (MLM) in BERT, BERT4Rec’s MIP randomly selects a subset of positions in a user’s interaction sequence, hides each item, and asks the model to fill it in based on both past and future interactions.</li><li><strong>Sampling Strategy</strong>: Typically, 15% of items are chosen for masking. Of those, 80% are replaced with [MASK], 10% with a random item (to encourage robustness), and 10% are left unchanged but still counted in the loss as if they were masked (to mitigate training/test mismatch) (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://github.com/FeiSun/BERT4Rec title="GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...">github.com</a>).</li><li><strong>Advantages</strong>: By predicting items anywhere in the sequence, the model learns co-occurrence patterns in all contexts, not just predicting the next item. This generates more training samples per sequence (since each masked position is a training example), potentially improving data efficiency (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://arxiv.org/pdf/1904.06690v1 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder ...">arxiv.org</a>).</li></ul><p>Analogy: When learning a language, filling in blank words anywhere in a paragraph helps both reading comprehension and vocabulary acquisition. Similarly, by practicing predicting missing items anywhere in their history, the model builds a more flexible representation of user preferences.</p><h3 id=comparison-with-unidirectional-models-eg-sasrec>Comparison with Unidirectional Models (e.g., SASRec)<a hidden class=anchor aria-hidden=true href=#comparison-with-unidirectional-models-eg-sasrec>#</a></h3><ul><li><p><strong>Context Scope</strong></p><ul><li><em>Unidirectional (SASRec)</em>: At position <em>t</em>, the model attends only to items 1 through <em>t–1</em>.</li><li><em>Bidirectional (BERT4Rec)</em>: At each masked position <em>t</em>, the model attends to all items except those that are also masked. When predicting the next item (by placing a [MASK] at <em>n+1</em>), it attends to items 1 through <em>n</em> and vice versa for other masked positions.</li></ul></li><li><p><strong>Training Objective</strong></p><ul><li><em>Unidirectional</em>: Usually uses next-item prediction with cross-entropy loss at each time step.</li><li><em>Bidirectional</em>: Uses Cloze objective, predicting multiple masked positions per sequence.</li></ul></li><li><p><strong>Data Efficiency</strong></p><ul><li><em>Unidirectional</em>: Generates one training sample per time step (predict next item).</li><li><em>Bidirectional</em>: Generates as many training samples as there are masked positions (typically ~15% of sequence length), often leading to more gradient updates per sequence.</li></ul></li><li><p><strong>Inference</strong></p><ul><li><em>Unidirectional</em>: Directly predicts the next item based on history.</li><li><em>Bidirectional</em>: Appends a [MASK] to the end to predict next item, or can mask any position for in-sequence imputation.</li></ul></li></ul><p>Several empirical studies have shown that BERT4Rec often outperforms SASRec, especially when long-range dependencies are important (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>). However, this performance advantage can require longer training times and careful hyperparameter tuning, as later work has pointed out (<a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>, <a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>).</p><h2 id=drawing-analogies-cloze-tests-human-learning-and-recommendation>Drawing Analogies: Cloze Tests, Human Learning, and Recommendation<a hidden class=anchor aria-hidden=true href=#drawing-analogies-cloze-tests-human-learning-and-recommendation>#</a></h2><h3 id=the-psychology-of-masked-tests>The Psychology of Masked Tests<a hidden class=anchor aria-hidden=true href=#the-psychology-of-masked-tests>#</a></h3><p>Cloze tests, introduced by W. L. Taylor in 1953, are exercises where learners fill in blanks in a passage of text, gauging language comprehension and vocabulary knowledge (<a href=https://www.kdnuggets.com/2019/07/pre-training-transformers-bi-directionality.html title="Pre-training, Transformers, and Bi-directionality - KDnuggets">kdnuggets.com</a>). Educational psychologists have found that Cloze tasks encourage active recall and semantic inference, as learners must use both local and global context to guess missing words correctly. Similarly, BERT’s MLM and BERT4Rec’s MIP require the model to infer missing tokens (words or items) from all available context, reinforcing rich contextual understanding.</p><p>In human terms:</p><ul><li><strong>Local Context</strong>: To guess a masked word in a sentence, you use nearby words.</li><li><strong>Global Context</strong>: Often, clues spread across the paragraph or entire document guide you toward the right answer.</li></ul><p>BERT4Rec’s masked items play the role of blank spaces in a text. The model, like a student in a Cloze test, must use all known interactions (both before and after the blank) to infer the missing preference. This leads to representations that capture not only pairwise item relationships but also how items co-occur across entire sessions.</p><h3 id=historical-perspective-from-prediction-to-comprehension>Historical Perspective: From Prediction to Comprehension<a hidden class=anchor aria-hidden=true href=#historical-perspective-from-prediction-to-comprehension>#</a></h3><p>Early recommendation models focused on <strong>prediction</strong>: given past clicks, what happens next? This is analogous to a fill-in-the-blank exercise where only the next word is blank. In mathematics, this is like knowing all terms of a sequence except the next one and trying to guess it from a recurrence relation. But modern language teaching emphasizes <strong>comprehension</strong>, teaching students to understand entire texts, not just predict the next word. BERT4Rec embodies that shift: from predicting sequentially to understanding a user’s entire session.</p><p>Consider reading Hamlet: if you only focus on predicting the next line, you might miss the broader themes. If you think about themes and motifs across the play, you get a richer understanding. BERT4Rec, by predicting masked items anywhere, learns themes and motifs in interaction sequences as well.</p><h3 id=real-world-analogy-playlist-shuffling>Real-World Analogy: Playlist Shuffling<a hidden class=anchor aria-hidden=true href=#real-world-analogy-playlist-shuffling>#</a></h3><p>Imagine you’re curating a playlist of songs you’ll listen to on a road trip. Instead of putting them in a fixed order (e.g., chronological from your latest favorites), you shuffle them but still want the transitions to feel coherent. A unidirectional model ensures each song transitions well from the previous one, like ensuring each next word makes sense after the last. A bidirectional approach would allow you to also consider the song that comes after when choosing a song for a particular slot, creating smooth transitions both forward and backward. In BERT4Rec, masked songs correspond to shuffled or missing approximate transitions, and the model learns what fits best given both neighbors.</p><h2 id=technical-deep-dive-bert4recs-mechanics>Technical Deep Dive: BERT4Rec’s Mechanics<a hidden class=anchor aria-hidden=true href=#technical-deep-dive-bert4recs-mechanics>#</a></h2><h3 id=input-representation>Input Representation<a hidden class=anchor aria-hidden=true href=#input-representation>#</a></h3><p>Given a user’s historical sequence of item interactions $i₁, i₂, …, iₙ$, BERT4Rec prepares inputs as follows (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://www.researchgate.net/profile/Fei-Sun-41/publication/332438773_BERT4Rec_Sequential_Recommendation_with_Bidirectional_Encoder_Representations_from_Transformer/links/6047567d299bf1e0786667eb/BERT4Rec-Sequential-Recommendation-with-Bidirectional-Encoder-Representations-from-Transformer.pdf title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder ...">researchgate.net</a>):</p><ol><li><p><strong>Masking Strategy</strong></p><ul><li><p>Randomly select 15% of positions for masking.</p></li><li><p>Of those positions:</p><ul><li>80% are replaced with [MASK].</li><li>10% are replaced with a random item ID from the vocabulary (to encourage robustness).</li><li>10% remain unchanged (but are still counted in the loss). This strategy mirrors BERT’s design to prevent the model from relying too heavily on the [MASK] token (<a href=https://arxiv.org/abs/1810.04805 title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">arxiv.org</a>, <a href=https://export.arxiv.org/abs/1810.04805 title="[1810.04805] BERT: Pre-training of Deep Bidirectional ... - arXiv">export.arxiv.org</a>).</li></ul></li></ul></li><li><p><strong>Item Embeddings</strong></p><ul><li>Each item ID has a learned embedding vector of dimension <em>d</em>.</li><li>A special embedding <code>E_mask</code> is used for [MASK] tokens.</li></ul></li><li><p><strong>Positional Embeddings</strong></p><ul><li>Since the Transformer has no notion of sequence order, add a learned positional embedding <code>E_pos(t)</code> for each position <em>t</em> ∈ {1,…,n}.</li><li>The sum <code>E_item(iₜ) + E_pos(t)</code> forms the input embedding at position <em>t</em>.</li></ul></li><li><p><strong>Sequence Length and Padding</strong></p><ul><li>For computational efficiency, fix a maximum sequence length <em>L</em> (e.g., 200).</li><li>If a user’s history has fewer than <em>L</em> interactions, pad the sequence with [PAD] tokens at the front or back.</li><li>[PAD] tokens have embeddings but are ignored in attention computations (i.e., their attention weights are set to zero).</li></ul></li><li><p><strong>Embedding Dropout</strong></p><ul><li>Optional dropout can be applied to the sum of item and positional embeddings to regularize training.</li></ul></li></ol><p>Mathematically, let</p><p>$$
xₜ = E_{item}(iₜ) + E_{pos}(t), \quad t = 1,\dots,n.
$$</p><p>Masked positions use</p><p>$$
xₜ = E_{mask} + E_{pos}(t).
$$</p><h3 id=transformer-encoder-stack>Transformer Encoder Stack<a hidden class=anchor aria-hidden=true href=#transformer-encoder-stack>#</a></h3><p>BERT4Rec typically uses a stack of <em>N</em> encoder layers (e.g., <em>N</em> = 2 or 3 for smaller datasets, up to <em>N</em> = 6 for larger ones), each consisting of:</p><ol><li><p><strong>Multi-Head Self-Attention</strong></p><ul><li><p>For layer <em>l</em>, each position <em>t</em> has queries, keys, and values computed as linear projections of the input from the previous layer.</p></li><li><p>Attention weights are computed as scaled dot products between queries and keys, followed by softmax.</p></li><li><p>Weighted sums of values produce the attention output for each head.</p></li><li><p>The outputs of all heads are concatenated and linearly projected back to dimension <em>d</em>.</p></li><li><p>Residual connection and layer normalization are applied:</p><p>$$
\text{SA}_l(X) = \text{LayerNorm}(X + \text{MultiHeadAttn}(X)).
$$</p></li></ul></li><li><p><strong>Position-Wise Feed-Forward Network</strong></p><ul><li><p>A two-layer feed-forward network with a GELU or ReLU activation:</p><p>$$
\text{FFN}_l(Y) = \text{LayerNorm}(Y + W₂ ,\phi(W₁ Y + b₁) + b₂),
$$</p><p>where $\phi$ is an activation (often GELU).</p></li></ul></li><li><p><strong>LayerNorm and Residual Connections</strong></p><ul><li>As in the original Transformer, each sub-layer has a residual (skip) connection followed by layer normalization, ensuring stable training and gradient flow (<a href=https://arxiv.org/abs/1706.03762 title="Attention Is All You Need">arxiv.org</a>, <a href=https://scispace.com/papers/attention-is-all-you-need-1hodz0wcqb title="(PDF) Attention is All you Need (2017) | Ashish Vaswani - Typeset">scispace.com</a>).</li></ul></li></ol><p>Because the self-attention is <em>full</em> (no masking of future positions), each position’s representation at each layer can incorporate information from any other unmasked position in the sequence.</p><h3 id=output-and-loss-computation>Output and Loss Computation<a hidden class=anchor aria-hidden=true href=#output-and-loss-computation>#</a></h3><p>After <em>N</em> encoder layers, we obtain final hidden representations ${h₁, h₂, \dots, hₙ}$ ∈ ℝ^{n×d}. For each position <em>t</em> that was masked during input preparation, we compute:</p><ol><li><p><strong>Item Prediction Scores</strong></p><p>$$
sₜ = W_{output} , hₜ + b_{output}, \quad sₜ ∈ ℝ^{|V|},
$$</p><p>where <em>|V|</em> is the size of the item vocabulary, and $W_{output} ∈ ℝ^{|V|×d}$.</p></li><li><p><strong>Softmax and Cross-Entropy Loss</strong></p><ul><li><p>Apply softmax to $sₜ$ to get predicted probability distribution $\hat{y}_t$.</p></li><li><p>If the true item ID at position <em>t</em> is $iₜ^*$, the cross-entropy loss for that position is:</p><p>$$
\mathcal{L}<em>t = -\log\bigl(\hat{y}</em>{t}[ iₜ^* ]\bigr).
$$</p></li><li><p>Aggregate loss across all masked positions in the batch, typically averaging over them:</p><p>$$
\mathcal{L} = \frac{1}{\sum_t mₜ} \sum_{t=1}^n mₜ , \mathcal{L}_t,
$$</p><p>where $mₜ = 1$ if position <em>t</em> was masked, else 0.</p></li></ul></li></ol><p>Because multiple positions are masked per sequence, each training example yields several prediction targets, improving data efficiency.</p><h3 id=inference-predicting-the-next-item>Inference: Predicting the Next Item<a hidden class=anchor aria-hidden=true href=#inference-predicting-the-next-item>#</a></h3><p>To recommend the next item for a user:</p><ol><li><p><strong>Extend the Sequence</strong></p><ul><li>Given the user’s last <em>n</em> interactions, append a [MASK] token at position <em>n+1</em> (if <em>n+1 ≤ L</em>). If <em>n</em> = <em>L</em>, one could remove the oldest item or use sliding window techniques.</li></ul></li><li><p><strong>Feed Through Model</strong></p><ul><li>The [MASK] at position <em>n+1</em> participates in bidirectional attention, attending to all positions 1 through <em>n</em>. Conversely, positions 1 through <em>n</em> attend to the [MASK] if full self‐attention is used.</li></ul></li><li><p><strong>Obtain Scores</strong></p><ul><li>Compute $s_{n+1} ∈ ℝ^{|V|}$ from the final hidden state $h_{n+1}$.</li><li>The highest-scoring items in $s_{n+1}$ are the top-K recommendations.</li></ul></li></ol><p>Because BERT4Rec’s training objective was to predict masked items given both left and right context, placing the [MASK] at the end simulates one masked position with only left context. While strictly speaking this isn’t bidirectional (the [MASK] at the end has no right context), it still benefits from richer item co-occurrence patterns learned during training. Empirically, this approach yields strong next-item recommendation accuracy.</p><h2 id=experimental-results-and-analysis>Experimental Results and Analysis<a hidden class=anchor aria-hidden=true href=#experimental-results-and-analysis>#</a></h2><h3 id=datasets-and-evaluation-protocols>Datasets and Evaluation Protocols<a hidden class=anchor aria-hidden=true href=#datasets-and-evaluation-protocols>#</a></h3><p>In the original BERT4Rec paper, Sun et al. evaluated the model on four public benchmark datasets:</p><ol><li><strong>MovieLens-1M (ML-1M)</strong>: 1 million ratings from ~6000 users on ~3900 movies.</li><li><strong>YooChoose</strong>: Click logs from the RecSys Challenge 2015, with ~8.6 million events.</li><li><strong>Steam</strong>: Game purchase and play logs from the Steam platform.</li><li><strong>Amazon Beauty</strong>: Reviews and ratings in the beauty product category from the Amazon Reviews dataset.</li></ol><p>For each user, interactions were chronologically ordered. The last interaction was used as the test item, the second last as validation, and earlier interactions for training. Performance metrics included Hit Rate (HR@K) and Normalized Discounted Cumulative Gain (NDCG@K) at various cut-offs (e.g., K = 5, 10) (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://arxiv.org/pdf/1904.06690v1 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder ...">arxiv.org</a>).</p><h3 id=baselines-compared>Baselines Compared<a hidden class=anchor aria-hidden=true href=#baselines-compared>#</a></h3><p>Sun et al. compared BERT4Rec against several state-of-the-art sequential recommendation methods:</p><ul><li><strong>GRU4Rec</strong>: RNN (GRU) based model with pairwise ranking loss.</li><li><strong>Casual Convolutional (CasualConv)</strong>: Convolutional neural network model for sequences.</li><li><strong>SASRec</strong>: Self-attention based unidirectional model.</li><li><strong>Caser</strong>: Convolutional sequence embedding model (vertical + horizontal convolution).</li><li><strong>NextItNet</strong>: Dilated residual network for sequential recommendation.</li></ul><h3 id=key-findings>Key Findings<a hidden class=anchor aria-hidden=true href=#key-findings>#</a></h3><ol><li><p><strong>BERT4Rec vs. SASRec</strong></p><ul><li>Across ML-1M and YooChoose, BERT4Rec improved HR@10 by ≈2–3% and NDCG@10 by ≈1–2% relative to SASRec (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://arxiv.org/pdf/1904.06690v1 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder ...">arxiv.org</a>).</li><li>On sparser datasets like Steam, the advantage increased, indicating that bidirectional context can better handle data sparsity by leveraging co-occurrence patterns across entire sessions.</li></ul></li><li><p><strong>Model Depth and Hidden Size</strong></p><ul><li>Deeper (more layers) or wider (larger <em>d</em>) BERT4Rec variants performed better on large datasets but risked overfitting on smaller ones.</li><li>Typical configurations: 2 layers, hidden size 64 for ML-1M; 3–4 layers for larger datasets.</li></ul></li><li><p><strong>Masking Ratio</strong></p><ul><li>Masking ~15% of items per sequence yielded a good trade-off. Masking too many positions reduced signal per position; masking too few yielded fewer training samples.</li></ul></li><li><p><strong>Training Time</strong></p><ul><li>BERT4Rec required more compute than SASRec due to larger parameter counts and Cloze objective.</li><li>Subsequent research (Petrov & Macdonald, 2022) noted that default training schedules in the original implementations were too short to fully converge on some datasets; when trained longer, BERT4Rec’s performance became more consistent (<a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>, <a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>).</li></ul></li></ol><h3 id=replicability-and-training-considerations>Replicability and Training Considerations<a hidden class=anchor aria-hidden=true href=#replicability-and-training-considerations>#</a></h3><p>Petrov and Macdonald (2022) conducted a systematic review and replicability study of BERT4Rec, finding:</p><ul><li><strong>Training Time Sensitivity</strong>: Default hyperparameters often led to under-trained models. Training 10–30× longer was sometimes necessary to reproduce reported results (<a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>, <a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>).</li><li><strong>Batch Size and Learning Rates</strong>: Smaller batch sizes with warm-up steps and linear decay of learning rates yielded more stable convergence.</li><li><strong>Alternative Architectures</strong>: Implementations using Hugging Face’s Transformers library, incorporating variants like DeBERTa’s disentangled attention, matched or exceeded original results with significantly less training time (<a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>, <a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>).</li></ul><p>Another study by Petrov & Macdonald (2023) introduced <strong>gSASRec</strong>, which showed that SASRec could outperform BERT4Rec when properly addressing overconfidence arising from negative sampling (<a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>). They argued that BERT4Rec’s bidirectional mechanism alone did not guarantee superiority; rather, loss formulations and training strategies play a crucial role.</p><h3 id=comparative-strengths-and-weaknesses>Comparative Strengths and Weaknesses<a hidden class=anchor aria-hidden=true href=#comparative-strengths-and-weaknesses>#</a></h3><ul><li><p><strong>Strengths</strong></p><ul><li><em>Rich Context Modeling</em>: By conditioning on both sides of a position, BERT4Rec captures intricate co-occurrence patterns.</li><li><em>Data Efficiency</em>: Masked positions generate more supervision signals per sequence.</li><li><em>Flexibility</em>: Can predict items at arbitrary positions, enabling applications like sequential imputation or session completion beyond next-item recommendation.</li></ul></li><li><p><strong>Weaknesses</strong></p><ul><li><em>Compute and Memory</em>: More parameters and bidirectional attention make it more expensive in both training and inference compared to unidirectional models.</li><li><em>Training Sensitivity</em>: Requires careful hyperparameter tuning and longer training times to reach optimal performance.</li><li><em>Inference Unidirectionality for Next-Item</em>: Although trained bidirectionally, predicting the next item requires inserting a [MASK] with no right context, effectively making inference unidirectional, possibly leaving some benefits unused.</li></ul></li></ul><h2 id=conceptual-insights-why-bert4rec-works>Conceptual Insights: Why BERT4Rec Works<a hidden class=anchor aria-hidden=true href=#conceptual-insights-why-bert4rec-works>#</a></h2><h3 id=learning-co-occurrence-vs-sequential-order>Learning Co-Occurrence vs. Sequential Order<a hidden class=anchor aria-hidden=true href=#learning-co-occurrence-vs-sequential-order>#</a></h3><p>Unlike unidirectional models that focus on ordering—item t predicts item t+1—BERT4Rec learns from co-occurrence patterns across sessions:</p><ul><li>Items A and B that consistently appear together in sessions might have high mutual information.</li><li>If A often precedes B and also often follows B, unidirectional models only see one direction; BERT4Rec sees both, learning a symmetric association.</li></ul><p>In recommendation, co-occurrence is often more informative than strict ordering. For example, if many users watch “The Matrix” and “Inception” in any order, a bidirectional model picks up that association, regardless of which came first.</p><h3 id=overcoming-exposure-bias>Overcoming Exposure Bias<a hidden class=anchor aria-hidden=true href=#overcoming-exposure-bias>#</a></h3><p>Unidirectional models train to predict the next item given ground-truth history. During inference, they must use predicted items (or no items) to form history, leading to <strong>exposure bias</strong>—errors compound as the model has never seen its own mistakes. In contrast, BERT4Rec’s masking randomly hides items during training, exposing the model to situations where parts of the sequence are unknown, resulting in more robust representations when some interactions are missing or noisy (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://arxiv.org/abs/1810.04805 title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">arxiv.org</a>).</p><h3 id=analogous-to-autoencoders>Analogous to Autoencoders<a hidden class=anchor aria-hidden=true href=#analogous-to-autoencoders>#</a></h3><p>BERT4Rec’s training resembles an <strong>autoencoder</strong>: it corrupts (masks) parts of the input and learns to reconstruct them. This formulation encourages the model to learn latent representations capturing holistic session semantics. In collaborative filtering, <strong>denoising autoencoders</strong> (e.g., CDAE) have been used for recommendation, where randomly corrupted user vectors are reconstructed (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://www.researchgate.net/profile/Fei-Sun-41/publication/332438773_BERT4Rec_Sequential_Recommendation_with_Bidirectional_Encoder_Representations_from_Transformer/links/6047567d299bf1e0786667eb/BERT4Rec-Sequential-Recommendation-with-Bidirectional-Encoder-Representations-from-Transformer.pdf title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder ...">researchgate.net</a>). BERT4Rec extends that idea to sequences of interactions with the Transformer’s bidirectional power.</p><h2 id=broader-context-from-language-to-recommendation>Broader Context: From Language to Recommendation<a hidden class=anchor aria-hidden=true href=#broader-context-from-language-to-recommendation>#</a></h2><h3 id=transfer-of-ideas-across-domains>Transfer of Ideas Across Domains<a hidden class=anchor aria-hidden=true href=#transfer-of-ideas-across-domains>#</a></h3><p>BERT4Rec is an instance of <strong>cross-pollination</strong> between NLP and recommendation research. Historically, many breakthroughs in one field find applications in others:</p><ul><li><strong>Word2Vec (2013)</strong>: Initially for word embeddings, later adapted for graph embeddings, collaborative filtering, and more.</li><li><strong>Convolutional Neural Networks (1995–2012)</strong>: Developed for image tasks, later adapted for text (CNNs for sentence classification) and recommendation (Caser uses convolution to model user-item sequences).</li><li><strong>Attention Mechanisms (2014–2017)</strong>: Originating in machine translation, now used in recommendation (e.g., SASRec, BERT4Rec, and many variants).</li></ul><p>The flow of ideas mirrors human creativity: when we learn a concept in one context, we often find analogous patterns in another.</p><p>Analogy: Leonardo da Vinci studied bird flight to design flying machines. Similarly, BERT4Rec studies how Transformers learn from language sequences to design better user modeling systems.</p><h3 id=historical-perspective-the-rise-of-pre-training>Historical Perspective: The Rise of Pre-Training<a hidden class=anchor aria-hidden=true href=#historical-perspective-the-rise-of-pre-training>#</a></h3><p>In both language and recommendation, there is a shift from <strong>task-specific training</strong> to <strong>pre-training + fine-tuning</strong>:</p><ul><li>In NLP, models like ELMo (2018), GPT (2018), and BERT (2018–2019) introduced large-scale pre-training on massive unlabeled corpora, followed by fine-tuning on downstream tasks (<a href=https://arxiv.org/abs/1810.04805 title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">arxiv.org</a>, <a href=https://aclanthology.org/N19-1423/ title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">aclanthology.org</a>).</li><li>In recommendation, early models trained from scratch on each dataset. Now, researchers explore <strong>pre-training on large interaction logs</strong> to learn general user behavior patterns, then fine-tune on specific domains (e.g., news, movies). BERT4Rec’s Cloze objective could be viewed as a form of self-supervised pre-training, although in the original work they trained on the target dataset from scratch (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>).</li></ul><p>This trend reflects a broader movement in AI: capturing general knowledge from large data and adapting it to specific tasks, mirroring human learning—children first learn language generally, then apply it to specialized domains like mathematics or science.</p><h2 id=limitations-and-challenges>Limitations and Challenges<a hidden class=anchor aria-hidden=true href=#limitations-and-challenges>#</a></h2><h3 id=computational-complexity>Computational Complexity<a hidden class=anchor aria-hidden=true href=#computational-complexity>#</a></h3><p>BERT4Rec’s bidirectional attention has <strong>quadratic</strong> time and memory complexity with respect to sequence length. In long sessions (e.g., browsing logs with hundreds of items), this becomes a bottleneck. Several strategies mitigate this:</p><ul><li><strong>Truncated Histories</strong>: Only consider the last <em>L</em> items (e.g., L = 200).</li><li><strong>Segmented or Sliding Windows</strong>: Process overlapping windows of fixed length rather than the entire history.</li><li><strong>Efficient Attention Variants</strong>: Use sparse attention (e.g., Linformer, Performer) to reduce complexity from O(L²) to O(L log L) or O(L) (<a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>).</li></ul><p>Nonetheless, these require extra engineering and can affect performance if important interactions get truncated.</p><h3 id=training-sensitivity-and-hyperparameters>Training Sensitivity and Hyperparameters<a hidden class=anchor aria-hidden=true href=#training-sensitivity-and-hyperparameters>#</a></h3><p>As noted by Petrov and Macdonald (2022), BERT4Rec’s performance is sensitive to:</p><ul><li><strong>Number of Training Epochs</strong>: Standard schedules may under-train the model.</li><li><strong>Learning Rate Schedules</strong>: Warm-up steps followed by linear decay often yield stable performance.</li><li><strong>Batch Size and Mask Ratio</strong>: Larger batches and masking too many positions can hinder learning.</li><li><strong>Negative Sampling Effects</strong>: Overconfidence in ranking due to unbalanced positive/negative sampling can lead to suboptimal results; alternative loss functions (e.g., gBCE) can mitigate this (<a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>, <a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>).</li></ul><p>This contrasts with smaller unidirectional models like SASRec, which often converge faster and require fewer tuning efforts.</p><h3 id=cold-start-and-long-tail-items>Cold-Start and Long-Tail Items<a hidden class=anchor aria-hidden=true href=#cold-start-and-long-tail-items>#</a></h3><p>Like many collaborative filtering methods, BERT4Rec struggles with:</p><ul><li><strong>Cold-Start Users</strong>: Users with very short or no interaction history. Masked predictions require context—if there’s no context, predictions degrade.</li><li><strong>Cold-Start Items</strong>: Items with very few interactions. Their embeddings are not well trained, making them less likely to be predicted.</li><li><strong>Long-Tail Distribution</strong>: Most items appear infrequently; BERT4Rec can overfit popular items seen many times in training, biasing recommendations.</li></ul><p>Mitigations include:</p><ul><li>Incorporating <strong>content features</strong> (e.g., item metadata, text descriptions) through hybrid models.</li><li>Using <strong>meta-learning</strong> to quickly adapt to new items or users.</li><li>Employing <strong>data augmentation</strong> (e.g., synthetic interactions) to enrich representations.</li></ul><h3 id=interpretability>Interpretability<a hidden class=anchor aria-hidden=true href=#interpretability>#</a></h3><p>Transformers are often regarded as “black boxes.” While attention weights can sometimes be visualized to show which items influence predictions, they do not guarantee human-interpretable explanations. Efforts to explain recommendation via attention often reveal that attention scores do not always align with intuitive importance (<a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>). For stakeholders demanding transparency, additional interpretability methods (e.g., counterfactual explanations, post-hoc analysis) may be needed.</p><h2 id=variants-and-extensions>Variants and Extensions<a hidden class=anchor aria-hidden=true href=#variants-and-extensions>#</a></h2><h3 id=incorporating-side-information>Incorporating Side Information<a hidden class=anchor aria-hidden=true href=#incorporating-side-information>#</a></h3><p>BERT4Rec can be extended to use side features:</p><ul><li><strong>User Features</strong>: Demographics, location, device, etc.</li><li><strong>Item Features</strong>: Category, price, textual description, images.</li><li><strong>Session Context</strong>: Time gaps, device changes, location transitions.</li></ul><p>One approach is to concatenate side feature embeddings with item embeddings at each position, then feed the combined vector into the Transformer (<a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>). Alternatively, one can use separate Transformer streams for different modalities and then merge them (e.g., multi-modality Transformers).</p><h3 id=pre-training-on-large-scale-logs>Pre-Training on Large-Scale Logs<a hidden class=anchor aria-hidden=true href=#pre-training-on-large-scale-logs>#</a></h3><p>Instead of training BERT4Rec from scratch on a target dataset, it can be <strong>pre-trained</strong> on massive generic interaction logs (e.g., clicks across many categories) and <strong>fine-tuned</strong> on a domain-specific dataset (e.g., music). Pre-training tasks might include:</p><ul><li><strong>Masked Item Prediction</strong> (as usual).</li><li><strong>Segment Prediction</strong>: Predict whether a sequence segment belongs to the same user.</li><li><strong>Next Session Prediction</strong>: Predict which next session a user will have.</li></ul><p>After pre-training, the model adapts faster to downstream tasks, especially in data-sparse domains. This mimics BERT’s success in NLP.</p><h3 id=combining-with-contrastive-learning>Combining with Contrastive Learning<a hidden class=anchor aria-hidden=true href=#combining-with-contrastive-learning>#</a></h3><p>Recent trends in self-supervised learning for recommendation incorporate <strong>contrastive objectives</strong>, encouraging similar user sequences or items to have similar representations. One can combine BERT4Rec’s Cloze objective with contrastive losses (e.g., SimCLR, MoCo) to further improve generalization:</p><ul><li><strong>Sequence-Level Contrast</strong>: Represent a user session by pooling BERT4Rec’s hidden states; contrast similar sessions against dissimilar ones.</li><li><strong>Item-Level Contrast</strong>: Encourage items co-occurring frequently to have similar embeddings.</li></ul><p>Contrastive learning can mitigate representation collapse and improve robustness.</p><h3 id=efficient-transformer-variants>Efficient Transformer Variants<a hidden class=anchor aria-hidden=true href=#efficient-transformer-variants>#</a></h3><p>To handle long sequences more efficiently:</p><ul><li><strong>Linformer</strong>: Projects keys and values to a lower dimension before computing attention, reducing complexity from O(L²) to O(L) (<a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>).</li><li><strong>Performer</strong>: Uses kernel methods to approximate softmax attention linearly in sequence length.</li><li><strong>Longformer</strong>: Employs sliding window (local) attention and global tokens.</li><li><strong>Reformer</strong>: Uses locality-sensitive hashing to reduce attention costs.</li></ul><p>These variants can be plugged into BERT4Rec’s framework to handle longer sessions while retaining bidirectional context.</p><h2 id=future-directions>Future Directions<a hidden class=anchor aria-hidden=true href=#future-directions>#</a></h2><h3 id=personalization-and-diversity>Personalization and Diversity<a hidden class=anchor aria-hidden=true href=#personalization-and-diversity>#</a></h3><p>While BERT4Rec focuses on accuracy metrics like HR@K and NDCG@K, real-world systems must balance <strong>personalization</strong> with <strong>diversity</strong> to avoid echo chambers. Future work could:</p><ul><li>Include <strong>diversity-aware objectives</strong>, penalizing recommendations that are too similar to each other.</li><li>Integrate <strong>exploration strategies</strong>, e.g., adding randomness to top-K predictions to surface niche items.</li><li>Leverage <strong>reinforcement learning</strong> to optimize long-term engagement rather than immediate next click.</li></ul><h3 id=adaptation-to-multi-objective-settings>Adaptation to Multi-Objective Settings<a hidden class=anchor aria-hidden=true href=#adaptation-to-multi-objective-settings>#</a></h3><p>E-commerce platforms care about metrics beyond clicks—revenues, lifetime value, churn reduction. Extensions of BERT4Rec could incorporate:</p><ul><li><strong>Multi-Task Learning</strong>: Jointly predict next item and other objectives (e.g., purchase probability, churn risk).</li><li><strong>Bandit Feedback</strong>: Combine BERT4Rec embeddings with contextual bandit algorithms to dynamically adapt to user feedback.</li><li><strong>Causal Inference</strong>: Adjust for selection bias in logged interactions, using inverse propensity scoring with BERT4Rec representations.</li></ul><h3 id=explainability-and-trust>Explainability and Trust<a hidden class=anchor aria-hidden=true href=#explainability-and-trust>#</a></h3><p>Building user trust in recommendations requires transparency. Research could focus on:</p><ul><li><strong>Attention-Based Explanations</strong>: Visualizing attention maps to show which past items influenced a recommendation.</li><li><strong>Counterfactual Explanations</strong>: Explaining “if you hadn’t clicked on item A, you might not see item B recommended.”</li><li><strong>User-Friendly Summaries</strong>: Summarizing session themes (e.g., “Because you watched yoga videos, we recommend this fitness product”).</li></ul><h3 id=cross-seat-and-cross-device-scenarios>Cross-Seat and Cross-Device Scenarios<a hidden class=anchor aria-hidden=true href=#cross-seat-and-cross-device-scenarios>#</a></h3><p>Users often switch between devices (phone, laptop, TV) and contexts (work, home). Modeling these cross-seat patterns requires:</p><ul><li><strong>Hierarchical Transformers</strong>: One level encodes per-device sequences; another encodes cross-device transitions.</li><li><strong>Time-Aware Modeling</strong>: Incorporate temporal embeddings for time gaps between interactions, using continuous time Transformers.</li></ul><h3 id=hybrid-with-knowledge-graphs>Hybrid with Knowledge Graphs<a hidden class=anchor aria-hidden=true href=#hybrid-with-knowledge-graphs>#</a></h3><p>Many platforms maintain <strong>knowledge graphs</strong> linking items to attributes, categories, and external entities. Integrating BERT4Rec embeddings with graph neural networks (GNNs) can enrich representations:</p><ul><li><strong>Graph-Enhanced Embeddings</strong>: Use GNNs to initialize item embeddings based on their neighbors in the knowledge graph.</li><li><strong>Joint Attention over Sequences and Graphs</strong>: Attend over historical interactions and relevant graph nodes.</li></ul><h2 id=personal-reflections-and-closing-thoughts>Personal Reflections and Closing Thoughts<a hidden class=anchor aria-hidden=true href=#personal-reflections-and-closing-thoughts>#</a></h2><p>Building BERT4Rec felt like standing on the shoulders of giants: from Markov models that taught me the basics of transitions, to RNNs that showed me how to carry hidden state, to attention mechanisms that revealed the power of flexible context, to BERT’s bidirectional pre-training that inspired me to look at user sequences holistically. Each step deepened my understanding of how to model dynamic preferences, echoing my own journey of learning and exploration.</p><p>I’ve always believed that technical advancements in AI should be connected to human-centered insights. When I see masked language models predicting words, I think of a student piecing together meaning. When I see masked item tasks predicting products, I imagine someone reconstructing their shopping trajectory, filling in forgotten steps. These analogies bridge the gap between cold mathematics and living experiences, reminding me that behind each click or purchase is a person with evolving interests, context, and purpose.</p><p>BERT4Rec is not the final word in sequential recommendation. It represents a milestone—a demonstration that ideas from language modeling can transform how we think about recommendation. But as we push forward, we must keep asking: How can we make models more efficient without sacrificing nuance? How can we ensure diversity and fairness? How can we respect privacy while learning from behavior? I hope this post not only explains BERT4Rec’s mechanics but also sparks your own curiosity to explore these questions further.</p><h2 id=references-and-further-reading>References and Further Reading<a hidden class=anchor aria-hidden=true href=#references-and-further-reading>#</a></h2><ul><li>Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>. In NAACL-HLT (pp. 4171–4186). (<a href=https://arxiv.org/abs/1810.04805 title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">arxiv.org</a>, <a href=https://aclanthology.org/N19-1423/ title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">aclanthology.org</a>)</li><li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). <em>Attention Is All You Need</em>. In NeurIPS (pp. 5998–6008). (<a href=https://arxiv.org/abs/1706.03762 title="Attention Is All You Need">arxiv.org</a>, <a href=https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf title="Attention is All you Need - NIPS">papers.nips.cc</a>)</li><li>Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., & Jiang, P. (2019). <em>BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</em>. In CIKM (pp. 1441–1450). (<a href=https://arxiv.org/abs/1904.06690 title="BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer">arxiv.org</a>, <a href=https://github.com/FeiSun/BERT4Rec title="GitHub - FeiSun/BERT4Rec: BERT4Rec: Sequential Recommendation with ...">github.com</a>)</li><li>Kang, W.-C., & McAuley, J. (2018). <em>Self-Attentive Sequential Recommendation</em>. In ICDM (pp. 197–206). (<a href=https://arxiv.org/abs/1808.09781 title="Self-Attentive Sequential Recommendation">arxiv.org</a>, <a href=https://cseweb.ucsd.edu/~jmcauley/pdfs/icdm18.pdf title="Self-Attentive Sequential Recommendation - University of California ...">cseweb.ucsd.edu</a>)</li><li>Petrov, A., & Macdonald, C. (2022). <em>A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation</em>. arXiv:2207.07483. (<a href=https://arxiv.org/abs/2207.07483 title="A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation">arxiv.org</a>, <a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>)</li><li>Petrov, A., & Macdonald, C. (2023). <em>gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling</em>. arXiv:2308.07192. (<a href=https://arxiv.org/abs/2308.07192 title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling">arxiv.org</a>)</li><li>Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>. arXiv:1810.04805. (<a href=https://arxiv.org/abs/1810.04805 title="BERT: Pre-training of Deep Bidirectional Transformers for Language ...">arxiv.org</a>, <a href=https://export.arxiv.org/abs/1810.04805 title="[1810.04805] BERT: Pre-training of Deep Bidirectional ... - arXiv">export.arxiv.org</a>)</li><li>Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>. arXiv:1810.04805v1. (<a href=https://eecs.csuohio.edu/~sschung/CIS660/BERTGoogle2018.pdf title="arXiv:1810.04805v1 [cs.CL] 11 Oct 2018 - Cleveland State University">eecs.csuohio.edu</a>, <a href=https://ar5iv.labs.arxiv.org/html/1810.04805 title="BERT : Pre-training of Deep Bidirectional Transformers for - ar5iv">ar5iv.labs.arxiv.org</a>)</li><li>Kang, W.-C., & McAuley, J. (2018). <em>Self-Attentive Sequential Recommendation</em>. arXiv:1808.09781. (<a href=https://arxiv.org/pdf/1808.09781 title="arXiv.org e-Print archive">arxiv.org</a>, <a href=https://ar5iv.labs.arxiv.org/html/1808.09781 title="[1808.09781] Self-Attentive Sequential Recommendation">ar5iv.labs.arxiv.org</a>)</li><li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). <em>Attention Is All You Need</em>. arXiv:1706.03762. (<a href=https://export.arxiv.org/abs/1706.03762v5 title="[1706.03762v5] Attention Is All You Need - arXiv">export.arxiv.org</a>, <a href=https://en.wikipedia.org/wiki/Attention_Is_All_You_Need title="Attention Is All You Need - Wikipedia">en.wikipedia.org</a>)</li><li>Petrov, A., & Macdonald, C. (2022). <em>A Systematic Review and Replicability Study of BERT4Rec for Sequential Recommendation</em>. arXiv:2207.07483.</li><li>Petrov, A., & Macdonald, C. (2023). <em>gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling</em>. arXiv:2308.07192.</li><li>Hu, Y., Zhang, Y., Sun, N., Murai, M., Li, M., & King, I. (2018). <em>Utilizing Long- and Short-Term Structure for Memory-Based Sequential Recommendation</em>. In WWW (pp. 1281–1290).</li><li>Wu, L., Sun, X., Wang, Y., & Wu, J. (2020). <em>S3-Rec: Self-Supervised Seq2Seq Autoregressive Reconstruction for Sequential Recommendation</em>. In KDD (pp. 1267–1277).</li><li>Tan, Y. K., & Yang, J. (2021). <em>Light-BERT4Rec: Accelerating BERT4Rec via Knowledge Distillation for Sequential Recommendation</em>. In CIKM.</li><li>Yang, N., Wang, W., & Zhao, J. (2021). <em>TransRec: Learning User and Item Representations for Sequential Recommendation with Multi-Head Self-Attention</em>. In Sarnoff Symposium.</li><li>Bi, W., Zhu, X., Lv, H., & Wang, W. (2021). <em>AdaSAS: Adaptive User Interest Modeling with Multi-Hop Self-Attention for Sequential Recommendation</em>. In RecSys.</li><li>Ying, C., Fei, K., Wang, X., Wei, F., Mao, J., & Gao, J. (2018). <em>Graph Convolutional Neural Networks for Web-Scale Recommender Systems</em>. In KDD. (Used as analogy for combining graph structures with sequence modeling.)</li><li>He, R., & McAuley, J. (2016). <em>VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback</em>. In AAAI. (Illustrates use of side information in recommendation.)</li><li>Wang, X., He, X., Cao, Y., Liu, M., & Chua, T.-S. (2019). <em>KGAT: Knowledge Graph Attention Network for Recommendation</em>. In KDD. (Shows integration of knowledge graphs for richer item representations.)</li></ul></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers on x" href="https://x.com/intent/tweet/?text=BERT4Rec%20%3a%20Decoding%20Sequential%20Recommendations%20with%20the%20Power%20of%20Transformers&amp;url=https%3a%2f%2fpjainish.github.io%2fposts%2fbert4rec-sequential-recommendation%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpjainish.github.io%2fposts%2fbert4rec-sequential-recommendation%2f&amp;title=BERT4Rec%20%3a%20Decoding%20Sequential%20Recommendations%20with%20the%20Power%20of%20Transformers&amp;summary=BERT4Rec%20%3a%20Decoding%20Sequential%20Recommendations%20with%20the%20Power%20of%20Transformers&amp;source=https%3a%2f%2fpjainish.github.io%2fposts%2fbert4rec-sequential-recommendation%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpjainish.github.io%2fposts%2fbert4rec-sequential-recommendation%2f&title=BERT4Rec%20%3a%20Decoding%20Sequential%20Recommendations%20with%20the%20Power%20of%20Transformers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpjainish.github.io%2fposts%2fbert4rec-sequential-recommendation%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers on whatsapp" href="https://api.whatsapp.com/send?text=BERT4Rec%20%3a%20Decoding%20Sequential%20Recommendations%20with%20the%20Power%20of%20Transformers%20-%20https%3a%2f%2fpjainish.github.io%2fposts%2fbert4rec-sequential-recommendation%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers on telegram" href="https://telegram.me/share/url?text=BERT4Rec%20%3a%20Decoding%20Sequential%20Recommendations%20with%20the%20Power%20of%20Transformers&amp;url=https%3a%2f%2fpjainish.github.io%2fposts%2fbert4rec-sequential-recommendation%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BERT4Rec : Decoding Sequential Recommendations with the Power of Transformers on ycombinator" href="https://news.ycombinator.com/submitlink?t=BERT4Rec%20%3a%20Decoding%20Sequential%20Recommendations%20with%20the%20Power%20of%20Transformers&u=https%3a%2f%2fpjainish.github.io%2fposts%2fbert4rec-sequential-recommendation%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://pjainish.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></article></main><footer class=footer><span>&copy; 2025 <a href=https://pjainish.github.io/>Jainish's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>